{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFAPP_4wUR0a"
   },
   "source": [
    "# Machine Learning and Neural Networks Final Project: Predicting Heart Disease Presence From Medical Records.\n",
    "\n",
    "## Define the problem\n",
    "\n",
    "The problem with undiagnosed heart conditions is that many individuals are unaware of their presence. Without early medical intervention, many undiagnosed cases unfortunately lead to avoidable deaths. For this project, the main objective is to predict the presence of heart disease in individuals based on a dataset from the 2020 Behavioural Risk Factor Surveillance System (BRFSS) survey, which has been curated and publised on Kaggle. **Please note that the Kaggle webpage provides both 2020 and 2022 datasets. For the purpose of this project, the 2020 dataset is chosen.** [2] [3]\n",
    "\n",
    "The BRFSS survey, run by the US Centers for Disease Control and Prevention (CDC), obtains annual data on health conditions and risk factors of the US population via phone interviews. The 2020 survey includes data from all 50 states, the District of Columbia, Guam and Puerto Rico, representing a dataset of over 400,000 records. The dataset used in this project had been further processed to emphasise variables that have a more significant impact on heart disease risk, including high blood pressure, high cholesterol, smoking habits, presence of diabetes etc. The preprocessed dataset has over 300,000 datapoints.\n",
    "\n",
    "The main goal of predicting the presence of heart disease falls under the category of binary classification. The column of \"HeartDisease\" indicating whether a respondent has had a heart disease (e.g. \"Yes\" or \"No\") serves as the label and the aforementioned preprocessed variables as features.\n",
    "\n",
    "The aim is to accurately identifying the patterns within this dataset that correlate with the presence of heart disease using deep learning techniques. A successful model could enhance the early detection and prevention strategies in healthcare settings.\n",
    "\n",
    "## Inputs, Outputs and Hypotheses\n",
    "\n",
    "### Inputs\n",
    "Out model will use the 17 features from the preprocessed Kaggle dataset as inputs. Specifically, these features are: Body Mass Index (BMI), Smoking status, Alcohol Drinking habits, history of Stroke, Physical Health and Mental Health status, difficulty Walking (DiffWalking), Sex, Age Category, Race, Diabetic status, engagement in Physical Activity, general Health status (GenHealth), average Sleep Time, and history of Asthma, Kidney Disease, and Skin Cancer.\n",
    "\n",
    "### Outputs\n",
    "The output is the binary classification of heart disease presence, where \"Yes\" indicates the respondent has experienced heart disease and \"No\" indicates otherwise.\n",
    "\n",
    "### Hypotheses:\n",
    "\n",
    "We hypothesise that the inputs have enough statistical power to detect the presence of heart disease and thus can help detect at-risk patients. More specifically, we want the model to be able to detect the vast majority of respondents with a presence of heart disease, meaning that the recall rate as a measurement is of particular importance.\n",
    "\n",
    "### Use case\n",
    "If proven effective, the trained model could sift through vast medical records to pinpoint undiagnosed patients at high risk of heart conditions. This could revolutionise early detection and prevention and help save lives.\n",
    "\n",
    "##Measure of success\n",
    "\n",
    "###Recall and Precision rates\n",
    "Given the importance of correctly detecting as many high-risk individuals as possible, the recall rate is the primary measure of success. A high recall rate ensures that the vast majority of real positive cases are captured, which minimises the chance of missing undiagnosed individuals who are at risk of heart disease.\n",
    "\n",
    "To maintain a balanced perspective, precision rate is also considered. Accuracy in this context is not relevant as the dataset is highly imbalanced (explained more below), with only around 8.5% of datapoints are positives. This means that if the model were to naively predict all datapoints as 0's, this would give an accuracy of around 92%, which would be false. Hence, accuracy is **not considered** in this project.\n",
    "\n",
    "Precision, the percentage of true positive predictions in all positive predictions, is a relevant measurement. A high precision rate ensures that the individuals that the model identifies has high risk are indeed likely to have a heart condition. This is because a high number of false positives would increase the anxiety of the patients and involve unnecessary medical evaluations.\n",
    "\n",
    "### Loss function\n",
    "We have chosen binary cossentropy as our loss function, which is suited for binary classification problems. Our model is compiled with an SGD optimiser and is evaluated using metrics that include accuracy, recall and precision.\n",
    "\n",
    "## Evaluation protocol\n",
    "With our dataset of over 300,000 data points, we've determined that using a hold-out validation set is the most optimal evaluation protocol for our model. We plan to allocate 80% of the dataset for training purposes, with the remaining 20% reserved for final testing. Within the training segment, we will further divide the data, allocating 80% for initial training and 20% for validation. This division allows us to fine tune our model's hyperparameters to achieve the most optimal settings. After identifying the most suitable hyperparameters, we will merge the initial training and validation sets to re-train the model using these optimised settings. The separate testing set, constituting the final 20% of the data, will then serve to evaluate our model's prediction performance, ensuring we have a clear understanding of its effectiveness in real world scenarios.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "msZputYlvfdP"
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "Let's begin by importing the 2020 dataset. We will then shuffle the dataset so that the dataset is more balanced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "qRTA_wGkUas7",
    "outputId": "2b7b18ed-ceb7-4f41-8d1f-89dbb061d902"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "variable_name": "df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-909258a8-f34f-4132-b4ce-5224323494af\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HeartDisease</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoking</th>\n",
       "      <th>AlcoholDrinking</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>PhysicalHealth</th>\n",
       "      <th>MentalHealth</th>\n",
       "      <th>DiffWalking</th>\n",
       "      <th>Sex</th>\n",
       "      <th>AgeCategory</th>\n",
       "      <th>Race</th>\n",
       "      <th>Diabetic</th>\n",
       "      <th>PhysicalActivity</th>\n",
       "      <th>GenHealth</th>\n",
       "      <th>SleepTime</th>\n",
       "      <th>Asthma</th>\n",
       "      <th>KidneyDisease</th>\n",
       "      <th>SkinCancer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>271884</th>\n",
       "      <td>No</td>\n",
       "      <td>27.63</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>No</td>\n",
       "      <td>Female</td>\n",
       "      <td>25-29</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Very good</td>\n",
       "      <td>7.0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270361</th>\n",
       "      <td>No</td>\n",
       "      <td>21.95</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>No</td>\n",
       "      <td>Female</td>\n",
       "      <td>30-34</td>\n",
       "      <td>White</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>6.0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219060</th>\n",
       "      <td>No</td>\n",
       "      <td>31.32</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>Female</td>\n",
       "      <td>40-44</td>\n",
       "      <td>White</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Very good</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24010</th>\n",
       "      <td>No</td>\n",
       "      <td>40.35</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>Female</td>\n",
       "      <td>65-69</td>\n",
       "      <td>White</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Good</td>\n",
       "      <td>8.0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181930</th>\n",
       "      <td>No</td>\n",
       "      <td>35.61</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Female</td>\n",
       "      <td>60-64</td>\n",
       "      <td>White</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Fair</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-909258a8-f34f-4132-b4ce-5224323494af')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-909258a8-f34f-4132-b4ce-5224323494af button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-909258a8-f34f-4132-b4ce-5224323494af');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-c53a5c95-1a40-4b29-b0ba-0390c427221b\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c53a5c95-1a40-4b29-b0ba-0390c427221b')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-c53a5c95-1a40-4b29-b0ba-0390c427221b button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "       HeartDisease    BMI Smoking AlcoholDrinking Stroke  PhysicalHealth  \\\n",
       "271884           No  27.63     Yes              No     No             0.0   \n",
       "270361           No  21.95      No              No     No             0.0   \n",
       "219060           No  31.32     Yes              No     No             0.0   \n",
       "24010            No  40.35      No              No     No            30.0   \n",
       "181930           No  35.61     Yes              No     No            30.0   \n",
       "\n",
       "        MentalHealth DiffWalking     Sex AgeCategory      Race Diabetic  \\\n",
       "271884          25.0          No  Female       25-29  Hispanic       No   \n",
       "270361          20.0          No  Female       30-34     White       No   \n",
       "219060           0.0          No  Female       40-44     White       No   \n",
       "24010            0.0          No  Female       65-69     White       No   \n",
       "181930          30.0         Yes  Female       60-64     White       No   \n",
       "\n",
       "       PhysicalActivity  GenHealth  SleepTime Asthma KidneyDisease SkinCancer  \n",
       "271884              Yes  Very good        7.0     No            No         No  \n",
       "270361              Yes  Excellent        6.0     No            No        Yes  \n",
       "219060              Yes  Very good        6.0    Yes            No         No  \n",
       "24010                No       Good        8.0     No            No         No  \n",
       "181930               No       Fair        4.0    Yes            No        Yes  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/heart_2020_cleaned.csv')\n",
    "\n",
    "#shuffle the dataset\n",
    "df = df.sample(frac=1, random_state=42)\n",
    "\n",
    "#set the option to display all columns, None means no limith\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKwGR3Nx4RBq"
   },
   "source": [
    "As the features are not in a form that can be processed by the machine learning algorithm, we need to encode them. To prepare out data for modelling, we implemented an encoding function to transform these categorical and orginal variables into a suitable format. The function begins by encoding binary features, namely Smoking, Alcohol Drinking, Stroke, Difficulty Walking, Diabetic status, Physical Activity, Asthma, Kidney Disease and Cancer by mapping \"Yes\" to 1 and \"No\" to 0.\n",
    "\n",
    "For the \"Sex\" variable, a binary encoding is applied, mapping \"Male\" to 1 and \"Female\" to 0.\n",
    "\n",
    "Ordinal encoding is utilised for GenHealth and AgeCategory features to reflect their order. \"GenHealth\" is encoded based on health status, ranging from Poor (0) to \"Excellent\" (4), while AgeCategory is mapped to a sequence starting from 18-24(0) to 80 or Older (12), which captures the progression through age groups.\n",
    "\n",
    "The Race feature is transformed into one-hot encoding to create binary columns for each cateogyr/ This ensures that our model can interpret these non-ordinal data without implying order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "id": "ABoqHcc6VT37",
    "outputId": "373c88a4-601c-4823-d190-0dfd1ff25d11"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "variable_name": "encoded_df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-dda2f738-caab-4a62-83de-447e8e91ac58\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HeartDisease</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoking</th>\n",
       "      <th>AlcoholDrinking</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>PhysicalHealth</th>\n",
       "      <th>MentalHealth</th>\n",
       "      <th>DiffWalking</th>\n",
       "      <th>Sex</th>\n",
       "      <th>AgeCategory</th>\n",
       "      <th>Diabetic</th>\n",
       "      <th>PhysicalActivity</th>\n",
       "      <th>GenHealth</th>\n",
       "      <th>SleepTime</th>\n",
       "      <th>Asthma</th>\n",
       "      <th>KidneyDisease</th>\n",
       "      <th>SkinCancer</th>\n",
       "      <th>Race_American Indian/Alaskan Native</th>\n",
       "      <th>Race_Asian</th>\n",
       "      <th>Race_Black</th>\n",
       "      <th>Race_Hispanic</th>\n",
       "      <th>Race_Other</th>\n",
       "      <th>Race_White</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>271884</th>\n",
       "      <td>0</td>\n",
       "      <td>27.63</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270361</th>\n",
       "      <td>0</td>\n",
       "      <td>21.95</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219060</th>\n",
       "      <td>0</td>\n",
       "      <td>31.32</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24010</th>\n",
       "      <td>0</td>\n",
       "      <td>40.35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181930</th>\n",
       "      <td>0</td>\n",
       "      <td>35.61</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dda2f738-caab-4a62-83de-447e8e91ac58')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-dda2f738-caab-4a62-83de-447e8e91ac58 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-dda2f738-caab-4a62-83de-447e8e91ac58');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-202838e5-ac2f-40f2-8bd0-68c80d14dc64\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-202838e5-ac2f-40f2-8bd0-68c80d14dc64')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-202838e5-ac2f-40f2-8bd0-68c80d14dc64 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "        HeartDisease    BMI  Smoking  AlcoholDrinking  Stroke  PhysicalHealth  \\\n",
       "271884             0  27.63        1                0       0             0.0   \n",
       "270361             0  21.95        0                0       0             0.0   \n",
       "219060             0  31.32        1                0       0             0.0   \n",
       "24010              0  40.35        0                0       0            30.0   \n",
       "181930             0  35.61        1                0       0            30.0   \n",
       "\n",
       "        MentalHealth  DiffWalking  Sex  AgeCategory  Diabetic  \\\n",
       "271884          25.0            0    0          1.0       0.0   \n",
       "270361          20.0            0    0          2.0       0.0   \n",
       "219060           0.0            0    0          4.0       0.0   \n",
       "24010            0.0            0    0          9.0       0.0   \n",
       "181930          30.0            1    0          8.0       0.0   \n",
       "\n",
       "        PhysicalActivity  GenHealth  SleepTime  Asthma  KidneyDisease  \\\n",
       "271884                 1        3.0        7.0       0              0   \n",
       "270361                 1        4.0        6.0       0              0   \n",
       "219060                 1        3.0        6.0       1              0   \n",
       "24010                  0        2.0        8.0       0              0   \n",
       "181930                 0        1.0        4.0       1              0   \n",
       "\n",
       "        SkinCancer  Race_American Indian/Alaskan Native  Race_Asian  \\\n",
       "271884           0                                    0           0   \n",
       "270361           1                                    0           0   \n",
       "219060           0                                    0           0   \n",
       "24010            0                                    0           0   \n",
       "181930           1                                    0           0   \n",
       "\n",
       "        Race_Black  Race_Hispanic  Race_Other  Race_White  \n",
       "271884           0              1           0           0  \n",
       "270361           0              0           0           1  \n",
       "219060           0              0           0           1  \n",
       "24010            0              0           0           1  \n",
       "181930           0              0           0           1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_features(table):\n",
    "\n",
    "    encoded_table = table.copy()\n",
    "\n",
    "    #encode binary value columns\n",
    "    binary_col = ['HeartDisease', 'Smoking', 'AlcoholDrinking', 'Stroke',\n",
    "                  'DiffWalking', 'Diabetic', 'PhysicalActivity', 'Asthma',\n",
    "                  'KidneyDisease', 'SkinCancer']\n",
    "\n",
    "    #mapping yes or no to 1/0 for specified columns\n",
    "    for col in binary_col:\n",
    "        encoded_table[col] = encoded_table[col].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "    #mapping gender\n",
    "    encoded_table['Sex'] = encoded_table['Sex'].map({'Male': 1, 'Female': 0})\n",
    "\n",
    "    #ordinal encoding for GenHealth\n",
    "    gen_health_mapping = {\n",
    "        'Poor': 0.,\n",
    "        'Fair': 1.,\n",
    "        'Good': 2.,\n",
    "        'Very good': 3.,\n",
    "        'Excellent': 4.,\n",
    "    }\n",
    "\n",
    "    encoded_table['GenHealth'] = encoded_table['GenHealth'].map(gen_health_mapping)\n",
    "\n",
    "    #ordinal encoding for AgeCategory\n",
    "    age_mapping = {\n",
    "        '18-24': 0.,\n",
    "        '25-29': 1.,\n",
    "        '30-34': 2.,\n",
    "        '35-39': 3.,\n",
    "        '40-44': 4.,\n",
    "        '45-49': 5.,\n",
    "        '50-54': 6.,\n",
    "        '55-59': 7.,\n",
    "        '60-64': 8.,\n",
    "        '65-69': 9.,\n",
    "        '70-74': 10.,\n",
    "        '75-79': 11.,\n",
    "        '80 or older': 12.\n",
    "    }\n",
    "\n",
    "    encoded_table['AgeCategory'] = encoded_table['AgeCategory'].map(age_mapping)\n",
    "\n",
    "    #encode categorical value columns\n",
    "    encoded_table = pd.get_dummies(encoded_table, columns=['Race'])\n",
    "\n",
    "    return encoded_table\n",
    "\n",
    "encoded_df = encode_features(df)\n",
    "encoded_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1W7Kfes06HM3"
   },
   "source": [
    "After encoding, we need to ensure that the relevant columns are not too highly skewed, which could distort our model. Highly skewed columns need to be corrected with a log function. [7] Then, we normalise certain features, namely BMI, PhysicalHealth, MentalHealth, SleepTime, GenHealth and AgeCategory by subtracting the mean and dividing by the standard deciation. This step is crucial as it scales the data to a standard range in order to reduce the sensitivity to the magnitude of values, especially during gradient descent. By ensuring all features contribute equally to the model's algorithm, we need to prevent any one feature from disproportionately influencing the model's performance due to scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J9xR7R4hT2F6",
    "outputId": "4c66e68a-11e8-411b-8ad7-c279efa8c845"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewness for HeartDisease: 2.962510851115938\n",
      "Skewness for BMI: 1.3324243931174087\n",
      "Skewness for Smoking: 0.35558317399512934\n",
      "Skewness for AlcoholDrinking: 3.4290029916894125\n",
      "Skewness for Stroke: 4.851437338465762\n",
      "Skewness for PhysicalHealth: 2.6039610482829403\n",
      "Skewness for MentalHealth: 2.3311006150318803\n",
      "Skewness for DiffWalking: 2.0885960328153232\n",
      "Skewness for Sex: 0.09902828096492025\n",
      "Skewness for AgeCategory: -0.26360963242463975\n",
      "Skewness for Diabetic: nan\n",
      "Skewness for PhysicalActivity: -1.3195957489859749\n",
      "Skewness for GenHealth: -0.45395972291181264\n",
      "Skewness for SleepTime: 0.6790314357818192\n",
      "Skewness for Asthma: 2.148048659158537\n",
      "Skewness for KidneyDisease: 4.91811241943461\n",
      "Skewness for SkinCancer: 2.7977438579025247\n",
      "Skewness for Race_American Indian/Alaskan Native: 7.648001682140592\n",
      "Skewness for Race_Asian: 6.055025866131191\n",
      "Skewness for Race_Black: 3.319394195468926\n",
      "Skewness for Race_Hispanic: 2.9573078823642285\n",
      "Skewness for Race_Other: 5.1282744310766555\n",
      "Skewness for Race_White: -1.2617173107461217\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import skew\n",
    "\n",
    "\n",
    "# skewness = skew(df['PhysicalHealth'])\n",
    "# skewness\n",
    "\n",
    "for col in encoded_df.columns:\n",
    "    skewness = skew(encoded_df[col])\n",
    "    print(f\"Skewness for {col}: {skewness}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_ztX3IvcoSO"
   },
   "source": [
    "The only relevant columns are PhysicalHealth, MentalHealth and BMI. The rest of the features are either categorical or Yes/No binary columns, which are not features that need to be corrected for their skewness. We chose 1.5 as the threshold for implementing skewness correction so that only the most badly skewed are corrected. Since two relevant columns, namely PhysicalHealth and MentalHealth have skewness values of in excess of 1.5 or -1.5, this suggests that they are highly skewed. We thus correct the skewness of the relevant columns by applying the log function within the normalise function. [6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "id": "-Dl_ac5JVaUw",
    "outputId": "10fb5441-307e-41d1-8ff1-a838b47a125f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "variable_name": "encoded_df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-7237660d-db12-4d41-beba-5572b39642e5\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HeartDisease</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoking</th>\n",
       "      <th>AlcoholDrinking</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>PhysicalHealth</th>\n",
       "      <th>MentalHealth</th>\n",
       "      <th>DiffWalking</th>\n",
       "      <th>Sex</th>\n",
       "      <th>AgeCategory</th>\n",
       "      <th>Diabetic</th>\n",
       "      <th>PhysicalActivity</th>\n",
       "      <th>GenHealth</th>\n",
       "      <th>SleepTime</th>\n",
       "      <th>Asthma</th>\n",
       "      <th>KidneyDisease</th>\n",
       "      <th>SkinCancer</th>\n",
       "      <th>Race_American Indian/Alaskan Native</th>\n",
       "      <th>Race_Asian</th>\n",
       "      <th>Race_Black</th>\n",
       "      <th>Race_Hispanic</th>\n",
       "      <th>Race_Other</th>\n",
       "      <th>Race_White</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>271884</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.109406</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.561911</td>\n",
       "      <td>2.201605</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.546959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.388307</td>\n",
       "      <td>-0.067600</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270361</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.003036</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.561911</td>\n",
       "      <td>2.014021</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.266435</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.347155</td>\n",
       "      <td>-0.763976</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219060</th>\n",
       "      <td>0</td>\n",
       "      <td>0.471138</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.561911</td>\n",
       "      <td>-0.660016</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.705387</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.388307</td>\n",
       "      <td>-0.763976</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24010</th>\n",
       "      <td>0</td>\n",
       "      <td>1.891821</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.602409</td>\n",
       "      <td>-0.660016</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.697232</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.570542</td>\n",
       "      <td>0.628775</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181930</th>\n",
       "      <td>0</td>\n",
       "      <td>1.146080</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.602409</td>\n",
       "      <td>2.356092</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.416708</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.529390</td>\n",
       "      <td>-2.156727</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7237660d-db12-4d41-beba-5572b39642e5')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-7237660d-db12-4d41-beba-5572b39642e5 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-7237660d-db12-4d41-beba-5572b39642e5');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-57eb79f6-3a7e-48c3-8f6f-349419a8de60\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-57eb79f6-3a7e-48c3-8f6f-349419a8de60')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-57eb79f6-3a7e-48c3-8f6f-349419a8de60 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "        HeartDisease       BMI  Smoking  AlcoholDrinking  Stroke  \\\n",
       "271884             0 -0.109406        1                0       0   \n",
       "270361             0 -1.003036        0                0       0   \n",
       "219060             0  0.471138        1                0       0   \n",
       "24010              0  1.891821        0                0       0   \n",
       "181930             0  1.146080        1                0       0   \n",
       "\n",
       "        PhysicalHealth  MentalHealth  DiffWalking  Sex  AgeCategory  Diabetic  \\\n",
       "271884       -0.561911      2.201605            0    0    -1.546959       0.0   \n",
       "270361       -0.561911      2.014021            0    0    -1.266435       0.0   \n",
       "219060       -0.561911     -0.660016            0    0    -0.705387       0.0   \n",
       "24010         2.602409     -0.660016            0    0     0.697232       0.0   \n",
       "181930        2.602409      2.356092            1    0     0.416708       0.0   \n",
       "\n",
       "        PhysicalActivity  GenHealth  SleepTime  Asthma  KidneyDisease  \\\n",
       "271884                 1   0.388307  -0.067600       0              0   \n",
       "270361                 1   1.347155  -0.763976       0              0   \n",
       "219060                 1   0.388307  -0.763976       1              0   \n",
       "24010                  0  -0.570542   0.628775       0              0   \n",
       "181930                 0  -1.529390  -2.156727       1              0   \n",
       "\n",
       "        SkinCancer  Race_American Indian/Alaskan Native  Race_Asian  \\\n",
       "271884           0                                    0           0   \n",
       "270361           1                                    0           0   \n",
       "219060           0                                    0           0   \n",
       "24010            0                                    0           0   \n",
       "181930           1                                    0           0   \n",
       "\n",
       "        Race_Black  Race_Hispanic  Race_Other  Race_White  \n",
       "271884           0              1           0           0  \n",
       "270361           0              0           0           1  \n",
       "219060           0              0           0           1  \n",
       "24010            0              0           0           1  \n",
       "181930           0              0           0           1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalise(table):\n",
    "\n",
    "    #correct the skewness for high skewness columns apart from categorical and Yes/No columns\n",
    "    table['PhysicalHealth'] = np.log1p(table['PhysicalHealth'])\n",
    "    table['MentalHealth'] = np.log1p(table['MentalHealth'])\n",
    "    # table['BMI'] = np.log1p(table['BMI'])\n",
    "\n",
    "    # print(skew(table['PhysicalHealth']))\n",
    "\n",
    "    normalise_cols = ['BMI', 'PhysicalHealth', 'MentalHealth', 'SleepTime', 'GenHealth', 'AgeCategory']\n",
    "\n",
    "\n",
    "    for col in normalise_cols:\n",
    "\n",
    "        mean = table[col].mean(axis=0)\n",
    "        table[col] -= mean\n",
    "        std = table[col].std(axis=0)\n",
    "        table[col] /= std\n",
    "\n",
    "\n",
    "\n",
    "    return table\n",
    "\n",
    "encoded_df = normalise(encoded_df)\n",
    "encoded_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5t24rZB62t_"
   },
   "source": [
    "After being encoded and normalised, we need to check to see if there are any null values in the dataset. If so, the associated rows will be deleted, provided that the number of rows with missing values is manageable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MddNuW8-1qor",
    "outputId": "222a53f5-aeb7-4453-88a9-11e5b1d533e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeartDisease                              0\n",
      "BMI                                       0\n",
      "Smoking                                   0\n",
      "AlcoholDrinking                           0\n",
      "Stroke                                    0\n",
      "PhysicalHealth                            0\n",
      "MentalHealth                              0\n",
      "DiffWalking                               0\n",
      "Sex                                       0\n",
      "AgeCategory                               0\n",
      "Diabetic                               9340\n",
      "PhysicalActivity                          0\n",
      "GenHealth                                 0\n",
      "SleepTime                                 0\n",
      "Asthma                                    0\n",
      "KidneyDisease                             0\n",
      "SkinCancer                                0\n",
      "Race_American Indian/Alaskan Native       0\n",
      "Race_Asian                                0\n",
      "Race_Black                                0\n",
      "Race_Hispanic                             0\n",
      "Race_Other                                0\n",
      "Race_White                                0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(encoded_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uacqpOaKBI6i"
   },
   "source": [
    "The number of null values in the Diabetic column is 9340, a small percentage of the overall dataset. Since this number is manageable, we will proceed to deleting these rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DdeWIJbn41-b",
    "outputId": "a8f2f387-29ce-47e4-fa3b-787dd523d2e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "encoded_df = encoded_df.dropna(subset=['Diabetic'])\n",
    "print(encoded_df['Diabetic'].isnull().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "axr-2l6FK8M7",
    "outputId": "159ccdc0-1b77-4d2b-8bbf-3713f8ad0afd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26476"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_disease_rows = encoded_df[encoded_df['HeartDisease'] == 1]\n",
    "len(heart_disease_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Le2ijct2BeOM"
   },
   "source": [
    "In order to train and test our model, we divided the preprocessed dataset into training and testing sets using an 80:20 split ratio. This resulted in the majority of the data, 80%, being allocated for training (all_train), and the remaining 20% set aside for testing (all_test).\n",
    "\n",
    "Then, we split the all_train dataset into partial training (all_partial) and validation (all_val) sets using another 80:20 split. This ensures proper splits for hyperparameter tuning and validation, allowing for evaluation before final testing.\n",
    "\n",
    "The dataset shows a huge imbalance, with only around 8.5% of the data in each segment representing individuals with heart disease. Such imbalance is a challenge as it can lead to model bias towards the majority class, affecting the training of the model.\n",
    "\n",
    "To resolve this imbalance and improve model performance, we implemented a function to oversample the minority class (HeartDisease == 1) in only the partial training set by duplicating these records 9 times. This method aims to get the all_partial set to a more balanced 50:50 ratio, enhancing the model's ability to learn from an equal representation of both classes. [5]\n",
    "\n",
    "It is crucial to stress that oversampling was performed on only the all_partial dataset segment to maintain the integrity of the unseen data and to prevent any data leakage. This approach is important for the reliability of our evaluation and the overall effectiveness of the predictive model, which is evaluated against the validation and final test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gYxt49xcLuZv"
   },
   "outputs": [],
   "source": [
    "#function to oversample\n",
    "def oversample(dataset_df):\n",
    "  heart_disease_df = dataset_df[dataset_df['HeartDisease'] == 1]\n",
    "\n",
    "  #duplicate these rows 9 times\n",
    "  duplicated_rows = pd.concat([heart_disease_df]*9, ignore_index=True)\n",
    "\n",
    "  #combine the original dataframe with the duplicated rows\n",
    "  df_combined = pd.concat([dataset_df, duplicated_rows], ignore_index=True)\n",
    "\n",
    "  #shuffle the combined DataFrame\n",
    "  dataset_df = df_combined.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "  return dataset_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qf_i_B3qQTnk",
    "outputId": "f49f6fea-e397-4509-e8ad-075e8e423892"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before oversample Percentage of HeartDisease == `1` in all_partial: 8.57%\n",
      "Before oversample Percentage of HeartDisease == `1` in all_val: 8.39%\n",
      "Before oversample Percentage of HeartDisease == `1` in all_test: 8.49%\n",
      "After oversample Percentage of HeartDisease == `1` in all_partial: 48.40%\n",
      "After oversample Percentage of HeartDisease == `1` in all_val: 8.39%\n",
      "After oversample Percentage of HeartDisease == `1` in all_test: 8.49%\n"
     ]
    }
   ],
   "source": [
    "#separate the data for training and testing\n",
    "\n",
    "#test train split ratio\n",
    "train_split_ratio = int(len(encoded_df)*0.8)\n",
    "\n",
    "#split encoded_df to all_train and all_test\n",
    "all_train = encoded_df[:train_split_ratio]\n",
    "all_test = encoded_df[train_split_ratio:]\n",
    "\n",
    "#keeping the original all_test just for testing. might delete later.\n",
    "all_test_original = encoded_df[train_split_ratio:]\n",
    "\n",
    "#partial train and val split ratio\n",
    "partial_split_ratio = int(len(all_train)*0.8)\n",
    "\n",
    "#split all_train to all_partial and all_val\n",
    "all_partial = all_train[:partial_split_ratio]\n",
    "all_val = all_train[partial_split_ratio:]\n",
    "\n",
    "\n",
    "#Before oversampling percentages:\n",
    "#formatting the calculation of the proportion of `1` in y_partial as a percentage\n",
    "percentage = (all_partial['HeartDisease'] == 1).sum() / len(all_partial) * 100\n",
    "print(f\"Before oversample Percentage of HeartDisease == `1` in all_partial: {percentage:.2f}%\")\n",
    "\n",
    "#formatting the calculation of the proportion of `1` in all_val as a percentage\n",
    "percentage = (all_val['HeartDisease'] == 1).sum() / len(all_val) * 100\n",
    "print(f\"Before oversample Percentage of HeartDisease == `1` in all_val: {percentage:.2f}%\")\n",
    "\n",
    "#formatting the calculation of the proportion of `1` in all_val as a percentage\n",
    "percentage = (all_test['HeartDisease'] == 1).sum() / len(all_test) * 100\n",
    "print(f\"Before oversample Percentage of HeartDisease == `1` in all_test: {percentage:.2f}%\")\n",
    "\n",
    "\n",
    "#oversample all_partial\n",
    "all_partial = oversample(all_partial)\n",
    "\n",
    "# #oversample the all_val\n",
    "# all_val = oversample(all_val)\n",
    "\n",
    "# #oversample the general test set\n",
    "# all_test = oversample(all_test)\n",
    "\n",
    "#After oversampling percentages\n",
    "#formatting the calculation of the proportion of `1` in y_partial as a percentage\n",
    "percentage = (all_partial['HeartDisease'] == 1).sum() / len(all_partial) * 100\n",
    "print(f\"After oversample Percentage of HeartDisease == `1` in all_partial: {percentage:.2f}%\")\n",
    "\n",
    "#formatting the calculation of the proportion of `1` in all_val as a percentage\n",
    "percentage = (all_val['HeartDisease'] == 1).sum() / len(all_val) * 100\n",
    "print(f\"After oversample Percentage of HeartDisease == `1` in all_val: {percentage:.2f}%\")\n",
    "\n",
    "#formatting the calculation of the proportion of `1` in all_val as a percentage\n",
    "percentage = (all_test['HeartDisease'] == 1).sum() / len(all_test) * 100\n",
    "print(f\"After oversample Percentage of HeartDisease == `1` in all_test: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "udxIVla_VfYE",
    "outputId": "ac4dafcb-24f0-438e-85cf-e7db34a678fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "Length of x_train: 248364\n",
      "Length of y_train: 248364\n",
      "\n",
      "Test:\n",
      "Length of x_test: 62091\n",
      "Length of y_test: 62091\n",
      "\n",
      "Partial:\n",
      "Length of x_partial: 352015\n",
      "Length of y_partial: 352015\n",
      "\n",
      "Validation (Val):\n",
      "Length of x_val: 49673\n",
      "Length of y_val: 49673\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#double check the lengths of each data segments to see everything looks correct.\n",
    "#overall training dataset\n",
    "y_train = all_train['HeartDisease']\n",
    "x_train = all_train.drop(columns=['HeartDisease'])\n",
    "\n",
    "#test training dataset\n",
    "y_test = all_test['HeartDisease']\n",
    "x_test = all_test.drop(columns=['HeartDisease'])\n",
    "\n",
    "#partial train and val\n",
    "y_partial = all_partial['HeartDisease']\n",
    "x_partial = all_partial.drop(columns=['HeartDisease'])\n",
    "\n",
    "y_val = all_val['HeartDisease']\n",
    "x_val = all_val.drop(columns=['HeartDisease'])\n",
    "\n",
    "#printing the sizes of the train, test, partial and validation datasets\n",
    "print(\"Train:\")\n",
    "print(f\"Length of x_train: {len(x_train)}\")\n",
    "print(f\"Length of y_train: {len(y_train)}\\n\")\n",
    "\n",
    "print(\"Test:\")\n",
    "print(f\"Length of x_test: {len(x_test)}\")\n",
    "print(f\"Length of y_test: {len(y_test)}\\n\")\n",
    "\n",
    "print(\"Partial:\")\n",
    "print(f\"Length of x_partial: {len(x_partial)}\")\n",
    "print(f\"Length of y_partial: {len(y_partial)}\\n\")\n",
    "\n",
    "print(\"Validation (Val):\")\n",
    "print(f\"Length of x_val: {len(x_val)}\")\n",
    "print(f\"Length of y_val: {len(y_val)}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2nyrrT-WdsM"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 135
    },
    "id": "CWF9LVnG9M4i",
    "outputId": "a3b37f3e-2752-421e-ab6f-26cbfd6972c8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-c0eae286-504a-4df9-956a-a2cb274cf817\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoking</th>\n",
       "      <th>AlcoholDrinking</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>PhysicalHealth</th>\n",
       "      <th>MentalHealth</th>\n",
       "      <th>DiffWalking</th>\n",
       "      <th>Sex</th>\n",
       "      <th>AgeCategory</th>\n",
       "      <th>Diabetic</th>\n",
       "      <th>PhysicalActivity</th>\n",
       "      <th>GenHealth</th>\n",
       "      <th>SleepTime</th>\n",
       "      <th>Asthma</th>\n",
       "      <th>KidneyDisease</th>\n",
       "      <th>SkinCancer</th>\n",
       "      <th>Race_American Indian/Alaskan Native</th>\n",
       "      <th>Race_Asian</th>\n",
       "      <th>Race_Black</th>\n",
       "      <th>Race_Hispanic</th>\n",
       "      <th>Race_Other</th>\n",
       "      <th>Race_White</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.962005</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.561911</td>\n",
       "      <td>2.356092</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.416708</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.570542</td>\n",
       "      <td>0.628775</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c0eae286-504a-4df9-956a-a2cb274cf817')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-c0eae286-504a-4df9-956a-a2cb274cf817 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-c0eae286-504a-4df9-956a-a2cb274cf817');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "        BMI  Smoking  AlcoholDrinking  Stroke  PhysicalHealth  MentalHealth  \\\n",
       "0  0.962005        0                0       0       -0.561911      2.356092   \n",
       "\n",
       "   DiffWalking  Sex  AgeCategory  Diabetic  PhysicalActivity  GenHealth  \\\n",
       "0            0    1     0.416708       0.0                 1  -0.570542   \n",
       "\n",
       "   SleepTime  Asthma  KidneyDisease  SkinCancer  \\\n",
       "0   0.628775       0              0           0   \n",
       "\n",
       "   Race_American Indian/Alaskan Native  Race_Asian  Race_Black  Race_Hispanic  \\\n",
       "0                                    0           0           0              0   \n",
       "\n",
       "   Race_Other  Race_White  \n",
       "0           0           1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_partial.iloc[[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n3bvocnCxFRa",
    "outputId": "3b05984c-93ec-4d23-e637-7576b08ac14c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "Name: HeartDisease, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_partial.iloc[[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQWk8XYStORa"
   },
   "source": [
    "# Developing the model\n",
    "\n",
    "We use a dense layer with 100 neurons and the relu activation function as the first layer. This layer takes input with 22 features, which match our dataset's characteristics. After this, we added a dropout layer with a rate of 0.3. \n",
    "\n",
    "We repeat this pattern three more times by adding a dense layer with 100 neurons and relu activation followed by a dropout layer. This structure helps the model learn complex patterns in the data by adding more depth while preventing overfitting with dropout.\n",
    "\n",
    "For the final layer, we use a dense layer with 1 neuron and a sigmoid activation function. The sigmoid function outputs a value between 0 and 1, making it suitable for our binary classification task.\n",
    "\n",
    "To train the model, we chose the Stochastic Gradient Descent (SGD) optimiser with a learning rate of 0.001 and a momentum of 0.9. \n",
    "\n",
    "We compiled the model with the binary crossentropy loss function, which is commonly used for binary classification tasks. \n",
    "\n",
    "When training the model, we used a batch size of 1500, which means the model updates its weights after looking at 1500 examples. \n",
    "\n",
    "# Experiments\n",
    "\n",
    "While developing the model, we experimented with hyperparameters to optimise the model's architecture and training process. These experiments were key to finding the sweet spot for each parameter, balancing model performance with computational efficiency.\n",
    "\n",
    "Our experiments included a variety of learning rates for both the Stochastic Gradient Descent (SGD) and Adam optimizers. By adjusting the learning rates, we aimed to find a balance where the model could converge without sacrificing performance or speed.\n",
    "\n",
    "Batch size was another crucial variable. We tested sizes from 500 to 5500 and identified 1500 as the optimal batch size. This size provided the best compromise, offering a good gradient estimation that avoids the distortions of too-small or too-large sizes. This improves the models ability to generalise from training data effectively.\n",
    "\n",
    "Initially, to tackle overfitting, which is a common issue where models learn the training data too well but perform poorly on unseen data, we experimented with dropout and regularisation techniques (L1 and L2). However, we observed that a higher dropout rate or the inclusion of L1/L2 regularisation made the model underfit, which caused the validation loss to remain in the 0.50s. This indicated that the model is struggling to learn the complex patterns within our dataset. Similar outcomes were noted when we reduced the number of layers, decreased neuron units, or increased the optimiser's learning rate. These prevented the model from capturing the dataset's complex patterns.\n",
    "\n",
    "It became clear that the model needed to be strengthened. We increased the neuron count in each layer and added more layers while removing L1/L2 regularisation. This approach, coupled with reducing the batch size to 1500, even after trying for sizes up to 5500, proved effective. These adjustments led to a significant decrease in validation loss to the lower 0.40s and even the 0.30s, signalling an improved fit. Notably, achieving a validation loss lower than the training loss, especially after these modifications, highlighted our model's ability to generalise and maintain strong recall rates on unseen, non-oversampled data. **Its important to remember that we only oversampled the partial training sets.**\n",
    "\n",
    "To resolve the dataset's imbalance, we initially considered using class weights to adjust the learning process in favour of the minority class. However, this strategy reduced precision even further while increasing validation loss. We then chose to oversample the minority class in the training data, instead, which proved to be an effective strategy for addressing class imbalance.\n",
    "\n",
    "Throughout this development phase, adjusting learning rates, batch sizes, layers, and neuron units, while strategically implementing dropout, class weights and oversampling, we refined our model. This process helped to improve the model's performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "A6nfll4VxKZy"
   },
   "outputs": [],
   "source": [
    "#Code taken from [1]\n",
    "\n",
    "#build the model\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from tensorflow.keras.metrics import Recall, Precision\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "def train_model(_epochs, X, Y, X_val=None, Y_val=None):\n",
    "    #initialise\n",
    "    model = models.Sequential()\n",
    "\n",
    "    #add layers\n",
    "    model.add(layers.Dense(100, activation = 'relu', input_shape = (22,)))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Dense(100, activation = 'relu'))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Dense(100, activation = 'relu'))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Dense(100, activation = 'relu'))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "\n",
    "    #chose sgd optimiser with the specific learning rate and momentum\n",
    "    # sgd_optimizer = SGD(learning_rate=0.0001, momentum=0.9)\n",
    "    sgd_optimizer = SGD(learning_rate=0.001, momentum=0.9)\n",
    "\n",
    "    # adam_optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "    model.compile(optimizer=sgd_optimizer,\n",
    "                loss='binary_crossentropy',\n",
    "                  # loss='mean_squared_error',\n",
    "                metrics=['accuracy', Recall(), Precision()])\n",
    "\n",
    "\n",
    "    if X_val is None or Y_val is None:\n",
    "      history = model.fit(X,\n",
    "                        Y,\n",
    "                        epochs=_epochs,\n",
    "                        batch_size=1500,)\n",
    "    else:\n",
    "      history = model.fit(X,\n",
    "                        Y,\n",
    "                        epochs=_epochs,\n",
    "                        batch_size=1500,\n",
    "                        validation_data=(X_val, Y_val),)\n",
    "\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZS5qLbpJVl60",
    "outputId": "ce18eec8-ed36-49c0-bb49-dd3ce11d0af9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "235/235 [==============================] - 5s 6ms/step - loss: 0.6852 - accuracy: 0.5488 - recall: 0.5544 - precision: 0.5325 - val_loss: 0.6731 - val_accuracy: 0.6893 - val_recall: 0.7078 - val_precision: 0.1719\n",
      "Epoch 2/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.6563 - accuracy: 0.6331 - recall: 0.6425 - precision: 0.6159 - val_loss: 0.6401 - val_accuracy: 0.7175 - val_recall: 0.7165 - val_precision: 0.1886\n",
      "Epoch 3/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.6208 - accuracy: 0.6815 - recall: 0.6649 - precision: 0.6731 - val_loss: 0.5964 - val_accuracy: 0.7246 - val_recall: 0.7321 - val_precision: 0.1955\n",
      "Epoch 4/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5902 - accuracy: 0.7052 - recall: 0.6818 - precision: 0.7010 - val_loss: 0.5607 - val_accuracy: 0.7242 - val_recall: 0.7580 - val_precision: 0.1994\n",
      "Epoch 5/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5706 - accuracy: 0.7160 - recall: 0.7005 - precision: 0.7091 - val_loss: 0.5416 - val_accuracy: 0.7152 - val_recall: 0.7851 - val_precision: 0.1981\n",
      "Epoch 6/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5574 - accuracy: 0.7251 - recall: 0.7226 - precision: 0.7132 - val_loss: 0.5253 - val_accuracy: 0.7087 - val_recall: 0.8004 - val_precision: 0.1966\n",
      "Epoch 7/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5485 - accuracy: 0.7310 - recall: 0.7378 - precision: 0.7153 - val_loss: 0.5134 - val_accuracy: 0.7041 - val_recall: 0.8146 - val_precision: 0.1960\n",
      "Epoch 8/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5425 - accuracy: 0.7343 - recall: 0.7501 - precision: 0.7150 - val_loss: 0.5086 - val_accuracy: 0.6975 - val_recall: 0.8254 - val_precision: 0.1940\n",
      "Epoch 9/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5390 - accuracy: 0.7370 - recall: 0.7617 - precision: 0.7140 - val_loss: 0.4988 - val_accuracy: 0.6968 - val_recall: 0.8295 - val_precision: 0.1942\n",
      "Epoch 10/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5344 - accuracy: 0.7407 - recall: 0.7710 - precision: 0.7154 - val_loss: 0.4933 - val_accuracy: 0.6959 - val_recall: 0.8343 - val_precision: 0.1944\n",
      "Epoch 11/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5319 - accuracy: 0.7425 - recall: 0.7774 - precision: 0.7153 - val_loss: 0.4864 - val_accuracy: 0.6967 - val_recall: 0.8347 - val_precision: 0.1949\n",
      "Epoch 12/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5294 - accuracy: 0.7437 - recall: 0.7817 - precision: 0.7152 - val_loss: 0.4856 - val_accuracy: 0.6951 - val_recall: 0.8395 - val_precision: 0.1947\n",
      "Epoch 13/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5265 - accuracy: 0.7462 - recall: 0.7882 - precision: 0.7160 - val_loss: 0.4814 - val_accuracy: 0.6968 - val_recall: 0.8393 - val_precision: 0.1956\n",
      "Epoch 14/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5252 - accuracy: 0.7466 - recall: 0.7898 - precision: 0.7159 - val_loss: 0.4769 - val_accuracy: 0.6986 - val_recall: 0.8391 - val_precision: 0.1965\n",
      "Epoch 15/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5246 - accuracy: 0.7474 - recall: 0.7922 - precision: 0.7161 - val_loss: 0.4757 - val_accuracy: 0.6986 - val_recall: 0.8400 - val_precision: 0.1967\n",
      "Epoch 16/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5233 - accuracy: 0.7490 - recall: 0.7959 - precision: 0.7168 - val_loss: 0.4727 - val_accuracy: 0.7000 - val_recall: 0.8388 - val_precision: 0.1972\n",
      "Epoch 17/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5213 - accuracy: 0.7495 - recall: 0.7976 - precision: 0.7167 - val_loss: 0.4691 - val_accuracy: 0.7022 - val_recall: 0.8379 - val_precision: 0.1984\n",
      "Epoch 18/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5210 - accuracy: 0.7504 - recall: 0.7995 - precision: 0.7172 - val_loss: 0.4668 - val_accuracy: 0.7033 - val_recall: 0.8369 - val_precision: 0.1988\n",
      "Epoch 19/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5197 - accuracy: 0.7503 - recall: 0.8002 - precision: 0.7168 - val_loss: 0.4651 - val_accuracy: 0.7047 - val_recall: 0.8359 - val_precision: 0.1995\n",
      "Epoch 20/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5187 - accuracy: 0.7506 - recall: 0.8009 - precision: 0.7170 - val_loss: 0.4618 - val_accuracy: 0.7070 - val_recall: 0.8343 - val_precision: 0.2005\n",
      "Epoch 21/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5177 - accuracy: 0.7524 - recall: 0.8028 - precision: 0.7187 - val_loss: 0.4628 - val_accuracy: 0.7063 - val_recall: 0.8345 - val_precision: 0.2002\n",
      "Epoch 22/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5166 - accuracy: 0.7526 - recall: 0.8038 - precision: 0.7184 - val_loss: 0.4625 - val_accuracy: 0.7066 - val_recall: 0.8343 - val_precision: 0.2004\n",
      "Epoch 23/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5164 - accuracy: 0.7534 - recall: 0.8054 - precision: 0.7189 - val_loss: 0.4597 - val_accuracy: 0.7087 - val_recall: 0.8328 - val_precision: 0.2014\n",
      "Epoch 24/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5155 - accuracy: 0.7532 - recall: 0.8055 - precision: 0.7186 - val_loss: 0.4575 - val_accuracy: 0.7101 - val_recall: 0.8319 - val_precision: 0.2020\n",
      "Epoch 25/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5150 - accuracy: 0.7533 - recall: 0.8058 - precision: 0.7186 - val_loss: 0.4563 - val_accuracy: 0.7110 - val_recall: 0.8309 - val_precision: 0.2024\n",
      "Epoch 26/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5142 - accuracy: 0.7545 - recall: 0.8068 - precision: 0.7198 - val_loss: 0.4549 - val_accuracy: 0.7117 - val_recall: 0.8302 - val_precision: 0.2027\n",
      "Epoch 27/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5136 - accuracy: 0.7544 - recall: 0.8071 - precision: 0.7195 - val_loss: 0.4549 - val_accuracy: 0.7117 - val_recall: 0.8302 - val_precision: 0.2027\n",
      "Epoch 28/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5131 - accuracy: 0.7549 - recall: 0.8084 - precision: 0.7197 - val_loss: 0.4524 - val_accuracy: 0.7138 - val_recall: 0.8287 - val_precision: 0.2038\n",
      "Epoch 29/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5126 - accuracy: 0.7550 - recall: 0.8074 - precision: 0.7201 - val_loss: 0.4530 - val_accuracy: 0.7135 - val_recall: 0.8290 - val_precision: 0.2036\n",
      "Epoch 30/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5123 - accuracy: 0.7554 - recall: 0.8085 - precision: 0.7203 - val_loss: 0.4506 - val_accuracy: 0.7147 - val_recall: 0.8285 - val_precision: 0.2043\n",
      "Epoch 31/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5115 - accuracy: 0.7561 - recall: 0.8104 - precision: 0.7205 - val_loss: 0.4509 - val_accuracy: 0.7150 - val_recall: 0.8287 - val_precision: 0.2045\n",
      "Epoch 32/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5110 - accuracy: 0.7563 - recall: 0.8101 - precision: 0.7209 - val_loss: 0.4496 - val_accuracy: 0.7158 - val_recall: 0.8283 - val_precision: 0.2049\n",
      "Epoch 33/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5104 - accuracy: 0.7561 - recall: 0.8104 - precision: 0.7205 - val_loss: 0.4490 - val_accuracy: 0.7163 - val_recall: 0.8283 - val_precision: 0.2052\n",
      "Epoch 34/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5106 - accuracy: 0.7572 - recall: 0.8119 - precision: 0.7213 - val_loss: 0.4488 - val_accuracy: 0.7165 - val_recall: 0.8273 - val_precision: 0.2052\n",
      "Epoch 35/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5097 - accuracy: 0.7561 - recall: 0.8102 - precision: 0.7205 - val_loss: 0.4472 - val_accuracy: 0.7174 - val_recall: 0.8261 - val_precision: 0.2055\n",
      "Epoch 36/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5093 - accuracy: 0.7575 - recall: 0.8119 - precision: 0.7217 - val_loss: 0.4473 - val_accuracy: 0.7174 - val_recall: 0.8263 - val_precision: 0.2056\n",
      "Epoch 37/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5087 - accuracy: 0.7574 - recall: 0.8115 - precision: 0.7219 - val_loss: 0.4471 - val_accuracy: 0.7178 - val_recall: 0.8263 - val_precision: 0.2058\n",
      "Epoch 38/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5084 - accuracy: 0.7575 - recall: 0.8122 - precision: 0.7216 - val_loss: 0.4465 - val_accuracy: 0.7184 - val_recall: 0.8259 - val_precision: 0.2061\n",
      "Epoch 39/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5082 - accuracy: 0.7573 - recall: 0.8116 - precision: 0.7216 - val_loss: 0.4453 - val_accuracy: 0.7189 - val_recall: 0.8256 - val_precision: 0.2064\n",
      "Epoch 40/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5079 - accuracy: 0.7575 - recall: 0.8125 - precision: 0.7216 - val_loss: 0.4442 - val_accuracy: 0.7198 - val_recall: 0.8259 - val_precision: 0.2069\n",
      "Epoch 41/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5080 - accuracy: 0.7581 - recall: 0.8125 - precision: 0.7224 - val_loss: 0.4434 - val_accuracy: 0.7205 - val_recall: 0.8247 - val_precision: 0.2072\n",
      "Epoch 42/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5073 - accuracy: 0.7582 - recall: 0.8130 - precision: 0.7222 - val_loss: 0.4442 - val_accuracy: 0.7204 - val_recall: 0.8254 - val_precision: 0.2072\n",
      "Epoch 43/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5067 - accuracy: 0.7577 - recall: 0.8125 - precision: 0.7218 - val_loss: 0.4426 - val_accuracy: 0.7213 - val_recall: 0.8237 - val_precision: 0.2076\n",
      "Epoch 44/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5064 - accuracy: 0.7581 - recall: 0.8123 - precision: 0.7224 - val_loss: 0.4447 - val_accuracy: 0.7203 - val_recall: 0.8256 - val_precision: 0.2072\n",
      "Epoch 45/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5060 - accuracy: 0.7589 - recall: 0.8137 - precision: 0.7229 - val_loss: 0.4410 - val_accuracy: 0.7224 - val_recall: 0.8225 - val_precision: 0.2081\n",
      "Epoch 46/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5058 - accuracy: 0.7587 - recall: 0.8126 - precision: 0.7230 - val_loss: 0.4416 - val_accuracy: 0.7220 - val_recall: 0.8227 - val_precision: 0.2079\n",
      "Epoch 47/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5056 - accuracy: 0.7595 - recall: 0.8134 - precision: 0.7238 - val_loss: 0.4424 - val_accuracy: 0.7215 - val_recall: 0.8227 - val_precision: 0.2076\n",
      "Epoch 48/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5052 - accuracy: 0.7590 - recall: 0.8147 - precision: 0.7227 - val_loss: 0.4396 - val_accuracy: 0.7236 - val_recall: 0.8215 - val_precision: 0.2087\n",
      "Epoch 49/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5058 - accuracy: 0.7594 - recall: 0.8132 - precision: 0.7238 - val_loss: 0.4410 - val_accuracy: 0.7225 - val_recall: 0.8220 - val_precision: 0.2081\n",
      "Epoch 50/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5045 - accuracy: 0.7595 - recall: 0.8150 - precision: 0.7231 - val_loss: 0.4398 - val_accuracy: 0.7231 - val_recall: 0.8215 - val_precision: 0.2084\n",
      "Epoch 51/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5047 - accuracy: 0.7593 - recall: 0.8145 - precision: 0.7231 - val_loss: 0.4381 - val_accuracy: 0.7245 - val_recall: 0.8203 - val_precision: 0.2091\n",
      "Epoch 52/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5046 - accuracy: 0.7594 - recall: 0.8144 - precision: 0.7233 - val_loss: 0.4403 - val_accuracy: 0.7229 - val_recall: 0.8215 - val_precision: 0.2083\n",
      "Epoch 53/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5049 - accuracy: 0.7600 - recall: 0.8150 - precision: 0.7238 - val_loss: 0.4377 - val_accuracy: 0.7248 - val_recall: 0.8203 - val_precision: 0.2093\n",
      "Epoch 54/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5043 - accuracy: 0.7589 - recall: 0.8146 - precision: 0.7226 - val_loss: 0.4378 - val_accuracy: 0.7242 - val_recall: 0.8208 - val_precision: 0.2090\n",
      "Epoch 55/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5038 - accuracy: 0.7598 - recall: 0.8150 - precision: 0.7236 - val_loss: 0.4383 - val_accuracy: 0.7236 - val_recall: 0.8211 - val_precision: 0.2086\n",
      "Epoch 56/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5038 - accuracy: 0.7601 - recall: 0.8162 - precision: 0.7235 - val_loss: 0.4375 - val_accuracy: 0.7244 - val_recall: 0.8208 - val_precision: 0.2091\n",
      "Epoch 57/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5032 - accuracy: 0.7598 - recall: 0.8165 - precision: 0.7230 - val_loss: 0.4364 - val_accuracy: 0.7251 - val_recall: 0.8201 - val_precision: 0.2095\n",
      "Epoch 58/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5034 - accuracy: 0.7599 - recall: 0.8160 - precision: 0.7233 - val_loss: 0.4377 - val_accuracy: 0.7238 - val_recall: 0.8213 - val_precision: 0.2088\n",
      "Epoch 59/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5032 - accuracy: 0.7597 - recall: 0.8155 - precision: 0.7233 - val_loss: 0.4362 - val_accuracy: 0.7251 - val_recall: 0.8199 - val_precision: 0.2094\n",
      "Epoch 60/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5034 - accuracy: 0.7606 - recall: 0.8162 - precision: 0.7242 - val_loss: 0.4354 - val_accuracy: 0.7259 - val_recall: 0.8196 - val_precision: 0.2099\n",
      "Epoch 61/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5026 - accuracy: 0.7608 - recall: 0.8169 - precision: 0.7241 - val_loss: 0.4356 - val_accuracy: 0.7255 - val_recall: 0.8203 - val_precision: 0.2097\n",
      "Epoch 62/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5024 - accuracy: 0.7602 - recall: 0.8160 - precision: 0.7238 - val_loss: 0.4363 - val_accuracy: 0.7251 - val_recall: 0.8213 - val_precision: 0.2096\n",
      "Epoch 63/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5028 - accuracy: 0.7601 - recall: 0.8159 - precision: 0.7236 - val_loss: 0.4359 - val_accuracy: 0.7254 - val_recall: 0.8211 - val_precision: 0.2098\n",
      "Epoch 64/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5024 - accuracy: 0.7610 - recall: 0.8168 - precision: 0.7245 - val_loss: 0.4356 - val_accuracy: 0.7253 - val_recall: 0.8206 - val_precision: 0.2096\n",
      "Epoch 65/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5023 - accuracy: 0.7605 - recall: 0.8172 - precision: 0.7236 - val_loss: 0.4350 - val_accuracy: 0.7254 - val_recall: 0.8206 - val_precision: 0.2097\n",
      "Epoch 66/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5019 - accuracy: 0.7612 - recall: 0.8185 - precision: 0.7241 - val_loss: 0.4343 - val_accuracy: 0.7256 - val_recall: 0.8203 - val_precision: 0.2098\n",
      "Epoch 67/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5019 - accuracy: 0.7605 - recall: 0.8179 - precision: 0.7233 - val_loss: 0.4337 - val_accuracy: 0.7259 - val_recall: 0.8199 - val_precision: 0.2100\n",
      "Epoch 68/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5018 - accuracy: 0.7609 - recall: 0.8177 - precision: 0.7240 - val_loss: 0.4335 - val_accuracy: 0.7261 - val_recall: 0.8194 - val_precision: 0.2100\n",
      "Epoch 69/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5017 - accuracy: 0.7607 - recall: 0.8176 - precision: 0.7238 - val_loss: 0.4340 - val_accuracy: 0.7254 - val_recall: 0.8208 - val_precision: 0.2098\n",
      "Epoch 70/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5014 - accuracy: 0.7607 - recall: 0.8182 - precision: 0.7235 - val_loss: 0.4334 - val_accuracy: 0.7260 - val_recall: 0.8196 - val_precision: 0.2100\n",
      "Epoch 71/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5015 - accuracy: 0.7605 - recall: 0.8174 - precision: 0.7236 - val_loss: 0.4330 - val_accuracy: 0.7260 - val_recall: 0.8199 - val_precision: 0.2100\n",
      "Epoch 72/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5016 - accuracy: 0.7607 - recall: 0.8187 - precision: 0.7234 - val_loss: 0.4342 - val_accuracy: 0.7251 - val_recall: 0.8215 - val_precision: 0.2096\n",
      "Epoch 73/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5018 - accuracy: 0.7611 - recall: 0.8188 - precision: 0.7238 - val_loss: 0.4317 - val_accuracy: 0.7268 - val_recall: 0.8184 - val_precision: 0.2103\n",
      "Epoch 74/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5005 - accuracy: 0.7610 - recall: 0.8181 - precision: 0.7239 - val_loss: 0.4337 - val_accuracy: 0.7256 - val_recall: 0.8203 - val_precision: 0.2098\n",
      "Epoch 75/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5010 - accuracy: 0.7608 - recall: 0.8184 - precision: 0.7236 - val_loss: 0.4325 - val_accuracy: 0.7265 - val_recall: 0.8196 - val_precision: 0.2103\n",
      "Epoch 76/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5009 - accuracy: 0.7608 - recall: 0.8179 - precision: 0.7237 - val_loss: 0.4328 - val_accuracy: 0.7260 - val_recall: 0.8199 - val_precision: 0.2100\n",
      "Epoch 77/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5006 - accuracy: 0.7610 - recall: 0.8182 - precision: 0.7239 - val_loss: 0.4331 - val_accuracy: 0.7256 - val_recall: 0.8206 - val_precision: 0.2099\n",
      "Epoch 78/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5004 - accuracy: 0.7612 - recall: 0.8189 - precision: 0.7238 - val_loss: 0.4305 - val_accuracy: 0.7269 - val_recall: 0.8184 - val_precision: 0.2103\n",
      "Epoch 79/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5003 - accuracy: 0.7618 - recall: 0.8195 - precision: 0.7244 - val_loss: 0.4317 - val_accuracy: 0.7263 - val_recall: 0.8203 - val_precision: 0.2102\n",
      "Epoch 80/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5000 - accuracy: 0.7611 - recall: 0.8194 - precision: 0.7236 - val_loss: 0.4307 - val_accuracy: 0.7269 - val_recall: 0.8187 - val_precision: 0.2104\n",
      "Epoch 81/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5002 - accuracy: 0.7610 - recall: 0.8184 - precision: 0.7238 - val_loss: 0.4328 - val_accuracy: 0.7257 - val_recall: 0.8206 - val_precision: 0.2099\n",
      "Epoch 82/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5000 - accuracy: 0.7613 - recall: 0.8188 - precision: 0.7240 - val_loss: 0.4320 - val_accuracy: 0.7261 - val_recall: 0.8196 - val_precision: 0.2100\n",
      "Epoch 83/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4997 - accuracy: 0.7617 - recall: 0.8196 - precision: 0.7242 - val_loss: 0.4312 - val_accuracy: 0.7267 - val_recall: 0.8196 - val_precision: 0.2104\n",
      "Epoch 84/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5001 - accuracy: 0.7610 - recall: 0.8188 - precision: 0.7237 - val_loss: 0.4315 - val_accuracy: 0.7261 - val_recall: 0.8201 - val_precision: 0.2101\n",
      "Epoch 85/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5000 - accuracy: 0.7612 - recall: 0.8189 - precision: 0.7239 - val_loss: 0.4298 - val_accuracy: 0.7272 - val_recall: 0.8184 - val_precision: 0.2105\n",
      "Epoch 86/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4996 - accuracy: 0.7616 - recall: 0.8200 - precision: 0.7241 - val_loss: 0.4302 - val_accuracy: 0.7270 - val_recall: 0.8194 - val_precision: 0.2105\n",
      "Epoch 87/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4998 - accuracy: 0.7620 - recall: 0.8201 - precision: 0.7244 - val_loss: 0.4299 - val_accuracy: 0.7272 - val_recall: 0.8194 - val_precision: 0.2107\n",
      "Epoch 88/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4996 - accuracy: 0.7617 - recall: 0.8197 - precision: 0.7242 - val_loss: 0.4295 - val_accuracy: 0.7272 - val_recall: 0.8194 - val_precision: 0.2107\n",
      "Epoch 89/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4996 - accuracy: 0.7618 - recall: 0.8205 - precision: 0.7240 - val_loss: 0.4303 - val_accuracy: 0.7265 - val_recall: 0.8206 - val_precision: 0.2104\n",
      "Epoch 90/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4996 - accuracy: 0.7615 - recall: 0.8206 - precision: 0.7237 - val_loss: 0.4301 - val_accuracy: 0.7266 - val_recall: 0.8206 - val_precision: 0.2105\n",
      "Epoch 91/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4995 - accuracy: 0.7615 - recall: 0.8205 - precision: 0.7237 - val_loss: 0.4284 - val_accuracy: 0.7276 - val_recall: 0.8191 - val_precision: 0.2109\n",
      "Epoch 92/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4993 - accuracy: 0.7614 - recall: 0.8199 - precision: 0.7237 - val_loss: 0.4298 - val_accuracy: 0.7265 - val_recall: 0.8206 - val_precision: 0.2104\n",
      "Epoch 93/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4992 - accuracy: 0.7623 - recall: 0.8212 - precision: 0.7244 - val_loss: 0.4287 - val_accuracy: 0.7271 - val_recall: 0.8196 - val_precision: 0.2107\n",
      "Epoch 94/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4991 - accuracy: 0.7622 - recall: 0.8201 - precision: 0.7248 - val_loss: 0.4290 - val_accuracy: 0.7269 - val_recall: 0.8203 - val_precision: 0.2107\n",
      "Epoch 95/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4987 - accuracy: 0.7624 - recall: 0.8208 - precision: 0.7247 - val_loss: 0.4294 - val_accuracy: 0.7266 - val_recall: 0.8215 - val_precision: 0.2106\n",
      "Epoch 96/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4992 - accuracy: 0.7622 - recall: 0.8212 - precision: 0.7243 - val_loss: 0.4294 - val_accuracy: 0.7265 - val_recall: 0.8211 - val_precision: 0.2105\n",
      "Epoch 97/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4988 - accuracy: 0.7621 - recall: 0.8212 - precision: 0.7242 - val_loss: 0.4288 - val_accuracy: 0.7268 - val_recall: 0.8201 - val_precision: 0.2105\n",
      "Epoch 98/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4983 - accuracy: 0.7622 - recall: 0.8211 - precision: 0.7244 - val_loss: 0.4268 - val_accuracy: 0.7278 - val_recall: 0.8187 - val_precision: 0.2110\n",
      "Epoch 99/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4986 - accuracy: 0.7619 - recall: 0.8207 - precision: 0.7241 - val_loss: 0.4271 - val_accuracy: 0.7275 - val_recall: 0.8191 - val_precision: 0.2109\n",
      "Epoch 100/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4983 - accuracy: 0.7621 - recall: 0.8208 - precision: 0.7244 - val_loss: 0.4285 - val_accuracy: 0.7267 - val_recall: 0.8201 - val_precision: 0.2104\n",
      "Epoch 101/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4985 - accuracy: 0.7621 - recall: 0.8209 - precision: 0.7243 - val_loss: 0.4272 - val_accuracy: 0.7275 - val_recall: 0.8191 - val_precision: 0.2108\n",
      "Epoch 102/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4981 - accuracy: 0.7622 - recall: 0.8217 - precision: 0.7241 - val_loss: 0.4271 - val_accuracy: 0.7275 - val_recall: 0.8189 - val_precision: 0.2108\n",
      "Epoch 103/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4985 - accuracy: 0.7626 - recall: 0.8219 - precision: 0.7246 - val_loss: 0.4257 - val_accuracy: 0.7286 - val_recall: 0.8179 - val_precision: 0.2114\n",
      "Epoch 104/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4984 - accuracy: 0.7623 - recall: 0.8203 - precision: 0.7248 - val_loss: 0.4270 - val_accuracy: 0.7276 - val_recall: 0.8187 - val_precision: 0.2108\n",
      "Epoch 105/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4980 - accuracy: 0.7622 - recall: 0.8215 - precision: 0.7242 - val_loss: 0.4273 - val_accuracy: 0.7272 - val_recall: 0.8189 - val_precision: 0.2106\n",
      "Epoch 106/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4977 - accuracy: 0.7619 - recall: 0.8213 - precision: 0.7238 - val_loss: 0.4275 - val_accuracy: 0.7271 - val_recall: 0.8196 - val_precision: 0.2107\n",
      "Epoch 107/500\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.4981 - accuracy: 0.7624 - recall: 0.8210 - precision: 0.7246 - val_loss: 0.4274 - val_accuracy: 0.7272 - val_recall: 0.8196 - val_precision: 0.2107\n",
      "Epoch 108/500\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.4977 - accuracy: 0.7627 - recall: 0.8222 - precision: 0.7245 - val_loss: 0.4270 - val_accuracy: 0.7275 - val_recall: 0.8196 - val_precision: 0.2109\n",
      "Epoch 109/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4982 - accuracy: 0.7630 - recall: 0.8221 - precision: 0.7250 - val_loss: 0.4263 - val_accuracy: 0.7281 - val_recall: 0.8184 - val_precision: 0.2111\n",
      "Epoch 110/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4977 - accuracy: 0.7628 - recall: 0.8211 - precision: 0.7251 - val_loss: 0.4254 - val_accuracy: 0.7285 - val_recall: 0.8182 - val_precision: 0.2113\n",
      "Epoch 111/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4981 - accuracy: 0.7629 - recall: 0.8216 - precision: 0.7250 - val_loss: 0.4260 - val_accuracy: 0.7282 - val_recall: 0.8187 - val_precision: 0.2113\n",
      "Epoch 112/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4982 - accuracy: 0.7625 - recall: 0.8214 - precision: 0.7247 - val_loss: 0.4256 - val_accuracy: 0.7284 - val_recall: 0.8175 - val_precision: 0.2112\n",
      "Epoch 113/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4979 - accuracy: 0.7632 - recall: 0.8212 - precision: 0.7256 - val_loss: 0.4256 - val_accuracy: 0.7284 - val_recall: 0.8175 - val_precision: 0.2112\n",
      "Epoch 114/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4976 - accuracy: 0.7626 - recall: 0.8222 - precision: 0.7244 - val_loss: 0.4236 - val_accuracy: 0.7297 - val_recall: 0.8160 - val_precision: 0.2118\n",
      "Epoch 115/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4980 - accuracy: 0.7624 - recall: 0.8203 - precision: 0.7249 - val_loss: 0.4244 - val_accuracy: 0.7291 - val_recall: 0.8165 - val_precision: 0.2115\n",
      "Epoch 116/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4974 - accuracy: 0.7637 - recall: 0.8224 - precision: 0.7258 - val_loss: 0.4259 - val_accuracy: 0.7281 - val_recall: 0.8184 - val_precision: 0.2111\n",
      "Epoch 117/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4972 - accuracy: 0.7630 - recall: 0.8224 - precision: 0.7248 - val_loss: 0.4260 - val_accuracy: 0.7282 - val_recall: 0.8179 - val_precision: 0.2111\n",
      "Epoch 118/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4971 - accuracy: 0.7626 - recall: 0.8217 - precision: 0.7247 - val_loss: 0.4244 - val_accuracy: 0.7291 - val_recall: 0.8167 - val_precision: 0.2115\n",
      "Epoch 119/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4972 - accuracy: 0.7628 - recall: 0.8219 - precision: 0.7248 - val_loss: 0.4252 - val_accuracy: 0.7287 - val_recall: 0.8172 - val_precision: 0.2114\n",
      "Epoch 120/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4973 - accuracy: 0.7625 - recall: 0.8212 - precision: 0.7247 - val_loss: 0.4259 - val_accuracy: 0.7281 - val_recall: 0.8189 - val_precision: 0.2112\n",
      "Epoch 121/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4969 - accuracy: 0.7633 - recall: 0.8232 - precision: 0.7250 - val_loss: 0.4256 - val_accuracy: 0.7282 - val_recall: 0.8184 - val_precision: 0.2112\n",
      "Epoch 122/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4974 - accuracy: 0.7626 - recall: 0.8217 - precision: 0.7246 - val_loss: 0.4256 - val_accuracy: 0.7282 - val_recall: 0.8182 - val_precision: 0.2112\n",
      "Epoch 123/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4968 - accuracy: 0.7633 - recall: 0.8228 - precision: 0.7251 - val_loss: 0.4246 - val_accuracy: 0.7288 - val_recall: 0.8172 - val_precision: 0.2114\n",
      "Epoch 124/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4966 - accuracy: 0.7631 - recall: 0.8215 - precision: 0.7254 - val_loss: 0.4261 - val_accuracy: 0.7281 - val_recall: 0.8191 - val_precision: 0.2112\n",
      "Epoch 125/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4972 - accuracy: 0.7630 - recall: 0.8224 - precision: 0.7249 - val_loss: 0.4241 - val_accuracy: 0.7295 - val_recall: 0.8165 - val_precision: 0.2117\n",
      "Epoch 126/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4964 - accuracy: 0.7627 - recall: 0.8223 - precision: 0.7245 - val_loss: 0.4257 - val_accuracy: 0.7285 - val_recall: 0.8187 - val_precision: 0.2114\n",
      "Epoch 127/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4963 - accuracy: 0.7633 - recall: 0.8221 - precision: 0.7254 - val_loss: 0.4251 - val_accuracy: 0.7285 - val_recall: 0.8187 - val_precision: 0.2114\n",
      "Epoch 128/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4967 - accuracy: 0.7630 - recall: 0.8219 - precision: 0.7251 - val_loss: 0.4239 - val_accuracy: 0.7295 - val_recall: 0.8170 - val_precision: 0.2118\n",
      "Epoch 129/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4967 - accuracy: 0.7625 - recall: 0.8219 - precision: 0.7245 - val_loss: 0.4243 - val_accuracy: 0.7290 - val_recall: 0.8179 - val_precision: 0.2117\n",
      "Epoch 130/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4963 - accuracy: 0.7634 - recall: 0.8231 - precision: 0.7251 - val_loss: 0.4223 - val_accuracy: 0.7300 - val_recall: 0.8158 - val_precision: 0.2120\n",
      "Epoch 131/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4967 - accuracy: 0.7629 - recall: 0.8229 - precision: 0.7245 - val_loss: 0.4231 - val_accuracy: 0.7296 - val_recall: 0.8167 - val_precision: 0.2119\n",
      "Epoch 132/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4966 - accuracy: 0.7630 - recall: 0.8228 - precision: 0.7247 - val_loss: 0.4236 - val_accuracy: 0.7296 - val_recall: 0.8170 - val_precision: 0.2119\n",
      "Epoch 133/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4963 - accuracy: 0.7630 - recall: 0.8226 - precision: 0.7247 - val_loss: 0.4227 - val_accuracy: 0.7301 - val_recall: 0.8158 - val_precision: 0.2120\n",
      "Epoch 134/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4960 - accuracy: 0.7630 - recall: 0.8217 - precision: 0.7252 - val_loss: 0.4232 - val_accuracy: 0.7298 - val_recall: 0.8165 - val_precision: 0.2119\n",
      "Epoch 135/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4962 - accuracy: 0.7636 - recall: 0.8234 - precision: 0.7253 - val_loss: 0.4224 - val_accuracy: 0.7303 - val_recall: 0.8151 - val_precision: 0.2121\n",
      "Epoch 136/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4960 - accuracy: 0.7633 - recall: 0.8220 - precision: 0.7254 - val_loss: 0.4238 - val_accuracy: 0.7297 - val_recall: 0.8170 - val_precision: 0.2119\n",
      "Epoch 137/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4959 - accuracy: 0.7630 - recall: 0.8224 - precision: 0.7249 - val_loss: 0.4229 - val_accuracy: 0.7301 - val_recall: 0.8163 - val_precision: 0.2121\n",
      "Epoch 138/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4960 - accuracy: 0.7637 - recall: 0.8223 - precision: 0.7259 - val_loss: 0.4252 - val_accuracy: 0.7286 - val_recall: 0.8177 - val_precision: 0.2113\n",
      "Epoch 139/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4963 - accuracy: 0.7631 - recall: 0.8230 - precision: 0.7247 - val_loss: 0.4234 - val_accuracy: 0.7299 - val_recall: 0.8170 - val_precision: 0.2121\n",
      "Epoch 140/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4963 - accuracy: 0.7635 - recall: 0.8229 - precision: 0.7254 - val_loss: 0.4230 - val_accuracy: 0.7303 - val_recall: 0.8163 - val_precision: 0.2123\n",
      "Epoch 141/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4958 - accuracy: 0.7633 - recall: 0.8223 - precision: 0.7254 - val_loss: 0.4221 - val_accuracy: 0.7308 - val_recall: 0.8158 - val_precision: 0.2125\n",
      "Epoch 142/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4961 - accuracy: 0.7631 - recall: 0.8221 - precision: 0.7252 - val_loss: 0.4234 - val_accuracy: 0.7300 - val_recall: 0.8167 - val_precision: 0.2121\n",
      "Epoch 143/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4960 - accuracy: 0.7630 - recall: 0.8217 - precision: 0.7251 - val_loss: 0.4225 - val_accuracy: 0.7307 - val_recall: 0.8158 - val_precision: 0.2124\n",
      "Epoch 144/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4956 - accuracy: 0.7635 - recall: 0.8226 - precision: 0.7255 - val_loss: 0.4230 - val_accuracy: 0.7305 - val_recall: 0.8167 - val_precision: 0.2124\n",
      "Epoch 145/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4959 - accuracy: 0.7630 - recall: 0.8223 - precision: 0.7250 - val_loss: 0.4233 - val_accuracy: 0.7301 - val_recall: 0.8167 - val_precision: 0.2122\n",
      "Epoch 146/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4958 - accuracy: 0.7632 - recall: 0.8225 - precision: 0.7251 - val_loss: 0.4233 - val_accuracy: 0.7301 - val_recall: 0.8167 - val_precision: 0.2121\n",
      "Epoch 147/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4957 - accuracy: 0.7636 - recall: 0.8222 - precision: 0.7257 - val_loss: 0.4225 - val_accuracy: 0.7305 - val_recall: 0.8165 - val_precision: 0.2124\n",
      "Epoch 148/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4957 - accuracy: 0.7635 - recall: 0.8236 - precision: 0.7251 - val_loss: 0.4220 - val_accuracy: 0.7307 - val_recall: 0.8163 - val_precision: 0.2125\n",
      "Epoch 149/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4954 - accuracy: 0.7638 - recall: 0.8229 - precision: 0.7258 - val_loss: 0.4234 - val_accuracy: 0.7297 - val_recall: 0.8167 - val_precision: 0.2119\n",
      "Epoch 150/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4955 - accuracy: 0.7635 - recall: 0.8228 - precision: 0.7254 - val_loss: 0.4223 - val_accuracy: 0.7307 - val_recall: 0.8165 - val_precision: 0.2126\n",
      "Epoch 151/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4956 - accuracy: 0.7629 - recall: 0.8226 - precision: 0.7247 - val_loss: 0.4222 - val_accuracy: 0.7307 - val_recall: 0.8163 - val_precision: 0.2125\n",
      "Epoch 152/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4955 - accuracy: 0.7632 - recall: 0.8226 - precision: 0.7251 - val_loss: 0.4220 - val_accuracy: 0.7309 - val_recall: 0.8163 - val_precision: 0.2126\n",
      "Epoch 153/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4956 - accuracy: 0.7632 - recall: 0.8233 - precision: 0.7249 - val_loss: 0.4226 - val_accuracy: 0.7306 - val_recall: 0.8163 - val_precision: 0.2124\n",
      "Epoch 154/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4955 - accuracy: 0.7637 - recall: 0.8228 - precision: 0.7257 - val_loss: 0.4228 - val_accuracy: 0.7311 - val_recall: 0.8163 - val_precision: 0.2127\n",
      "Epoch 155/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4954 - accuracy: 0.7632 - recall: 0.8229 - precision: 0.7250 - val_loss: 0.4230 - val_accuracy: 0.7307 - val_recall: 0.8165 - val_precision: 0.2126\n",
      "Epoch 156/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4956 - accuracy: 0.7635 - recall: 0.8224 - precision: 0.7255 - val_loss: 0.4224 - val_accuracy: 0.7312 - val_recall: 0.8158 - val_precision: 0.2128\n",
      "Epoch 157/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4954 - accuracy: 0.7639 - recall: 0.8228 - precision: 0.7259 - val_loss: 0.4223 - val_accuracy: 0.7315 - val_recall: 0.8153 - val_precision: 0.2129\n",
      "Epoch 158/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4955 - accuracy: 0.7639 - recall: 0.8223 - precision: 0.7261 - val_loss: 0.4221 - val_accuracy: 0.7317 - val_recall: 0.8151 - val_precision: 0.2130\n",
      "Epoch 159/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4954 - accuracy: 0.7637 - recall: 0.8233 - precision: 0.7255 - val_loss: 0.4214 - val_accuracy: 0.7319 - val_recall: 0.8146 - val_precision: 0.2131\n",
      "Epoch 160/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4953 - accuracy: 0.7640 - recall: 0.8232 - precision: 0.7258 - val_loss: 0.4213 - val_accuracy: 0.7320 - val_recall: 0.8141 - val_precision: 0.2131\n",
      "Epoch 161/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4955 - accuracy: 0.7640 - recall: 0.8225 - precision: 0.7262 - val_loss: 0.4222 - val_accuracy: 0.7313 - val_recall: 0.8148 - val_precision: 0.2127\n",
      "Epoch 162/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4954 - accuracy: 0.7635 - recall: 0.8228 - precision: 0.7254 - val_loss: 0.4217 - val_accuracy: 0.7316 - val_recall: 0.8141 - val_precision: 0.2128\n",
      "Epoch 163/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4952 - accuracy: 0.7640 - recall: 0.8236 - precision: 0.7257 - val_loss: 0.4200 - val_accuracy: 0.7328 - val_recall: 0.8129 - val_precision: 0.2134\n",
      "Epoch 164/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4953 - accuracy: 0.7637 - recall: 0.8215 - precision: 0.7262 - val_loss: 0.4231 - val_accuracy: 0.7310 - val_recall: 0.8165 - val_precision: 0.2128\n",
      "Epoch 165/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4947 - accuracy: 0.7637 - recall: 0.8234 - precision: 0.7254 - val_loss: 0.4212 - val_accuracy: 0.7320 - val_recall: 0.8136 - val_precision: 0.2130\n",
      "Epoch 166/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4953 - accuracy: 0.7634 - recall: 0.8229 - precision: 0.7252 - val_loss: 0.4213 - val_accuracy: 0.7321 - val_recall: 0.8136 - val_precision: 0.2131\n",
      "Epoch 167/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4950 - accuracy: 0.7640 - recall: 0.8230 - precision: 0.7260 - val_loss: 0.4231 - val_accuracy: 0.7311 - val_recall: 0.8146 - val_precision: 0.2125\n",
      "Epoch 168/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4945 - accuracy: 0.7638 - recall: 0.8234 - precision: 0.7255 - val_loss: 0.4215 - val_accuracy: 0.7324 - val_recall: 0.8139 - val_precision: 0.2133\n",
      "Epoch 169/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4948 - accuracy: 0.7637 - recall: 0.8225 - precision: 0.7258 - val_loss: 0.4210 - val_accuracy: 0.7327 - val_recall: 0.8131 - val_precision: 0.2133\n",
      "Epoch 170/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4951 - accuracy: 0.7644 - recall: 0.8231 - precision: 0.7265 - val_loss: 0.4206 - val_accuracy: 0.7331 - val_recall: 0.8127 - val_precision: 0.2136\n",
      "Epoch 171/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4949 - accuracy: 0.7635 - recall: 0.8221 - precision: 0.7257 - val_loss: 0.4206 - val_accuracy: 0.7326 - val_recall: 0.8134 - val_precision: 0.2134\n",
      "Epoch 172/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4950 - accuracy: 0.7633 - recall: 0.8222 - precision: 0.7253 - val_loss: 0.4203 - val_accuracy: 0.7327 - val_recall: 0.8129 - val_precision: 0.2133\n",
      "Epoch 173/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4947 - accuracy: 0.7644 - recall: 0.8230 - precision: 0.7265 - val_loss: 0.4216 - val_accuracy: 0.7323 - val_recall: 0.8131 - val_precision: 0.2131\n",
      "Epoch 174/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4950 - accuracy: 0.7637 - recall: 0.8232 - precision: 0.7255 - val_loss: 0.4204 - val_accuracy: 0.7333 - val_recall: 0.8117 - val_precision: 0.2135\n",
      "Epoch 175/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4948 - accuracy: 0.7635 - recall: 0.8223 - precision: 0.7255 - val_loss: 0.4205 - val_accuracy: 0.7335 - val_recall: 0.8117 - val_precision: 0.2137\n",
      "Epoch 176/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4945 - accuracy: 0.7640 - recall: 0.8235 - precision: 0.7257 - val_loss: 0.4210 - val_accuracy: 0.7330 - val_recall: 0.8119 - val_precision: 0.2134\n",
      "Epoch 177/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4948 - accuracy: 0.7637 - recall: 0.8221 - precision: 0.7260 - val_loss: 0.4227 - val_accuracy: 0.7318 - val_recall: 0.8139 - val_precision: 0.2129\n",
      "Epoch 178/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4943 - accuracy: 0.7643 - recall: 0.8227 - precision: 0.7265 - val_loss: 0.4209 - val_accuracy: 0.7332 - val_recall: 0.8119 - val_precision: 0.2135\n",
      "Epoch 179/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4944 - accuracy: 0.7643 - recall: 0.8226 - precision: 0.7265 - val_loss: 0.4222 - val_accuracy: 0.7324 - val_recall: 0.8134 - val_precision: 0.2132\n",
      "Epoch 180/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4943 - accuracy: 0.7641 - recall: 0.8238 - precision: 0.7258 - val_loss: 0.4196 - val_accuracy: 0.7342 - val_recall: 0.8112 - val_precision: 0.2141\n",
      "Epoch 181/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4942 - accuracy: 0.7642 - recall: 0.8225 - precision: 0.7265 - val_loss: 0.4202 - val_accuracy: 0.7337 - val_recall: 0.8122 - val_precision: 0.2139\n",
      "Epoch 182/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4947 - accuracy: 0.7638 - recall: 0.8228 - precision: 0.7258 - val_loss: 0.4192 - val_accuracy: 0.7347 - val_recall: 0.8110 - val_precision: 0.2144\n",
      "Epoch 183/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4946 - accuracy: 0.7638 - recall: 0.8219 - precision: 0.7261 - val_loss: 0.4209 - val_accuracy: 0.7336 - val_recall: 0.8117 - val_precision: 0.2137\n",
      "Epoch 184/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4945 - accuracy: 0.7639 - recall: 0.8230 - precision: 0.7259 - val_loss: 0.4192 - val_accuracy: 0.7351 - val_recall: 0.8105 - val_precision: 0.2146\n",
      "Epoch 185/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4942 - accuracy: 0.7642 - recall: 0.8222 - precision: 0.7265 - val_loss: 0.4194 - val_accuracy: 0.7350 - val_recall: 0.8107 - val_precision: 0.2146\n",
      "Epoch 186/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4943 - accuracy: 0.7638 - recall: 0.8224 - precision: 0.7259 - val_loss: 0.4198 - val_accuracy: 0.7346 - val_recall: 0.8110 - val_precision: 0.2143\n",
      "Epoch 187/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4942 - accuracy: 0.7643 - recall: 0.8226 - precision: 0.7265 - val_loss: 0.4201 - val_accuracy: 0.7349 - val_recall: 0.8110 - val_precision: 0.2145\n",
      "Epoch 188/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4945 - accuracy: 0.7641 - recall: 0.8219 - precision: 0.7265 - val_loss: 0.4210 - val_accuracy: 0.7343 - val_recall: 0.8117 - val_precision: 0.2142\n",
      "Epoch 189/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4941 - accuracy: 0.7640 - recall: 0.8231 - precision: 0.7259 - val_loss: 0.4195 - val_accuracy: 0.7356 - val_recall: 0.8103 - val_precision: 0.2149\n",
      "Epoch 190/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4945 - accuracy: 0.7642 - recall: 0.8221 - precision: 0.7266 - val_loss: 0.4205 - val_accuracy: 0.7349 - val_recall: 0.8107 - val_precision: 0.2145\n",
      "Epoch 191/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4945 - accuracy: 0.7641 - recall: 0.8223 - precision: 0.7264 - val_loss: 0.4203 - val_accuracy: 0.7351 - val_recall: 0.8107 - val_precision: 0.2146\n",
      "Epoch 192/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4942 - accuracy: 0.7642 - recall: 0.8226 - precision: 0.7264 - val_loss: 0.4202 - val_accuracy: 0.7352 - val_recall: 0.8107 - val_precision: 0.2147\n",
      "Epoch 193/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4945 - accuracy: 0.7644 - recall: 0.8225 - precision: 0.7267 - val_loss: 0.4189 - val_accuracy: 0.7362 - val_recall: 0.8093 - val_precision: 0.2151\n",
      "Epoch 194/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4939 - accuracy: 0.7645 - recall: 0.8227 - precision: 0.7267 - val_loss: 0.4183 - val_accuracy: 0.7365 - val_recall: 0.8093 - val_precision: 0.2153\n",
      "Epoch 195/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4944 - accuracy: 0.7645 - recall: 0.8220 - precision: 0.7271 - val_loss: 0.4200 - val_accuracy: 0.7357 - val_recall: 0.8098 - val_precision: 0.2148\n",
      "Epoch 196/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4944 - accuracy: 0.7644 - recall: 0.8220 - precision: 0.7269 - val_loss: 0.4206 - val_accuracy: 0.7353 - val_recall: 0.8107 - val_precision: 0.2147\n",
      "Epoch 197/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4937 - accuracy: 0.7643 - recall: 0.8228 - precision: 0.7264 - val_loss: 0.4198 - val_accuracy: 0.7359 - val_recall: 0.8095 - val_precision: 0.2150\n",
      "Epoch 198/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4934 - accuracy: 0.7648 - recall: 0.8227 - precision: 0.7272 - val_loss: 0.4193 - val_accuracy: 0.7361 - val_recall: 0.8095 - val_precision: 0.2151\n",
      "Epoch 199/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4940 - accuracy: 0.7642 - recall: 0.8218 - precision: 0.7267 - val_loss: 0.4201 - val_accuracy: 0.7356 - val_recall: 0.8107 - val_precision: 0.2150\n",
      "Epoch 200/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4935 - accuracy: 0.7648 - recall: 0.8228 - precision: 0.7271 - val_loss: 0.4200 - val_accuracy: 0.7359 - val_recall: 0.8105 - val_precision: 0.2151\n",
      "Epoch 201/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4936 - accuracy: 0.7645 - recall: 0.8227 - precision: 0.7267 - val_loss: 0.4188 - val_accuracy: 0.7365 - val_recall: 0.8091 - val_precision: 0.2153\n",
      "Epoch 202/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4941 - accuracy: 0.7641 - recall: 0.8216 - precision: 0.7266 - val_loss: 0.4194 - val_accuracy: 0.7363 - val_recall: 0.8095 - val_precision: 0.2153\n",
      "Epoch 203/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4939 - accuracy: 0.7643 - recall: 0.8213 - precision: 0.7270 - val_loss: 0.4195 - val_accuracy: 0.7364 - val_recall: 0.8095 - val_precision: 0.2153\n",
      "Epoch 204/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4940 - accuracy: 0.7645 - recall: 0.8220 - precision: 0.7270 - val_loss: 0.4201 - val_accuracy: 0.7361 - val_recall: 0.8103 - val_precision: 0.2152\n",
      "Epoch 205/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4941 - accuracy: 0.7643 - recall: 0.8217 - precision: 0.7268 - val_loss: 0.4188 - val_accuracy: 0.7367 - val_recall: 0.8088 - val_precision: 0.2154\n",
      "Epoch 206/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4937 - accuracy: 0.7642 - recall: 0.8211 - precision: 0.7270 - val_loss: 0.4194 - val_accuracy: 0.7363 - val_recall: 0.8093 - val_precision: 0.2152\n",
      "Epoch 207/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4937 - accuracy: 0.7641 - recall: 0.8215 - precision: 0.7267 - val_loss: 0.4185 - val_accuracy: 0.7371 - val_recall: 0.8088 - val_precision: 0.2157\n",
      "Epoch 208/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4940 - accuracy: 0.7640 - recall: 0.8211 - precision: 0.7267 - val_loss: 0.4204 - val_accuracy: 0.7362 - val_recall: 0.8098 - val_precision: 0.2152\n",
      "Epoch 209/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4940 - accuracy: 0.7643 - recall: 0.8217 - precision: 0.7269 - val_loss: 0.4191 - val_accuracy: 0.7370 - val_recall: 0.8081 - val_precision: 0.2155\n",
      "Epoch 210/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4939 - accuracy: 0.7644 - recall: 0.8218 - precision: 0.7270 - val_loss: 0.4186 - val_accuracy: 0.7370 - val_recall: 0.8081 - val_precision: 0.2155\n",
      "Epoch 211/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4935 - accuracy: 0.7648 - recall: 0.8223 - precision: 0.7273 - val_loss: 0.4196 - val_accuracy: 0.7366 - val_recall: 0.8093 - val_precision: 0.2154\n",
      "Epoch 212/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4934 - accuracy: 0.7643 - recall: 0.8213 - precision: 0.7270 - val_loss: 0.4194 - val_accuracy: 0.7368 - val_recall: 0.8093 - val_precision: 0.2155\n",
      "Epoch 213/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4936 - accuracy: 0.7641 - recall: 0.8221 - precision: 0.7265 - val_loss: 0.4194 - val_accuracy: 0.7369 - val_recall: 0.8088 - val_precision: 0.2156\n",
      "Epoch 214/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4935 - accuracy: 0.7641 - recall: 0.8211 - precision: 0.7269 - val_loss: 0.4199 - val_accuracy: 0.7365 - val_recall: 0.8091 - val_precision: 0.2153\n",
      "Epoch 215/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4936 - accuracy: 0.7644 - recall: 0.8210 - precision: 0.7273 - val_loss: 0.4205 - val_accuracy: 0.7362 - val_recall: 0.8100 - val_precision: 0.2152\n",
      "Epoch 216/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4935 - accuracy: 0.7644 - recall: 0.8229 - precision: 0.7266 - val_loss: 0.4198 - val_accuracy: 0.7370 - val_recall: 0.8088 - val_precision: 0.2156\n",
      "Epoch 217/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4932 - accuracy: 0.7642 - recall: 0.8213 - precision: 0.7269 - val_loss: 0.4182 - val_accuracy: 0.7382 - val_recall: 0.8071 - val_precision: 0.2162\n",
      "Epoch 218/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4939 - accuracy: 0.7645 - recall: 0.8207 - precision: 0.7276 - val_loss: 0.4192 - val_accuracy: 0.7376 - val_recall: 0.8083 - val_precision: 0.2159\n",
      "Epoch 219/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4933 - accuracy: 0.7642 - recall: 0.8215 - precision: 0.7269 - val_loss: 0.4184 - val_accuracy: 0.7380 - val_recall: 0.8079 - val_precision: 0.2162\n",
      "Epoch 220/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4930 - accuracy: 0.7642 - recall: 0.8207 - precision: 0.7271 - val_loss: 0.4202 - val_accuracy: 0.7368 - val_recall: 0.8086 - val_precision: 0.2155\n",
      "Epoch 221/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4930 - accuracy: 0.7648 - recall: 0.8221 - precision: 0.7274 - val_loss: 0.4180 - val_accuracy: 0.7383 - val_recall: 0.8071 - val_precision: 0.2163\n",
      "Epoch 222/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4933 - accuracy: 0.7645 - recall: 0.8207 - precision: 0.7276 - val_loss: 0.4196 - val_accuracy: 0.7378 - val_recall: 0.8083 - val_precision: 0.2161\n",
      "Epoch 223/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4934 - accuracy: 0.7648 - recall: 0.8217 - precision: 0.7276 - val_loss: 0.4192 - val_accuracy: 0.7377 - val_recall: 0.8083 - val_precision: 0.2160\n",
      "Epoch 224/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4933 - accuracy: 0.7645 - recall: 0.8215 - precision: 0.7272 - val_loss: 0.4187 - val_accuracy: 0.7381 - val_recall: 0.8071 - val_precision: 0.2161\n",
      "Epoch 225/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4930 - accuracy: 0.7652 - recall: 0.8222 - precision: 0.7278 - val_loss: 0.4193 - val_accuracy: 0.7379 - val_recall: 0.8081 - val_precision: 0.2161\n",
      "Epoch 226/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4929 - accuracy: 0.7648 - recall: 0.8213 - precision: 0.7278 - val_loss: 0.4196 - val_accuracy: 0.7379 - val_recall: 0.8083 - val_precision: 0.2162\n",
      "Epoch 227/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4935 - accuracy: 0.7649 - recall: 0.8216 - precision: 0.7278 - val_loss: 0.4193 - val_accuracy: 0.7383 - val_recall: 0.8081 - val_precision: 0.2164\n",
      "Epoch 228/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4931 - accuracy: 0.7648 - recall: 0.8213 - precision: 0.7278 - val_loss: 0.4193 - val_accuracy: 0.7384 - val_recall: 0.8079 - val_precision: 0.2164\n",
      "Epoch 229/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4932 - accuracy: 0.7641 - recall: 0.8205 - precision: 0.7271 - val_loss: 0.4195 - val_accuracy: 0.7381 - val_recall: 0.8081 - val_precision: 0.2162\n",
      "Epoch 230/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4931 - accuracy: 0.7646 - recall: 0.8211 - precision: 0.7275 - val_loss: 0.4193 - val_accuracy: 0.7384 - val_recall: 0.8081 - val_precision: 0.2165\n",
      "Epoch 231/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4932 - accuracy: 0.7646 - recall: 0.8211 - precision: 0.7275 - val_loss: 0.4187 - val_accuracy: 0.7391 - val_recall: 0.8067 - val_precision: 0.2167\n",
      "Epoch 232/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4932 - accuracy: 0.7649 - recall: 0.8218 - precision: 0.7276 - val_loss: 0.4182 - val_accuracy: 0.7397 - val_recall: 0.8059 - val_precision: 0.2171\n",
      "Epoch 233/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4933 - accuracy: 0.7649 - recall: 0.8211 - precision: 0.7279 - val_loss: 0.4183 - val_accuracy: 0.7396 - val_recall: 0.8057 - val_precision: 0.2169\n",
      "Epoch 234/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4931 - accuracy: 0.7648 - recall: 0.8211 - precision: 0.7278 - val_loss: 0.4188 - val_accuracy: 0.7393 - val_recall: 0.8064 - val_precision: 0.2168\n",
      "Epoch 235/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4931 - accuracy: 0.7646 - recall: 0.8211 - precision: 0.7275 - val_loss: 0.4193 - val_accuracy: 0.7389 - val_recall: 0.8076 - val_precision: 0.2167\n",
      "Epoch 236/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4930 - accuracy: 0.7645 - recall: 0.8209 - precision: 0.7274 - val_loss: 0.4185 - val_accuracy: 0.7397 - val_recall: 0.8059 - val_precision: 0.2170\n",
      "Epoch 237/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4932 - accuracy: 0.7650 - recall: 0.8217 - precision: 0.7277 - val_loss: 0.4178 - val_accuracy: 0.7403 - val_recall: 0.8052 - val_precision: 0.2174\n",
      "Epoch 238/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4932 - accuracy: 0.7648 - recall: 0.8201 - precision: 0.7282 - val_loss: 0.4184 - val_accuracy: 0.7399 - val_recall: 0.8057 - val_precision: 0.2171\n",
      "Epoch 239/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4930 - accuracy: 0.7645 - recall: 0.8205 - precision: 0.7277 - val_loss: 0.4192 - val_accuracy: 0.7397 - val_recall: 0.8055 - val_precision: 0.2170\n",
      "Epoch 240/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4929 - accuracy: 0.7646 - recall: 0.8203 - precision: 0.7278 - val_loss: 0.4193 - val_accuracy: 0.7396 - val_recall: 0.8059 - val_precision: 0.2169\n",
      "Epoch 241/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4930 - accuracy: 0.7646 - recall: 0.8203 - precision: 0.7279 - val_loss: 0.4182 - val_accuracy: 0.7407 - val_recall: 0.8055 - val_precision: 0.2177\n",
      "Epoch 242/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4925 - accuracy: 0.7656 - recall: 0.8215 - precision: 0.7287 - val_loss: 0.4186 - val_accuracy: 0.7406 - val_recall: 0.8052 - val_precision: 0.2175\n",
      "Epoch 243/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4932 - accuracy: 0.7647 - recall: 0.8205 - precision: 0.7280 - val_loss: 0.4193 - val_accuracy: 0.7403 - val_recall: 0.8055 - val_precision: 0.2174\n",
      "Epoch 244/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4933 - accuracy: 0.7650 - recall: 0.8210 - precision: 0.7281 - val_loss: 0.4177 - val_accuracy: 0.7411 - val_recall: 0.8047 - val_precision: 0.2178\n",
      "Epoch 245/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4923 - accuracy: 0.7650 - recall: 0.8202 - precision: 0.7284 - val_loss: 0.4184 - val_accuracy: 0.7407 - val_recall: 0.8052 - val_precision: 0.2177\n",
      "Epoch 246/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4927 - accuracy: 0.7649 - recall: 0.8204 - precision: 0.7282 - val_loss: 0.4193 - val_accuracy: 0.7401 - val_recall: 0.8055 - val_precision: 0.2172\n",
      "Epoch 247/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4926 - accuracy: 0.7649 - recall: 0.8210 - precision: 0.7280 - val_loss: 0.4186 - val_accuracy: 0.7408 - val_recall: 0.8052 - val_precision: 0.2177\n",
      "Epoch 248/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4928 - accuracy: 0.7652 - recall: 0.8211 - precision: 0.7284 - val_loss: 0.4183 - val_accuracy: 0.7408 - val_recall: 0.8052 - val_precision: 0.2177\n",
      "Epoch 249/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4928 - accuracy: 0.7647 - recall: 0.8201 - precision: 0.7281 - val_loss: 0.4190 - val_accuracy: 0.7403 - val_recall: 0.8052 - val_precision: 0.2174\n",
      "Epoch 250/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4925 - accuracy: 0.7648 - recall: 0.8211 - precision: 0.7278 - val_loss: 0.4183 - val_accuracy: 0.7409 - val_recall: 0.8052 - val_precision: 0.2178\n",
      "Epoch 251/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4928 - accuracy: 0.7649 - recall: 0.8208 - precision: 0.7281 - val_loss: 0.4188 - val_accuracy: 0.7408 - val_recall: 0.8050 - val_precision: 0.2176\n",
      "Epoch 252/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4927 - accuracy: 0.7650 - recall: 0.8204 - precision: 0.7284 - val_loss: 0.4183 - val_accuracy: 0.7412 - val_recall: 0.8045 - val_precision: 0.2179\n",
      "Epoch 253/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4925 - accuracy: 0.7649 - recall: 0.8196 - precision: 0.7286 - val_loss: 0.4178 - val_accuracy: 0.7417 - val_recall: 0.8045 - val_precision: 0.2182\n",
      "Epoch 254/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4926 - accuracy: 0.7652 - recall: 0.8204 - precision: 0.7286 - val_loss: 0.4187 - val_accuracy: 0.7414 - val_recall: 0.8045 - val_precision: 0.2180\n",
      "Epoch 255/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4926 - accuracy: 0.7648 - recall: 0.8195 - precision: 0.7285 - val_loss: 0.4190 - val_accuracy: 0.7410 - val_recall: 0.8047 - val_precision: 0.2178\n",
      "Epoch 256/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4926 - accuracy: 0.7650 - recall: 0.8204 - precision: 0.7283 - val_loss: 0.4180 - val_accuracy: 0.7416 - val_recall: 0.8043 - val_precision: 0.2181\n",
      "Epoch 257/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4924 - accuracy: 0.7650 - recall: 0.8207 - precision: 0.7283 - val_loss: 0.4181 - val_accuracy: 0.7416 - val_recall: 0.8043 - val_precision: 0.2181\n",
      "Epoch 258/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4927 - accuracy: 0.7654 - recall: 0.8207 - precision: 0.7287 - val_loss: 0.4177 - val_accuracy: 0.7418 - val_recall: 0.8043 - val_precision: 0.2183\n",
      "Epoch 259/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4925 - accuracy: 0.7654 - recall: 0.8203 - precision: 0.7289 - val_loss: 0.4182 - val_accuracy: 0.7417 - val_recall: 0.8045 - val_precision: 0.2182\n",
      "Epoch 260/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4927 - accuracy: 0.7650 - recall: 0.8204 - precision: 0.7284 - val_loss: 0.4177 - val_accuracy: 0.7417 - val_recall: 0.8043 - val_precision: 0.2182\n",
      "Epoch 261/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4926 - accuracy: 0.7648 - recall: 0.8204 - precision: 0.7280 - val_loss: 0.4170 - val_accuracy: 0.7423 - val_recall: 0.8038 - val_precision: 0.2185\n",
      "Epoch 262/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4923 - accuracy: 0.7649 - recall: 0.8205 - precision: 0.7282 - val_loss: 0.4165 - val_accuracy: 0.7427 - val_recall: 0.8033 - val_precision: 0.2187\n",
      "Epoch 263/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4922 - accuracy: 0.7655 - recall: 0.8212 - precision: 0.7288 - val_loss: 0.4176 - val_accuracy: 0.7422 - val_recall: 0.8040 - val_precision: 0.2185\n",
      "Epoch 264/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4923 - accuracy: 0.7652 - recall: 0.8202 - precision: 0.7287 - val_loss: 0.4176 - val_accuracy: 0.7421 - val_recall: 0.8040 - val_precision: 0.2184\n",
      "Epoch 265/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4922 - accuracy: 0.7652 - recall: 0.8200 - precision: 0.7289 - val_loss: 0.4174 - val_accuracy: 0.7428 - val_recall: 0.8033 - val_precision: 0.2188\n",
      "Epoch 266/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4924 - accuracy: 0.7651 - recall: 0.8191 - precision: 0.7289 - val_loss: 0.4178 - val_accuracy: 0.7424 - val_recall: 0.8036 - val_precision: 0.2186\n",
      "Epoch 267/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4926 - accuracy: 0.7649 - recall: 0.8198 - precision: 0.7284 - val_loss: 0.4177 - val_accuracy: 0.7424 - val_recall: 0.8038 - val_precision: 0.2186\n",
      "Epoch 268/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4924 - accuracy: 0.7650 - recall: 0.8196 - precision: 0.7287 - val_loss: 0.4187 - val_accuracy: 0.7417 - val_recall: 0.8045 - val_precision: 0.2182\n",
      "Epoch 269/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4924 - accuracy: 0.7653 - recall: 0.8210 - precision: 0.7284 - val_loss: 0.4179 - val_accuracy: 0.7424 - val_recall: 0.8033 - val_precision: 0.2185\n",
      "Epoch 270/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4928 - accuracy: 0.7649 - recall: 0.8201 - precision: 0.7283 - val_loss: 0.4174 - val_accuracy: 0.7426 - val_recall: 0.8033 - val_precision: 0.2186\n",
      "Epoch 271/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4923 - accuracy: 0.7651 - recall: 0.8201 - precision: 0.7287 - val_loss: 0.4172 - val_accuracy: 0.7429 - val_recall: 0.8031 - val_precision: 0.2189\n",
      "Epoch 272/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4925 - accuracy: 0.7645 - recall: 0.8194 - precision: 0.7282 - val_loss: 0.4183 - val_accuracy: 0.7424 - val_recall: 0.8038 - val_precision: 0.2186\n",
      "Epoch 273/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4922 - accuracy: 0.7650 - recall: 0.8200 - precision: 0.7286 - val_loss: 0.4175 - val_accuracy: 0.7430 - val_recall: 0.8031 - val_precision: 0.2189\n",
      "Epoch 274/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4923 - accuracy: 0.7651 - recall: 0.8196 - precision: 0.7288 - val_loss: 0.4176 - val_accuracy: 0.7430 - val_recall: 0.8031 - val_precision: 0.2189\n",
      "Epoch 275/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4926 - accuracy: 0.7653 - recall: 0.8203 - precision: 0.7288 - val_loss: 0.4173 - val_accuracy: 0.7433 - val_recall: 0.8026 - val_precision: 0.2191\n",
      "Epoch 276/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4924 - accuracy: 0.7650 - recall: 0.8190 - precision: 0.7289 - val_loss: 0.4169 - val_accuracy: 0.7437 - val_recall: 0.8026 - val_precision: 0.2193\n",
      "Epoch 277/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4921 - accuracy: 0.7653 - recall: 0.8201 - precision: 0.7289 - val_loss: 0.4184 - val_accuracy: 0.7428 - val_recall: 0.8031 - val_precision: 0.2188\n",
      "Epoch 278/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4924 - accuracy: 0.7650 - recall: 0.8198 - precision: 0.7285 - val_loss: 0.4179 - val_accuracy: 0.7428 - val_recall: 0.8031 - val_precision: 0.2188\n",
      "Epoch 279/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4922 - accuracy: 0.7651 - recall: 0.8203 - precision: 0.7285 - val_loss: 0.4176 - val_accuracy: 0.7430 - val_recall: 0.8031 - val_precision: 0.2189\n",
      "Epoch 280/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4917 - accuracy: 0.7655 - recall: 0.8197 - precision: 0.7293 - val_loss: 0.4175 - val_accuracy: 0.7429 - val_recall: 0.8031 - val_precision: 0.2188\n",
      "Epoch 281/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4923 - accuracy: 0.7656 - recall: 0.8200 - precision: 0.7294 - val_loss: 0.4172 - val_accuracy: 0.7431 - val_recall: 0.8033 - val_precision: 0.2190\n",
      "Epoch 282/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4921 - accuracy: 0.7654 - recall: 0.8199 - precision: 0.7290 - val_loss: 0.4173 - val_accuracy: 0.7432 - val_recall: 0.8033 - val_precision: 0.2191\n",
      "Epoch 283/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4917 - accuracy: 0.7648 - recall: 0.8199 - precision: 0.7284 - val_loss: 0.4177 - val_accuracy: 0.7432 - val_recall: 0.8033 - val_precision: 0.2191\n",
      "Epoch 284/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4921 - accuracy: 0.7652 - recall: 0.8194 - precision: 0.7290 - val_loss: 0.4169 - val_accuracy: 0.7438 - val_recall: 0.8021 - val_precision: 0.2193\n",
      "Epoch 285/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4921 - accuracy: 0.7651 - recall: 0.8198 - precision: 0.7288 - val_loss: 0.4168 - val_accuracy: 0.7440 - val_recall: 0.8024 - val_precision: 0.2196\n",
      "Epoch 286/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4925 - accuracy: 0.7651 - recall: 0.8187 - precision: 0.7291 - val_loss: 0.4174 - val_accuracy: 0.7435 - val_recall: 0.8028 - val_precision: 0.2192\n",
      "Epoch 287/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4922 - accuracy: 0.7649 - recall: 0.8195 - precision: 0.7286 - val_loss: 0.4180 - val_accuracy: 0.7432 - val_recall: 0.8036 - val_precision: 0.2191\n",
      "Epoch 288/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4923 - accuracy: 0.7652 - recall: 0.8196 - precision: 0.7289 - val_loss: 0.4171 - val_accuracy: 0.7435 - val_recall: 0.8024 - val_precision: 0.2192\n",
      "Epoch 289/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4920 - accuracy: 0.7656 - recall: 0.8198 - precision: 0.7293 - val_loss: 0.4172 - val_accuracy: 0.7434 - val_recall: 0.8024 - val_precision: 0.2191\n",
      "Epoch 290/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4919 - accuracy: 0.7657 - recall: 0.8200 - precision: 0.7294 - val_loss: 0.4173 - val_accuracy: 0.7435 - val_recall: 0.8024 - val_precision: 0.2192\n",
      "Epoch 291/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4920 - accuracy: 0.7650 - recall: 0.8194 - precision: 0.7288 - val_loss: 0.4169 - val_accuracy: 0.7440 - val_recall: 0.8021 - val_precision: 0.2195\n",
      "Epoch 292/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4922 - accuracy: 0.7655 - recall: 0.8198 - precision: 0.7292 - val_loss: 0.4171 - val_accuracy: 0.7438 - val_recall: 0.8019 - val_precision: 0.2193\n",
      "Epoch 293/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4918 - accuracy: 0.7654 - recall: 0.8189 - precision: 0.7294 - val_loss: 0.4161 - val_accuracy: 0.7445 - val_recall: 0.8021 - val_precision: 0.2199\n",
      "Epoch 294/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4922 - accuracy: 0.7652 - recall: 0.8200 - precision: 0.7288 - val_loss: 0.4171 - val_accuracy: 0.7440 - val_recall: 0.8024 - val_precision: 0.2195\n",
      "Epoch 295/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4918 - accuracy: 0.7655 - recall: 0.8191 - precision: 0.7295 - val_loss: 0.4175 - val_accuracy: 0.7437 - val_recall: 0.8024 - val_precision: 0.2193\n",
      "Epoch 296/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4915 - accuracy: 0.7652 - recall: 0.8189 - precision: 0.7293 - val_loss: 0.4175 - val_accuracy: 0.7440 - val_recall: 0.8021 - val_precision: 0.2195\n",
      "Epoch 297/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4920 - accuracy: 0.7651 - recall: 0.8185 - precision: 0.7293 - val_loss: 0.4174 - val_accuracy: 0.7444 - val_recall: 0.8021 - val_precision: 0.2198\n",
      "Epoch 298/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4919 - accuracy: 0.7652 - recall: 0.8196 - precision: 0.7289 - val_loss: 0.4168 - val_accuracy: 0.7448 - val_recall: 0.8012 - val_precision: 0.2199\n",
      "Epoch 299/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4915 - accuracy: 0.7654 - recall: 0.8193 - precision: 0.7293 - val_loss: 0.4172 - val_accuracy: 0.7445 - val_recall: 0.8012 - val_precision: 0.2197\n",
      "Epoch 300/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4919 - accuracy: 0.7655 - recall: 0.8195 - precision: 0.7294 - val_loss: 0.4163 - val_accuracy: 0.7451 - val_recall: 0.8007 - val_precision: 0.2201\n",
      "Epoch 301/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4918 - accuracy: 0.7657 - recall: 0.8196 - precision: 0.7296 - val_loss: 0.4154 - val_accuracy: 0.7459 - val_recall: 0.7988 - val_precision: 0.2204\n",
      "Epoch 302/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4915 - accuracy: 0.7649 - recall: 0.8178 - precision: 0.7293 - val_loss: 0.4177 - val_accuracy: 0.7444 - val_recall: 0.8014 - val_precision: 0.2196\n",
      "Epoch 303/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4918 - accuracy: 0.7653 - recall: 0.8190 - precision: 0.7293 - val_loss: 0.4173 - val_accuracy: 0.7450 - val_recall: 0.8012 - val_precision: 0.2201\n",
      "Epoch 304/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4918 - accuracy: 0.7655 - recall: 0.8196 - precision: 0.7294 - val_loss: 0.4157 - val_accuracy: 0.7457 - val_recall: 0.7995 - val_precision: 0.2203\n",
      "Epoch 305/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4915 - accuracy: 0.7655 - recall: 0.8195 - precision: 0.7293 - val_loss: 0.4155 - val_accuracy: 0.7458 - val_recall: 0.7997 - val_precision: 0.2204\n",
      "Epoch 306/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4916 - accuracy: 0.7657 - recall: 0.8195 - precision: 0.7296 - val_loss: 0.4162 - val_accuracy: 0.7453 - val_recall: 0.8002 - val_precision: 0.2201\n",
      "Epoch 307/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4915 - accuracy: 0.7657 - recall: 0.8197 - precision: 0.7296 - val_loss: 0.4155 - val_accuracy: 0.7459 - val_recall: 0.7997 - val_precision: 0.2205\n",
      "Epoch 308/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4915 - accuracy: 0.7656 - recall: 0.8194 - precision: 0.7296 - val_loss: 0.4159 - val_accuracy: 0.7457 - val_recall: 0.8002 - val_precision: 0.2204\n",
      "Epoch 309/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4921 - accuracy: 0.7654 - recall: 0.8189 - precision: 0.7296 - val_loss: 0.4177 - val_accuracy: 0.7447 - val_recall: 0.8012 - val_precision: 0.2199\n",
      "Epoch 310/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4921 - accuracy: 0.7654 - recall: 0.8187 - precision: 0.7296 - val_loss: 0.4170 - val_accuracy: 0.7453 - val_recall: 0.8009 - val_precision: 0.2202\n",
      "Epoch 311/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4916 - accuracy: 0.7654 - recall: 0.8192 - precision: 0.7293 - val_loss: 0.4167 - val_accuracy: 0.7453 - val_recall: 0.8007 - val_precision: 0.2202\n",
      "Epoch 312/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4913 - accuracy: 0.7662 - recall: 0.8199 - precision: 0.7302 - val_loss: 0.4165 - val_accuracy: 0.7454 - val_recall: 0.8007 - val_precision: 0.2203\n",
      "Epoch 313/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4914 - accuracy: 0.7655 - recall: 0.8188 - precision: 0.7297 - val_loss: 0.4179 - val_accuracy: 0.7443 - val_recall: 0.8014 - val_precision: 0.2196\n",
      "Epoch 314/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4918 - accuracy: 0.7657 - recall: 0.8200 - precision: 0.7294 - val_loss: 0.4169 - val_accuracy: 0.7452 - val_recall: 0.8012 - val_precision: 0.2202\n",
      "Epoch 315/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4919 - accuracy: 0.7657 - recall: 0.8193 - precision: 0.7297 - val_loss: 0.4172 - val_accuracy: 0.7447 - val_recall: 0.8012 - val_precision: 0.2198\n",
      "Epoch 316/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4913 - accuracy: 0.7656 - recall: 0.8194 - precision: 0.7296 - val_loss: 0.4165 - val_accuracy: 0.7452 - val_recall: 0.8007 - val_precision: 0.2201\n",
      "Epoch 317/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4912 - accuracy: 0.7652 - recall: 0.8195 - precision: 0.7289 - val_loss: 0.4160 - val_accuracy: 0.7459 - val_recall: 0.8002 - val_precision: 0.2205\n",
      "Epoch 318/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4910 - accuracy: 0.7658 - recall: 0.8194 - precision: 0.7298 - val_loss: 0.4162 - val_accuracy: 0.7458 - val_recall: 0.8000 - val_precision: 0.2205\n",
      "Epoch 319/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4920 - accuracy: 0.7653 - recall: 0.8195 - precision: 0.7292 - val_loss: 0.4163 - val_accuracy: 0.7456 - val_recall: 0.8007 - val_precision: 0.2204\n",
      "Epoch 320/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4916 - accuracy: 0.7657 - recall: 0.8196 - precision: 0.7296 - val_loss: 0.4168 - val_accuracy: 0.7452 - val_recall: 0.8007 - val_precision: 0.2202\n",
      "Epoch 321/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4917 - accuracy: 0.7652 - recall: 0.8194 - precision: 0.7291 - val_loss: 0.4157 - val_accuracy: 0.7461 - val_recall: 0.7997 - val_precision: 0.2206\n",
      "Epoch 322/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4912 - accuracy: 0.7654 - recall: 0.8188 - precision: 0.7295 - val_loss: 0.4155 - val_accuracy: 0.7465 - val_recall: 0.7990 - val_precision: 0.2208\n",
      "Epoch 323/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4914 - accuracy: 0.7653 - recall: 0.8182 - precision: 0.7296 - val_loss: 0.4168 - val_accuracy: 0.7459 - val_recall: 0.8002 - val_precision: 0.2205\n",
      "Epoch 324/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4913 - accuracy: 0.7654 - recall: 0.8193 - precision: 0.7293 - val_loss: 0.4156 - val_accuracy: 0.7466 - val_recall: 0.7990 - val_precision: 0.2209\n",
      "Epoch 325/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4916 - accuracy: 0.7653 - recall: 0.8193 - precision: 0.7292 - val_loss: 0.4154 - val_accuracy: 0.7471 - val_recall: 0.7978 - val_precision: 0.2211\n",
      "Epoch 326/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4915 - accuracy: 0.7654 - recall: 0.8183 - precision: 0.7298 - val_loss: 0.4161 - val_accuracy: 0.7466 - val_recall: 0.7980 - val_precision: 0.2207\n",
      "Epoch 327/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4914 - accuracy: 0.7659 - recall: 0.8198 - precision: 0.7297 - val_loss: 0.4154 - val_accuracy: 0.7469 - val_recall: 0.7980 - val_precision: 0.2210\n",
      "Epoch 328/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4913 - accuracy: 0.7659 - recall: 0.8192 - precision: 0.7301 - val_loss: 0.4164 - val_accuracy: 0.7461 - val_recall: 0.7990 - val_precision: 0.2206\n",
      "Epoch 329/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4913 - accuracy: 0.7655 - recall: 0.8190 - precision: 0.7297 - val_loss: 0.4163 - val_accuracy: 0.7463 - val_recall: 0.7985 - val_precision: 0.2206\n",
      "Epoch 330/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4913 - accuracy: 0.7656 - recall: 0.8186 - precision: 0.7298 - val_loss: 0.4162 - val_accuracy: 0.7469 - val_recall: 0.7983 - val_precision: 0.2210\n",
      "Epoch 331/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4912 - accuracy: 0.7654 - recall: 0.8191 - precision: 0.7294 - val_loss: 0.4155 - val_accuracy: 0.7472 - val_recall: 0.7978 - val_precision: 0.2211\n",
      "Epoch 332/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4912 - accuracy: 0.7654 - recall: 0.8183 - precision: 0.7297 - val_loss: 0.4164 - val_accuracy: 0.7465 - val_recall: 0.7985 - val_precision: 0.2207\n",
      "Epoch 333/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4909 - accuracy: 0.7660 - recall: 0.8196 - precision: 0.7300 - val_loss: 0.4166 - val_accuracy: 0.7463 - val_recall: 0.7995 - val_precision: 0.2207\n",
      "Epoch 334/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4906 - accuracy: 0.7660 - recall: 0.8191 - precision: 0.7302 - val_loss: 0.4165 - val_accuracy: 0.7466 - val_recall: 0.7990 - val_precision: 0.2209\n",
      "Epoch 335/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4913 - accuracy: 0.7650 - recall: 0.8182 - precision: 0.7293 - val_loss: 0.4167 - val_accuracy: 0.7464 - val_recall: 0.7990 - val_precision: 0.2208\n",
      "Epoch 336/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4910 - accuracy: 0.7657 - recall: 0.8183 - precision: 0.7302 - val_loss: 0.4174 - val_accuracy: 0.7461 - val_recall: 0.7992 - val_precision: 0.2205\n",
      "Epoch 337/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4911 - accuracy: 0.7657 - recall: 0.8185 - precision: 0.7301 - val_loss: 0.4160 - val_accuracy: 0.7472 - val_recall: 0.7980 - val_precision: 0.2212\n",
      "Epoch 338/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4912 - accuracy: 0.7658 - recall: 0.8186 - precision: 0.7302 - val_loss: 0.4160 - val_accuracy: 0.7473 - val_recall: 0.7978 - val_precision: 0.2212\n",
      "Epoch 339/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4909 - accuracy: 0.7658 - recall: 0.8178 - precision: 0.7305 - val_loss: 0.4158 - val_accuracy: 0.7476 - val_recall: 0.7978 - val_precision: 0.2214\n",
      "Epoch 340/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4913 - accuracy: 0.7656 - recall: 0.8186 - precision: 0.7299 - val_loss: 0.4151 - val_accuracy: 0.7481 - val_recall: 0.7971 - val_precision: 0.2217\n",
      "Epoch 341/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4910 - accuracy: 0.7659 - recall: 0.8182 - precision: 0.7304 - val_loss: 0.4163 - val_accuracy: 0.7474 - val_recall: 0.7980 - val_precision: 0.2214\n",
      "Epoch 342/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4915 - accuracy: 0.7656 - recall: 0.8183 - precision: 0.7299 - val_loss: 0.4157 - val_accuracy: 0.7479 - val_recall: 0.7973 - val_precision: 0.2216\n",
      "Epoch 343/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4911 - accuracy: 0.7655 - recall: 0.8180 - precision: 0.7301 - val_loss: 0.4168 - val_accuracy: 0.7470 - val_recall: 0.7985 - val_precision: 0.2211\n",
      "Epoch 344/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4909 - accuracy: 0.7655 - recall: 0.8187 - precision: 0.7298 - val_loss: 0.4156 - val_accuracy: 0.7480 - val_recall: 0.7976 - val_precision: 0.2217\n",
      "Epoch 345/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4910 - accuracy: 0.7659 - recall: 0.8187 - precision: 0.7303 - val_loss: 0.4159 - val_accuracy: 0.7479 - val_recall: 0.7976 - val_precision: 0.2216\n",
      "Epoch 346/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4911 - accuracy: 0.7659 - recall: 0.8181 - precision: 0.7304 - val_loss: 0.4150 - val_accuracy: 0.7488 - val_recall: 0.7966 - val_precision: 0.2222\n",
      "Epoch 347/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4913 - accuracy: 0.7658 - recall: 0.8177 - precision: 0.7305 - val_loss: 0.4157 - val_accuracy: 0.7483 - val_recall: 0.7968 - val_precision: 0.2218\n",
      "Epoch 348/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4913 - accuracy: 0.7655 - recall: 0.8176 - precision: 0.7302 - val_loss: 0.4161 - val_accuracy: 0.7483 - val_recall: 0.7971 - val_precision: 0.2218\n",
      "Epoch 349/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4911 - accuracy: 0.7654 - recall: 0.8176 - precision: 0.7300 - val_loss: 0.4147 - val_accuracy: 0.7494 - val_recall: 0.7961 - val_precision: 0.2225\n",
      "Epoch 350/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4912 - accuracy: 0.7652 - recall: 0.8164 - precision: 0.7302 - val_loss: 0.4164 - val_accuracy: 0.7481 - val_recall: 0.7973 - val_precision: 0.2217\n",
      "Epoch 351/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4910 - accuracy: 0.7659 - recall: 0.8181 - precision: 0.7305 - val_loss: 0.4157 - val_accuracy: 0.7484 - val_recall: 0.7968 - val_precision: 0.2219\n",
      "Epoch 352/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4906 - accuracy: 0.7660 - recall: 0.8183 - precision: 0.7306 - val_loss: 0.4155 - val_accuracy: 0.7486 - val_recall: 0.7968 - val_precision: 0.2220\n",
      "Epoch 353/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4911 - accuracy: 0.7657 - recall: 0.8181 - precision: 0.7302 - val_loss: 0.4160 - val_accuracy: 0.7483 - val_recall: 0.7971 - val_precision: 0.2218\n",
      "Epoch 354/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4911 - accuracy: 0.7659 - recall: 0.8185 - precision: 0.7304 - val_loss: 0.4148 - val_accuracy: 0.7492 - val_recall: 0.7964 - val_precision: 0.2224\n",
      "Epoch 355/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4912 - accuracy: 0.7656 - recall: 0.8173 - precision: 0.7304 - val_loss: 0.4160 - val_accuracy: 0.7486 - val_recall: 0.7966 - val_precision: 0.2219\n",
      "Epoch 356/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4908 - accuracy: 0.7655 - recall: 0.8181 - precision: 0.7299 - val_loss: 0.4156 - val_accuracy: 0.7487 - val_recall: 0.7966 - val_precision: 0.2221\n",
      "Epoch 357/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4909 - accuracy: 0.7657 - recall: 0.8172 - precision: 0.7306 - val_loss: 0.4151 - val_accuracy: 0.7491 - val_recall: 0.7966 - val_precision: 0.2223\n",
      "Epoch 358/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4910 - accuracy: 0.7657 - recall: 0.8176 - precision: 0.7305 - val_loss: 0.4160 - val_accuracy: 0.7484 - val_recall: 0.7966 - val_precision: 0.2218\n",
      "Epoch 359/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4907 - accuracy: 0.7659 - recall: 0.8184 - precision: 0.7303 - val_loss: 0.4147 - val_accuracy: 0.7493 - val_recall: 0.7964 - val_precision: 0.2225\n",
      "Epoch 360/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4906 - accuracy: 0.7661 - recall: 0.8185 - precision: 0.7306 - val_loss: 0.4153 - val_accuracy: 0.7489 - val_recall: 0.7966 - val_precision: 0.2222\n",
      "Epoch 361/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4908 - accuracy: 0.7659 - recall: 0.8180 - precision: 0.7306 - val_loss: 0.4158 - val_accuracy: 0.7487 - val_recall: 0.7966 - val_precision: 0.2220\n",
      "Epoch 362/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4910 - accuracy: 0.7658 - recall: 0.8180 - precision: 0.7304 - val_loss: 0.4149 - val_accuracy: 0.7495 - val_recall: 0.7961 - val_precision: 0.2225\n",
      "Epoch 363/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4909 - accuracy: 0.7658 - recall: 0.8178 - precision: 0.7306 - val_loss: 0.4159 - val_accuracy: 0.7488 - val_recall: 0.7966 - val_precision: 0.2221\n",
      "Epoch 364/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4912 - accuracy: 0.7652 - recall: 0.8174 - precision: 0.7298 - val_loss: 0.4159 - val_accuracy: 0.7491 - val_recall: 0.7964 - val_precision: 0.2223\n",
      "Epoch 365/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4906 - accuracy: 0.7663 - recall: 0.8182 - precision: 0.7310 - val_loss: 0.4155 - val_accuracy: 0.7493 - val_recall: 0.7964 - val_precision: 0.2225\n",
      "Epoch 366/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4907 - accuracy: 0.7659 - recall: 0.8181 - precision: 0.7305 - val_loss: 0.4157 - val_accuracy: 0.7493 - val_recall: 0.7964 - val_precision: 0.2224\n",
      "Epoch 367/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4907 - accuracy: 0.7664 - recall: 0.8184 - precision: 0.7310 - val_loss: 0.4140 - val_accuracy: 0.7505 - val_recall: 0.7964 - val_precision: 0.2234\n",
      "Epoch 368/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4907 - accuracy: 0.7656 - recall: 0.8181 - precision: 0.7301 - val_loss: 0.4136 - val_accuracy: 0.7513 - val_recall: 0.7942 - val_precision: 0.2236\n",
      "Epoch 369/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4910 - accuracy: 0.7661 - recall: 0.8170 - precision: 0.7312 - val_loss: 0.4163 - val_accuracy: 0.7488 - val_recall: 0.7964 - val_precision: 0.2221\n",
      "Epoch 370/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4906 - accuracy: 0.7659 - recall: 0.8180 - precision: 0.7306 - val_loss: 0.4157 - val_accuracy: 0.7494 - val_recall: 0.7964 - val_precision: 0.2225\n",
      "Epoch 371/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4910 - accuracy: 0.7654 - recall: 0.8177 - precision: 0.7300 - val_loss: 0.4145 - val_accuracy: 0.7502 - val_recall: 0.7964 - val_precision: 0.2231\n",
      "Epoch 372/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4909 - accuracy: 0.7658 - recall: 0.8174 - precision: 0.7307 - val_loss: 0.4169 - val_accuracy: 0.7487 - val_recall: 0.7964 - val_precision: 0.2220\n",
      "Epoch 373/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4909 - accuracy: 0.7661 - recall: 0.8182 - precision: 0.7307 - val_loss: 0.4145 - val_accuracy: 0.7505 - val_recall: 0.7961 - val_precision: 0.2234\n",
      "Epoch 374/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4906 - accuracy: 0.7659 - recall: 0.8174 - precision: 0.7308 - val_loss: 0.4146 - val_accuracy: 0.7507 - val_recall: 0.7961 - val_precision: 0.2235\n",
      "Epoch 375/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4909 - accuracy: 0.7659 - recall: 0.8170 - precision: 0.7310 - val_loss: 0.4150 - val_accuracy: 0.7505 - val_recall: 0.7964 - val_precision: 0.2233\n",
      "Epoch 376/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4906 - accuracy: 0.7662 - recall: 0.8184 - precision: 0.7308 - val_loss: 0.4160 - val_accuracy: 0.7499 - val_recall: 0.7964 - val_precision: 0.2229\n",
      "Epoch 377/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4903 - accuracy: 0.7659 - recall: 0.8178 - precision: 0.7306 - val_loss: 0.4148 - val_accuracy: 0.7505 - val_recall: 0.7956 - val_precision: 0.2233\n",
      "Epoch 378/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4908 - accuracy: 0.7658 - recall: 0.8171 - precision: 0.7308 - val_loss: 0.4158 - val_accuracy: 0.7502 - val_recall: 0.7964 - val_precision: 0.2232\n",
      "Epoch 379/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4906 - accuracy: 0.7662 - recall: 0.8178 - precision: 0.7310 - val_loss: 0.4157 - val_accuracy: 0.7502 - val_recall: 0.7959 - val_precision: 0.2231\n",
      "Epoch 380/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4907 - accuracy: 0.7659 - recall: 0.8169 - precision: 0.7310 - val_loss: 0.4165 - val_accuracy: 0.7496 - val_recall: 0.7961 - val_precision: 0.2227\n",
      "Epoch 381/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4906 - accuracy: 0.7658 - recall: 0.8178 - precision: 0.7304 - val_loss: 0.4142 - val_accuracy: 0.7514 - val_recall: 0.7947 - val_precision: 0.2238\n",
      "Epoch 382/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4908 - accuracy: 0.7665 - recall: 0.8174 - precision: 0.7315 - val_loss: 0.4145 - val_accuracy: 0.7512 - val_recall: 0.7952 - val_precision: 0.2237\n",
      "Epoch 383/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4907 - accuracy: 0.7660 - recall: 0.8178 - precision: 0.7308 - val_loss: 0.4139 - val_accuracy: 0.7517 - val_recall: 0.7940 - val_precision: 0.2238\n",
      "Epoch 384/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4905 - accuracy: 0.7657 - recall: 0.8163 - precision: 0.7310 - val_loss: 0.4152 - val_accuracy: 0.7511 - val_recall: 0.7952 - val_precision: 0.2236\n",
      "Epoch 385/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4905 - accuracy: 0.7659 - recall: 0.8170 - precision: 0.7310 - val_loss: 0.4144 - val_accuracy: 0.7516 - val_recall: 0.7942 - val_precision: 0.2238\n",
      "Epoch 386/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4901 - accuracy: 0.7663 - recall: 0.8180 - precision: 0.7311 - val_loss: 0.4144 - val_accuracy: 0.7517 - val_recall: 0.7937 - val_precision: 0.2238\n",
      "Epoch 387/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4907 - accuracy: 0.7662 - recall: 0.8169 - precision: 0.7314 - val_loss: 0.4143 - val_accuracy: 0.7519 - val_recall: 0.7940 - val_precision: 0.2240\n",
      "Epoch 388/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4902 - accuracy: 0.7660 - recall: 0.8166 - precision: 0.7312 - val_loss: 0.4145 - val_accuracy: 0.7516 - val_recall: 0.7947 - val_precision: 0.2239\n",
      "Epoch 389/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4907 - accuracy: 0.7655 - recall: 0.8163 - precision: 0.7307 - val_loss: 0.4135 - val_accuracy: 0.7526 - val_recall: 0.7930 - val_precision: 0.2244\n",
      "Epoch 390/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4908 - accuracy: 0.7657 - recall: 0.8156 - precision: 0.7313 - val_loss: 0.4139 - val_accuracy: 0.7520 - val_recall: 0.7940 - val_precision: 0.2241\n",
      "Epoch 391/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4901 - accuracy: 0.7661 - recall: 0.8165 - precision: 0.7314 - val_loss: 0.4141 - val_accuracy: 0.7518 - val_recall: 0.7944 - val_precision: 0.2240\n",
      "Epoch 392/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4899 - accuracy: 0.7663 - recall: 0.8168 - precision: 0.7316 - val_loss: 0.4147 - val_accuracy: 0.7516 - val_recall: 0.7954 - val_precision: 0.2240\n",
      "Epoch 393/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4903 - accuracy: 0.7657 - recall: 0.8167 - precision: 0.7308 - val_loss: 0.4144 - val_accuracy: 0.7518 - val_recall: 0.7949 - val_precision: 0.2241\n",
      "Epoch 394/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4909 - accuracy: 0.7656 - recall: 0.8162 - precision: 0.7309 - val_loss: 0.4157 - val_accuracy: 0.7509 - val_recall: 0.7956 - val_precision: 0.2235\n",
      "Epoch 395/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4902 - accuracy: 0.7665 - recall: 0.8177 - precision: 0.7315 - val_loss: 0.4155 - val_accuracy: 0.7509 - val_recall: 0.7956 - val_precision: 0.2236\n",
      "Epoch 396/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4905 - accuracy: 0.7660 - recall: 0.8176 - precision: 0.7309 - val_loss: 0.4144 - val_accuracy: 0.7518 - val_recall: 0.7944 - val_precision: 0.2240\n",
      "Epoch 397/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4906 - accuracy: 0.7659 - recall: 0.8165 - precision: 0.7312 - val_loss: 0.4154 - val_accuracy: 0.7510 - val_recall: 0.7956 - val_precision: 0.2236\n",
      "Epoch 398/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4906 - accuracy: 0.7657 - recall: 0.8174 - precision: 0.7306 - val_loss: 0.4150 - val_accuracy: 0.7517 - val_recall: 0.7947 - val_precision: 0.2240\n",
      "Epoch 399/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4906 - accuracy: 0.7656 - recall: 0.8168 - precision: 0.7306 - val_loss: 0.4136 - val_accuracy: 0.7531 - val_recall: 0.7935 - val_precision: 0.2249\n",
      "Epoch 400/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4903 - accuracy: 0.7664 - recall: 0.8170 - precision: 0.7316 - val_loss: 0.4143 - val_accuracy: 0.7522 - val_recall: 0.7937 - val_precision: 0.2242\n",
      "Epoch 401/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4905 - accuracy: 0.7657 - recall: 0.8171 - precision: 0.7307 - val_loss: 0.4127 - val_accuracy: 0.7534 - val_recall: 0.7920 - val_precision: 0.2249\n",
      "Epoch 402/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4910 - accuracy: 0.7652 - recall: 0.8160 - precision: 0.7305 - val_loss: 0.4142 - val_accuracy: 0.7523 - val_recall: 0.7940 - val_precision: 0.2244\n",
      "Epoch 403/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4901 - accuracy: 0.7658 - recall: 0.8165 - precision: 0.7310 - val_loss: 0.4137 - val_accuracy: 0.7530 - val_recall: 0.7937 - val_precision: 0.2248\n",
      "Epoch 404/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4900 - accuracy: 0.7665 - recall: 0.8170 - precision: 0.7318 - val_loss: 0.4145 - val_accuracy: 0.7525 - val_recall: 0.7942 - val_precision: 0.2246\n",
      "Epoch 405/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4906 - accuracy: 0.7658 - recall: 0.8167 - precision: 0.7310 - val_loss: 0.4136 - val_accuracy: 0.7533 - val_recall: 0.7930 - val_precision: 0.2249\n",
      "Epoch 406/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4902 - accuracy: 0.7661 - recall: 0.8163 - precision: 0.7315 - val_loss: 0.4137 - val_accuracy: 0.7531 - val_recall: 0.7930 - val_precision: 0.2248\n",
      "Epoch 407/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4903 - accuracy: 0.7660 - recall: 0.8161 - precision: 0.7314 - val_loss: 0.4135 - val_accuracy: 0.7532 - val_recall: 0.7928 - val_precision: 0.2249\n",
      "Epoch 408/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4903 - accuracy: 0.7661 - recall: 0.8161 - precision: 0.7316 - val_loss: 0.4150 - val_accuracy: 0.7527 - val_recall: 0.7935 - val_precision: 0.2245\n",
      "Epoch 409/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4900 - accuracy: 0.7661 - recall: 0.8161 - precision: 0.7316 - val_loss: 0.4154 - val_accuracy: 0.7522 - val_recall: 0.7942 - val_precision: 0.2243\n",
      "Epoch 410/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4904 - accuracy: 0.7664 - recall: 0.8180 - precision: 0.7312 - val_loss: 0.4134 - val_accuracy: 0.7538 - val_recall: 0.7911 - val_precision: 0.2250\n",
      "Epoch 411/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4904 - accuracy: 0.7660 - recall: 0.8163 - precision: 0.7313 - val_loss: 0.4128 - val_accuracy: 0.7544 - val_recall: 0.7896 - val_precision: 0.2252\n",
      "Epoch 412/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4901 - accuracy: 0.7658 - recall: 0.8160 - precision: 0.7312 - val_loss: 0.4138 - val_accuracy: 0.7537 - val_recall: 0.7913 - val_precision: 0.2250\n",
      "Epoch 413/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4899 - accuracy: 0.7662 - recall: 0.8169 - precision: 0.7314 - val_loss: 0.4153 - val_accuracy: 0.7526 - val_recall: 0.7932 - val_precision: 0.2244\n",
      "Epoch 414/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4901 - accuracy: 0.7662 - recall: 0.8172 - precision: 0.7313 - val_loss: 0.4139 - val_accuracy: 0.7536 - val_recall: 0.7916 - val_precision: 0.2250\n",
      "Epoch 415/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4895 - accuracy: 0.7663 - recall: 0.8163 - precision: 0.7317 - val_loss: 0.4134 - val_accuracy: 0.7540 - val_recall: 0.7904 - val_precision: 0.2251\n",
      "Epoch 416/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4901 - accuracy: 0.7658 - recall: 0.8157 - precision: 0.7313 - val_loss: 0.4151 - val_accuracy: 0.7527 - val_recall: 0.7932 - val_precision: 0.2245\n",
      "Epoch 417/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4900 - accuracy: 0.7656 - recall: 0.8169 - precision: 0.7306 - val_loss: 0.4141 - val_accuracy: 0.7536 - val_recall: 0.7913 - val_precision: 0.2249\n",
      "Epoch 418/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4900 - accuracy: 0.7663 - recall: 0.8167 - precision: 0.7317 - val_loss: 0.4137 - val_accuracy: 0.7540 - val_recall: 0.7906 - val_precision: 0.2251\n",
      "Epoch 419/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4901 - accuracy: 0.7663 - recall: 0.8168 - precision: 0.7316 - val_loss: 0.4134 - val_accuracy: 0.7545 - val_recall: 0.7896 - val_precision: 0.2254\n",
      "Epoch 420/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4902 - accuracy: 0.7665 - recall: 0.8163 - precision: 0.7321 - val_loss: 0.4139 - val_accuracy: 0.7542 - val_recall: 0.7901 - val_precision: 0.2252\n",
      "Epoch 421/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4898 - accuracy: 0.7662 - recall: 0.8159 - precision: 0.7317 - val_loss: 0.4138 - val_accuracy: 0.7543 - val_recall: 0.7896 - val_precision: 0.2252\n",
      "Epoch 422/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4899 - accuracy: 0.7663 - recall: 0.8165 - precision: 0.7317 - val_loss: 0.4134 - val_accuracy: 0.7545 - val_recall: 0.7889 - val_precision: 0.2252\n",
      "Epoch 423/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4903 - accuracy: 0.7657 - recall: 0.8148 - precision: 0.7316 - val_loss: 0.4145 - val_accuracy: 0.7538 - val_recall: 0.7911 - val_precision: 0.2250\n",
      "Epoch 424/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4902 - accuracy: 0.7660 - recall: 0.8160 - precision: 0.7314 - val_loss: 0.4150 - val_accuracy: 0.7535 - val_recall: 0.7916 - val_precision: 0.2249\n",
      "Epoch 425/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4901 - accuracy: 0.7659 - recall: 0.8167 - precision: 0.7310 - val_loss: 0.4133 - val_accuracy: 0.7545 - val_recall: 0.7892 - val_precision: 0.2252\n",
      "Epoch 426/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4902 - accuracy: 0.7659 - recall: 0.8153 - precision: 0.7316 - val_loss: 0.4151 - val_accuracy: 0.7532 - val_recall: 0.7913 - val_precision: 0.2246\n",
      "Epoch 427/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4898 - accuracy: 0.7663 - recall: 0.8162 - precision: 0.7319 - val_loss: 0.4145 - val_accuracy: 0.7538 - val_recall: 0.7908 - val_precision: 0.2250\n",
      "Epoch 428/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4898 - accuracy: 0.7661 - recall: 0.8165 - precision: 0.7314 - val_loss: 0.4142 - val_accuracy: 0.7540 - val_recall: 0.7899 - val_precision: 0.2250\n",
      "Epoch 429/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4898 - accuracy: 0.7667 - recall: 0.8166 - precision: 0.7323 - val_loss: 0.4138 - val_accuracy: 0.7546 - val_recall: 0.7887 - val_precision: 0.2252\n",
      "Epoch 430/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4897 - accuracy: 0.7662 - recall: 0.8150 - precision: 0.7321 - val_loss: 0.4144 - val_accuracy: 0.7543 - val_recall: 0.7892 - val_precision: 0.2251\n",
      "Epoch 431/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4901 - accuracy: 0.7662 - recall: 0.8159 - precision: 0.7318 - val_loss: 0.4145 - val_accuracy: 0.7543 - val_recall: 0.7896 - val_precision: 0.2252\n",
      "Epoch 432/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4896 - accuracy: 0.7662 - recall: 0.8165 - precision: 0.7315 - val_loss: 0.4138 - val_accuracy: 0.7547 - val_recall: 0.7889 - val_precision: 0.2254\n",
      "Epoch 433/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4901 - accuracy: 0.7669 - recall: 0.8163 - precision: 0.7325 - val_loss: 0.4145 - val_accuracy: 0.7544 - val_recall: 0.7894 - val_precision: 0.2252\n",
      "Epoch 434/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4899 - accuracy: 0.7665 - recall: 0.8165 - precision: 0.7320 - val_loss: 0.4129 - val_accuracy: 0.7557 - val_recall: 0.7882 - val_precision: 0.2260\n",
      "Epoch 435/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4897 - accuracy: 0.7664 - recall: 0.8156 - precision: 0.7321 - val_loss: 0.4132 - val_accuracy: 0.7556 - val_recall: 0.7882 - val_precision: 0.2260\n",
      "Epoch 436/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4900 - accuracy: 0.7661 - recall: 0.8148 - precision: 0.7321 - val_loss: 0.4140 - val_accuracy: 0.7550 - val_recall: 0.7894 - val_precision: 0.2257\n",
      "Epoch 437/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4896 - accuracy: 0.7663 - recall: 0.8161 - precision: 0.7319 - val_loss: 0.4132 - val_accuracy: 0.7557 - val_recall: 0.7880 - val_precision: 0.2260\n",
      "Epoch 438/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4895 - accuracy: 0.7664 - recall: 0.8152 - precision: 0.7323 - val_loss: 0.4134 - val_accuracy: 0.7556 - val_recall: 0.7884 - val_precision: 0.2260\n",
      "Epoch 439/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4897 - accuracy: 0.7662 - recall: 0.8157 - precision: 0.7319 - val_loss: 0.4123 - val_accuracy: 0.7563 - val_recall: 0.7875 - val_precision: 0.2264\n",
      "Epoch 440/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4897 - accuracy: 0.7661 - recall: 0.8154 - precision: 0.7319 - val_loss: 0.4126 - val_accuracy: 0.7564 - val_recall: 0.7875 - val_precision: 0.2265\n",
      "Epoch 441/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4900 - accuracy: 0.7662 - recall: 0.8150 - precision: 0.7321 - val_loss: 0.4152 - val_accuracy: 0.7544 - val_recall: 0.7896 - val_precision: 0.2253\n",
      "Epoch 442/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4897 - accuracy: 0.7662 - recall: 0.8162 - precision: 0.7316 - val_loss: 0.4133 - val_accuracy: 0.7556 - val_recall: 0.7877 - val_precision: 0.2259\n",
      "Epoch 443/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4896 - accuracy: 0.7661 - recall: 0.8153 - precision: 0.7319 - val_loss: 0.4146 - val_accuracy: 0.7548 - val_recall: 0.7894 - val_precision: 0.2255\n",
      "Epoch 444/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4898 - accuracy: 0.7660 - recall: 0.8158 - precision: 0.7316 - val_loss: 0.4128 - val_accuracy: 0.7561 - val_recall: 0.7875 - val_precision: 0.2262\n",
      "Epoch 445/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4902 - accuracy: 0.7659 - recall: 0.8146 - precision: 0.7320 - val_loss: 0.4136 - val_accuracy: 0.7556 - val_recall: 0.7884 - val_precision: 0.2260\n",
      "Epoch 446/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4898 - accuracy: 0.7661 - recall: 0.8158 - precision: 0.7318 - val_loss: 0.4134 - val_accuracy: 0.7556 - val_recall: 0.7880 - val_precision: 0.2259\n",
      "Epoch 447/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4897 - accuracy: 0.7667 - recall: 0.8162 - precision: 0.7324 - val_loss: 0.4136 - val_accuracy: 0.7554 - val_recall: 0.7882 - val_precision: 0.2258\n",
      "Epoch 448/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4896 - accuracy: 0.7663 - recall: 0.8170 - precision: 0.7315 - val_loss: 0.4125 - val_accuracy: 0.7562 - val_recall: 0.7875 - val_precision: 0.2263\n",
      "Epoch 449/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4901 - accuracy: 0.7655 - recall: 0.8144 - precision: 0.7314 - val_loss: 0.4135 - val_accuracy: 0.7556 - val_recall: 0.7882 - val_precision: 0.2259\n",
      "Epoch 450/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4895 - accuracy: 0.7661 - recall: 0.8164 - precision: 0.7315 - val_loss: 0.4127 - val_accuracy: 0.7561 - val_recall: 0.7880 - val_precision: 0.2263\n",
      "Epoch 451/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4898 - accuracy: 0.7665 - recall: 0.8161 - precision: 0.7321 - val_loss: 0.4132 - val_accuracy: 0.7556 - val_recall: 0.7880 - val_precision: 0.2259\n",
      "Epoch 452/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4895 - accuracy: 0.7665 - recall: 0.8160 - precision: 0.7322 - val_loss: 0.4129 - val_accuracy: 0.7559 - val_recall: 0.7877 - val_precision: 0.2261\n",
      "Epoch 453/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4898 - accuracy: 0.7661 - recall: 0.8152 - precision: 0.7319 - val_loss: 0.4137 - val_accuracy: 0.7552 - val_recall: 0.7880 - val_precision: 0.2256\n",
      "Epoch 454/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4898 - accuracy: 0.7661 - recall: 0.8162 - precision: 0.7316 - val_loss: 0.4129 - val_accuracy: 0.7560 - val_recall: 0.7877 - val_precision: 0.2262\n",
      "Epoch 455/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4899 - accuracy: 0.7662 - recall: 0.8155 - precision: 0.7320 - val_loss: 0.4146 - val_accuracy: 0.7551 - val_recall: 0.7894 - val_precision: 0.2257\n",
      "Epoch 456/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4898 - accuracy: 0.7660 - recall: 0.8159 - precision: 0.7316 - val_loss: 0.4133 - val_accuracy: 0.7559 - val_recall: 0.7877 - val_precision: 0.2261\n",
      "Epoch 457/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4897 - accuracy: 0.7665 - recall: 0.8162 - precision: 0.7321 - val_loss: 0.4131 - val_accuracy: 0.7561 - val_recall: 0.7875 - val_precision: 0.2262\n",
      "Epoch 458/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4896 - accuracy: 0.7663 - recall: 0.8159 - precision: 0.7319 - val_loss: 0.4123 - val_accuracy: 0.7566 - val_recall: 0.7870 - val_precision: 0.2266\n",
      "Epoch 459/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4893 - accuracy: 0.7671 - recall: 0.8160 - precision: 0.7329 - val_loss: 0.4137 - val_accuracy: 0.7558 - val_recall: 0.7880 - val_precision: 0.2261\n",
      "Epoch 460/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4899 - accuracy: 0.7661 - recall: 0.8160 - precision: 0.7316 - val_loss: 0.4117 - val_accuracy: 0.7576 - val_recall: 0.7856 - val_precision: 0.2271\n",
      "Epoch 461/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4895 - accuracy: 0.7670 - recall: 0.8154 - precision: 0.7331 - val_loss: 0.4110 - val_accuracy: 0.7575 - val_recall: 0.7858 - val_precision: 0.2271\n",
      "Epoch 462/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4896 - accuracy: 0.7661 - recall: 0.8155 - precision: 0.7319 - val_loss: 0.4122 - val_accuracy: 0.7568 - val_recall: 0.7868 - val_precision: 0.2267\n",
      "Epoch 463/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4897 - accuracy: 0.7659 - recall: 0.8144 - precision: 0.7320 - val_loss: 0.4136 - val_accuracy: 0.7559 - val_recall: 0.7882 - val_precision: 0.2262\n",
      "Epoch 464/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4895 - accuracy: 0.7666 - recall: 0.8158 - precision: 0.7324 - val_loss: 0.4131 - val_accuracy: 0.7561 - val_recall: 0.7880 - val_precision: 0.2263\n",
      "Epoch 465/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4896 - accuracy: 0.7655 - recall: 0.8151 - precision: 0.7312 - val_loss: 0.4122 - val_accuracy: 0.7568 - val_recall: 0.7872 - val_precision: 0.2268\n",
      "Epoch 466/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4897 - accuracy: 0.7665 - recall: 0.8160 - precision: 0.7321 - val_loss: 0.4130 - val_accuracy: 0.7563 - val_recall: 0.7875 - val_precision: 0.2264\n",
      "Epoch 467/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4897 - accuracy: 0.7664 - recall: 0.8156 - precision: 0.7322 - val_loss: 0.4125 - val_accuracy: 0.7571 - val_recall: 0.7858 - val_precision: 0.2267\n",
      "Epoch 468/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4895 - accuracy: 0.7662 - recall: 0.8155 - precision: 0.7320 - val_loss: 0.4122 - val_accuracy: 0.7573 - val_recall: 0.7860 - val_precision: 0.2270\n",
      "Epoch 469/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4894 - accuracy: 0.7665 - recall: 0.8148 - precision: 0.7326 - val_loss: 0.4129 - val_accuracy: 0.7570 - val_recall: 0.7870 - val_precision: 0.2268\n",
      "Epoch 470/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4895 - accuracy: 0.7664 - recall: 0.8148 - precision: 0.7325 - val_loss: 0.4128 - val_accuracy: 0.7572 - val_recall: 0.7865 - val_precision: 0.2269\n",
      "Epoch 471/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4893 - accuracy: 0.7663 - recall: 0.8146 - precision: 0.7326 - val_loss: 0.4125 - val_accuracy: 0.7571 - val_recall: 0.7865 - val_precision: 0.2269\n",
      "Epoch 472/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4897 - accuracy: 0.7665 - recall: 0.8148 - precision: 0.7327 - val_loss: 0.4142 - val_accuracy: 0.7561 - val_recall: 0.7877 - val_precision: 0.2263\n",
      "Epoch 473/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4893 - accuracy: 0.7666 - recall: 0.8160 - precision: 0.7323 - val_loss: 0.4124 - val_accuracy: 0.7572 - val_recall: 0.7865 - val_precision: 0.2269\n",
      "Epoch 474/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4893 - accuracy: 0.7668 - recall: 0.8151 - precision: 0.7329 - val_loss: 0.4128 - val_accuracy: 0.7573 - val_recall: 0.7860 - val_precision: 0.2269\n",
      "Epoch 475/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4892 - accuracy: 0.7666 - recall: 0.8146 - precision: 0.7329 - val_loss: 0.4133 - val_accuracy: 0.7573 - val_recall: 0.7858 - val_precision: 0.2269\n",
      "Epoch 476/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4893 - accuracy: 0.7662 - recall: 0.8145 - precision: 0.7323 - val_loss: 0.4120 - val_accuracy: 0.7584 - val_recall: 0.7844 - val_precision: 0.2275\n",
      "Epoch 477/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4895 - accuracy: 0.7666 - recall: 0.8138 - precision: 0.7332 - val_loss: 0.4139 - val_accuracy: 0.7569 - val_recall: 0.7870 - val_precision: 0.2268\n",
      "Epoch 478/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4894 - accuracy: 0.7662 - recall: 0.8147 - precision: 0.7323 - val_loss: 0.4140 - val_accuracy: 0.7566 - val_recall: 0.7870 - val_precision: 0.2265\n",
      "Epoch 479/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4893 - accuracy: 0.7666 - recall: 0.8155 - precision: 0.7326 - val_loss: 0.4138 - val_accuracy: 0.7569 - val_recall: 0.7870 - val_precision: 0.2268\n",
      "Epoch 480/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4896 - accuracy: 0.7666 - recall: 0.8149 - precision: 0.7328 - val_loss: 0.4136 - val_accuracy: 0.7572 - val_recall: 0.7865 - val_precision: 0.2269\n",
      "Epoch 481/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4895 - accuracy: 0.7668 - recall: 0.8158 - precision: 0.7326 - val_loss: 0.4130 - val_accuracy: 0.7575 - val_recall: 0.7858 - val_precision: 0.2271\n",
      "Epoch 482/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4895 - accuracy: 0.7665 - recall: 0.8141 - precision: 0.7330 - val_loss: 0.4147 - val_accuracy: 0.7564 - val_recall: 0.7872 - val_precision: 0.2265\n",
      "Epoch 483/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4891 - accuracy: 0.7666 - recall: 0.8157 - precision: 0.7324 - val_loss: 0.4126 - val_accuracy: 0.7576 - val_recall: 0.7856 - val_precision: 0.2271\n",
      "Epoch 484/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4895 - accuracy: 0.7664 - recall: 0.8154 - precision: 0.7323 - val_loss: 0.4133 - val_accuracy: 0.7574 - val_recall: 0.7858 - val_precision: 0.2270\n",
      "Epoch 485/500\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4894 - accuracy: 0.7666 - recall: 0.8150 - precision: 0.7327 - val_loss: 0.4145 - val_accuracy: 0.7567 - val_recall: 0.7872 - val_precision: 0.2266\n",
      "Epoch 486/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4892 - accuracy: 0.7666 - recall: 0.8152 - precision: 0.7326 - val_loss: 0.4134 - val_accuracy: 0.7572 - val_recall: 0.7860 - val_precision: 0.2268\n",
      "Epoch 487/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4895 - accuracy: 0.7664 - recall: 0.8155 - precision: 0.7323 - val_loss: 0.4129 - val_accuracy: 0.7576 - val_recall: 0.7851 - val_precision: 0.2270\n",
      "Epoch 488/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4890 - accuracy: 0.7667 - recall: 0.8148 - precision: 0.7329 - val_loss: 0.4139 - val_accuracy: 0.7570 - val_recall: 0.7868 - val_precision: 0.2268\n",
      "Epoch 489/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4891 - accuracy: 0.7666 - recall: 0.8154 - precision: 0.7326 - val_loss: 0.4134 - val_accuracy: 0.7572 - val_recall: 0.7858 - val_precision: 0.2268\n",
      "Epoch 490/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4893 - accuracy: 0.7669 - recall: 0.8156 - precision: 0.7329 - val_loss: 0.4127 - val_accuracy: 0.7575 - val_recall: 0.7851 - val_precision: 0.2269\n",
      "Epoch 491/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4888 - accuracy: 0.7667 - recall: 0.8147 - precision: 0.7329 - val_loss: 0.4133 - val_accuracy: 0.7574 - val_recall: 0.7856 - val_precision: 0.2270\n",
      "Epoch 492/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4896 - accuracy: 0.7659 - recall: 0.8146 - precision: 0.7320 - val_loss: 0.4130 - val_accuracy: 0.7577 - val_recall: 0.7851 - val_precision: 0.2271\n",
      "Epoch 493/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4890 - accuracy: 0.7666 - recall: 0.8146 - precision: 0.7329 - val_loss: 0.4124 - val_accuracy: 0.7579 - val_recall: 0.7846 - val_precision: 0.2272\n",
      "Epoch 494/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4890 - accuracy: 0.7667 - recall: 0.8145 - precision: 0.7331 - val_loss: 0.4132 - val_accuracy: 0.7576 - val_recall: 0.7853 - val_precision: 0.2271\n",
      "Epoch 495/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4892 - accuracy: 0.7668 - recall: 0.8150 - precision: 0.7329 - val_loss: 0.4127 - val_accuracy: 0.7581 - val_recall: 0.7846 - val_precision: 0.2273\n",
      "Epoch 496/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4892 - accuracy: 0.7664 - recall: 0.8151 - precision: 0.7325 - val_loss: 0.4134 - val_accuracy: 0.7573 - val_recall: 0.7856 - val_precision: 0.2269\n",
      "Epoch 497/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4893 - accuracy: 0.7665 - recall: 0.8151 - precision: 0.7326 - val_loss: 0.4127 - val_accuracy: 0.7578 - val_recall: 0.7846 - val_precision: 0.2271\n",
      "Epoch 498/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4890 - accuracy: 0.7665 - recall: 0.8143 - precision: 0.7329 - val_loss: 0.4132 - val_accuracy: 0.7576 - val_recall: 0.7856 - val_precision: 0.2271\n",
      "Epoch 499/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4887 - accuracy: 0.7663 - recall: 0.8147 - precision: 0.7324 - val_loss: 0.4131 - val_accuracy: 0.7577 - val_recall: 0.7853 - val_precision: 0.2271\n",
      "Epoch 500/500\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.4892 - accuracy: 0.7663 - recall: 0.8144 - precision: 0.7326 - val_loss: 0.4133 - val_accuracy: 0.7579 - val_recall: 0.7841 - val_precision: 0.2271\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "\n",
    "initial_model, initial_history = train_model(500, x_partial, y_partial, x_val, y_val)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "veWIF0Ol8H7O",
    "outputId": "49f23137-d6a2-4848-90c2-f3b6c535f421"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1cAAAIjCAYAAADvBuGTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABzLUlEQVR4nO3de1yUZf7/8fcwclABUUBEoTBPaXkoK1eNTZMyNbNQU3PXQ5m7ppuHrM3tm6cOtlmmlpvlVtZvSy0i2y2zjNQ1NTPt4JqZlgc08YAKntHh/v1xOyMDMzDAwMzA6/l4zAPmnuu+5xqYbN5c1/W5LIZhGAIAAAAAlEuQrzsAAAAAAFUB4QoAAAAAvIBwBQAAAABeQLgCAAAAAC8gXAEAAACAFxCuAAAAAMALCFcAAAAA4AWEKwAAAADwAsIVAAAAAHgB4QoAqpikpCQNGzasTOd26dJFXbp08Wp/qiqLxaKpU6c67i9cuFAWi0W7d+8u8dzy/I7cGTZsmJKSkrx6TQBA6RCuAKCSrVu3TlOnTtXx48d93RUEmN9++01Tp07Vd9995+uuOOzevVsWi0XPPfecr7sCAD5Xw9cdAIDqZt26dZo2bZqGDRumqKgor19/+/btCgoq29/OPvvsMy/3pvr44x//qIEDByo0NLTCnuO3337TtGnTlJSUpHbt2jk9tmDBAuXn51fYcwMASka4AgA/lp+fr7y8PIWFhXl8Tnk+3IeEhJT53OrOarXKarX67PmDg4N99twAABPTAgGgEk2dOlUPP/ywJKlx48ayWCxO63QsFovGjBmjt99+W1dddZVCQ0O1fPlySdJzzz2nTp06KTo6WjVr1lT79u2VlpZW5DkKr+exrwVau3atJkyYoNjYWNWuXVt33XWXDh8+7HRu4TVXq1atksVi0bvvvqunnnpKCQkJCgsLU7du3bRz584izz1v3jxdccUVqlmzpm644QatWbPGo3VcV199tbp27VrkeH5+vho1aqR+/fo5ji1evFjt27dXRESEIiMj1bp1a82ZM8fttc+fP6969epp+PDhRR7Lzc1VWFiYJk6cKEnKy8vT5MmT1b59e9WpU0e1a9dWcnKyVq5cWWz/JddrrgzD0JNPPqmEhATVqlVLXbt21datW4uce/ToUU2cOFGtW7dWeHi4IiMj1aNHD33//feONqtWrdL1118vSRo+fLjjvbNw4UJJrtdcnTp1Sg899JASExMVGhqqFi1a6LnnnpNhGE7t7O+7pUuX6uqrr1ZoaKiuuuoqx3vPGw4dOqT77rtPcXFxCgsLU9u2bfXmm28WaVfS7/f8+fOaNm2amjVrprCwMEVHR+vGG2/UihUrvNZXACgrRq4AoBKlpqbq559/1qJFi/TCCy8oJiZGkhQbG+to88UXX+jdd9/VmDFjFBMT4/jAPGfOHN1xxx0aPHiw8vLytHjxYvXv318fffSRevXqVeJz/+Uvf1HdunU1ZcoU7d69W7Nnz9aYMWO0ZMmSEs995plnFBQUpIkTJyonJ0fPPvusBg8erA0bNjjavPzyyxozZoySk5M1fvx47d69W3feeafq1q2rhISEYq8/YMAATZ06VVlZWWrQoIHj+JdffqnffvtNAwcOlCStWLFCgwYNUrdu3fT3v/9dkrRt2zatXbtWY8eOdXnt4OBg3XXXXUpPT9crr7ziNDq3dOlSnTt3znH93Nxc/fOf/9SgQYN0//3368SJE3rttdfUvXt3ff3110Wm4pVk8uTJevLJJ9WzZ0/17NlTmzdv1q233qq8vDyndr/++quWLl2q/v37q3Hjxjp48KBeeeUV3XTTTfrxxx/VsGFDtWzZUtOnT9fkyZM1cuRIJScnS5I6derk8rkNw9Add9yhlStX6r777lO7du306aef6uGHH9b+/fv1wgsvOLX/8ssvlZ6ergceeEARERGaO3eu+vbtq7179yo6OrpUr7uwM2fOqEuXLtq5c6fGjBmjxo0b67333tOwYcN0/Phxx+/Ok9/v1KlTNWPGDI0YMUI33HCDcnNz9c0332jz5s265ZZbytVPACg3AwBQqWbOnGlIMnbt2lXkMUlGUFCQsXXr1iKPnT592ul+Xl6ecfXVVxs333yz0/HLL7/cGDp0qOP+G2+8YUgyUlJSjPz8fMfx8ePHG1ar1Th+/Ljj2E033WTcdNNNjvsrV640JBktW7Y0zp075zg+Z84cQ5KxZcsWwzAM49y5c0Z0dLRx/fXXG+fPn3e0W7hwoSHJ6ZqubN++3ZBkvPjii07HH3jgASM8PNzx2seOHWtERkYaFy5cKPZ6hX366aeGJOM///mP0/GePXsaV1xxheP+hQsXnF6nYRjGsWPHjLi4OOPee+91Oi7JmDJliuO+/eds/70eOnTICAkJMXr16uX0c//b3/5mSHL6HZ09e9aw2WxO19+1a5cRGhpqTJ8+3XFs48aNhiTjjTfeKPIahw4dalx++eWO+0uXLjUkGU8++aRTu379+hkWi8XYuXOn02sJCQlxOvb999+7/J0UtmvXLkOSMXPmTLdtZs+ebUgy/vWvfzmO5eXlGR07djTCw8ON3NxcwzA8+/22bdvW6NWrV7F9AgBfYVogAPiZm266Sa1atSpyvGbNmo7vjx07ppycHCUnJ2vz5s0eXXfkyJGyWCyO+8nJybLZbNqzZ0+J5w4fPtxpxMc+avLrr79Kkr755htlZ2fr/vvvV40alyZFDB48WHXr1i3x+s2bN1e7du2cRtFsNpvS0tLUu3dvx2uPiorSqVOnSj0F7Oabb1ZMTIzT9Y8dO6YVK1ZowIABjmNWq9XxOvPz83X06FFduHBB1113ncc/Z7vPP/9ceXl5+stf/uL0cx83blyRtqGhoY4iJDabTdnZ2QoPD1eLFi1K/bx2y5Ytk9Vq1YMPPuh0/KGHHpJhGPrkk0+cjqekpKhJkyaO+23atFFkZKTjd1wey5YtU4MGDTRo0CDHseDgYD344IM6efKkVq9eLcmz329UVJS2bt2qHTt2lLtfAOBthCsA8DONGzd2efyjjz7S7373O4WFhalevXqKjY3Vyy+/rJycHI+ue9lllzndt4eeY8eOlftce0Br2rSpU7saNWp4vPfSgAEDtHbtWu3fv1+Sucbo0KFDTuHngQceUPPmzdWjRw8lJCTo3nvv9WhdUI0aNdS3b199+OGHOnfunCQpPT1d58+fd7q+JL355ptq06aNYz1PbGysPv74Y49/znb2n0mzZs2cjsfGxhYJnPn5+XrhhRfUrFkzhYaGKiYmRrGxsfrhhx9K/bwFn79hw4aKiIhwOt6yZUun/tkV/h1L5u/Zk/eHJ31p1qxZkSqWhfviye93+vTpOn78uJo3b67WrVvr4Ycf1g8//FDuPgKANxCuAMDPFByhsluzZo3uuOMOhYWF6R//+IeWLVumFStW6J577ilSnMAdd5XsPDm/POd6asCAATIMQ++9954k6d1331WdOnV02223OdrUr19f3333nf7973871hP16NFDQ4cOLfH6AwcO1IkTJxwjNu+++66uvPJKtW3b1tHmX//6l4YNG6YmTZrotdde0/Lly7VixQrdfPPNFVrm/Omnn9aECRP0+9//Xv/617/06aefasWKFbrqqqsqrbx6ZfyOS+LJ7/f3v/+9fvnlF73++uu6+uqr9c9//lPXXnut/vnPf1ZaPwHAHcIVAFSyglPEPPX+++8rLCxMn376qe6991716NFDKSkpFdC7srn88sslqUgFwQsXLjhVzytO48aNdcMNN2jJkiW6cOGC0tPTdeeddxYpLR8SEqLevXvrH//4h3755Rf96U9/0ltvveWyemFBv//97xUfH68lS5boyJEj+uKLL4qMWqWlpemKK65Qenq6/vjHP6p79+5KSUnR2bNnPXoNBdl/JoWnrx0+fLjIaFBaWpq6du2q1157TQMHDtStt96qlJSUIhtNl+a9c/nll+u3337TiRMnnI7/9NNPTv2rDJdffrl27NhRJCi66osnv1979cdFixYpMzNTbdq00dSpUyvltQBAcQhXAFDJateuLUlFPjgXx2q1ymKxyGazOY7t3r1bS5cu9XLvyua6665TdHS0FixYoAsXLjiOv/3226WaVjZgwAB99dVXev3113XkyJEi4Sc7O9vpflBQkNq0aSNJjul+7gQFBalfv376z3/+o//3//6fLly4UOT69tGbgqM1GzZs0Pr16z1+DXYpKSkKDg7Wiy++6HS92bNnF2lrtVqLjBC99957jimSdqV57/Ts2VM2m00vvfSS0/EXXnhBFotFPXr08PCVlF/Pnj2VlZXltObtwoULevHFFxUeHq6bbrpJkme/38JtwsPD1bRp0xJ//wBQGSjFDgCVrH379pKkxx57TAMHDlRwcLB69+7t+ODsSq9evTRr1izddtttuueee3To0CHNmzdPTZs29Yv1JiEhIZo6dar+8pe/6Oabb9bdd9+t3bt3a+HChWrSpInHIy533323Jk6cqIkTJ6pevXpFRudGjBiho0eP6uabb1ZCQoL27NmjF198Ue3atXOs3ynOgAED9OKLL2rKlClq3bp1kXNuv/12paen66677lKvXr20a9cuzZ8/X61atdLJkyc9/4HIXFs1ceJEzZgxQ7fffrt69uypb7/9Vp988omjBH/B550+fbqGDx+uTp06acuWLXr77bd1xRVXOLVr0qSJoqKiNH/+fEVERKh27drq0KGDy3V6vXv3VteuXfXYY49p9+7datu2rT777DN9+OGHGjdunFPxCm/IyMhwOcJ35513auTIkXrllVc0bNgwbdq0SUlJSUpLS9PatWs1e/Zsx7owT36/rVq1UpcuXdS+fXvVq1dP33zzjdLS0jRmzBivvh4AKBOf1SkEgGrsiSeeMBo1amQEBQU5le+WZIwePdrlOa+99prRrFkzIzQ01LjyyiuNN954w5gyZYpR+J9yd6XYN27c6NTOXmZ95cqVjmPuSrG/9957Tufay28XLgk+d+5c4/LLLzdCQ0ONG264wVi7dq3Rvn1747bbbvPsB2MYRufOnQ1JxogRI4o8lpaWZtx6661G/fr1jZCQEOOyyy4z/vSnPxkHDhzw6Nr5+flGYmKiyxLl9seffvppx2u45pprjI8++qhImXPDKLkUu2EYhs1mM6ZNm2bEx8cbNWvWNLp06WL873//K/I7Onv2rPHQQw852nXu3NlYv359kd+HYRjGhx9+aLRq1cqoUaOG0+/AVR9PnDhhjB8/3mjYsKERHBxsNGvWzJg5c6ZTaXj7a3H1vivcT1fs7wV3t//3//6fYRiGcfDgQWP48OFGTEyMERISYrRu3brI+8eT3++TTz5p3HDDDUZUVJRRs2ZN48orrzSeeuopIy8vr9h+AkBlsBhGJa5UBQBUK/n5+YqNjVVqaqoWLFjg6+4AAFChWHMFAPCKs2fPFlk39NZbb+no0aPq0qWLbzoFAEAlYuQKAOAVq1at0vjx49W/f39FR0dr8+bNeu2119SyZUtt2rTJaRNiAACqIgpaAAC8IikpSYmJiZo7d66OHj2qevXqaciQIXrmmWcIVgCAaoGRKwAAAADwAtZcAQAAAIAXEK4AAAAAwAtYc+VCfn6+fvvtN0VERHi88SUAAACAqscwDJ04cUINGzZUUFDxY1OEKxd+++03JSYm+robAAAAAPxEZmamEhISim1DuHIhIiJCkvkDjIyM9HFvAAAAAPhKbm6uEhMTHRmhOIQrF+xTASMjIwlXAAAAADxaLkRBCwAAAADwAsIVAAAAAHgB4QoAAAAAvIA1VwAAAEAlsdlsOn/+vK+7gQKsVqtq1KjhlS2YCFcAAABAJTh58qT27dsnwzB83RUUUqtWLcXHxyskJKRc1yFcAQAAABXMZrNp3759qlWrlmJjY70ySoLyMwxDeXl5Onz4sHbt2qVmzZqVuFFwcQhXAAAAQAU7f/68DMNQbGysatas6evuoICaNWsqODhYe/bsUV5ensLCwsp8LQpaAAAAAJWEESv/VJ7RKqfreOUqAAAAAFDNEa4AAAAAwAsIVwAAAECAsNmkVaukRYvMrzZbxT5fly5dNG7cuIp9kiqEghYAAABAAEhPl8aOlfbtu3QsIUGaM0dKTfVdv3AJI1cAAACAn0tPl/r1cw5WkrR/v3k8Pd03/YIzwpUfq+xhXwAAAFQOw5BOnfLslpsrPfigeY6r60jmiFZurmfXK+sexseOHdOQIUNUt25d1apVSz169NCOHTscj+/Zs0e9e/dW3bp1Vbt2bV111VVatmyZ49zBgwc7StE3a9ZMb7zxRtk64seYFuinGPYFAACouk6flsLDvXMtwzA/M9ap41n7kyel2rVL/zzDhg3Tjh079O9//1uRkZH661//qp49e+rHH39UcHCwRo8erby8PP33v/9V7dq19eOPPyr84ot8/PHH9eOPP+qTTz5RTEyMdu7cqTNnzpS+E36OcOWH7MO+hf+qYB/2TUsjYAEAAKDy2EPV2rVr1alTJ0nS22+/rcTERC1dulT9+/fX3r171bdvX7Vu3VqSdMUVVzjO37t3r6655hpdd911kqSkpKRKfw2VgWmBfsZmM0esihv2HTeOKYIAAACBrFYtcwTJk9vFmXUlWrbMs+vVqlX6/m7btk01atRQhw4dHMeio6PVokULbdu2TZL04IMP6sknn1Tnzp01ZcoU/fDDD462o0aN0uLFi9WuXTs98sgjWrduXek7EQAIV35mzZqiCxULMgwpM9NsBwAAgMBksZhT8zy53XqruTzEYnF/rcREs50n13N3nfIaMWKEfv31V/3xj3/Uli1bdN111+nFF1+UJPXo0UN79uzR+PHj9dtvv6lbt26aOHFixXTEhwhXfubAAe+2AwAAQGCzWs1191LRYGS/P3u22a6itGzZUhcuXNCGDRscx7Kzs7V9+3a1atXKcSwxMVF//vOflZ6eroceekgLFixwPBYbG6uhQ4fqX//6l2bPnq1XX3214jrsI4QrPxMf7912AAAACHypqea6+0aNnI8nJFTOevxmzZqpT58+uv/++/Xll1/q+++/1x/+8Ac1atRIffr0kSSNGzdOn376qXbt2qXNmzdr5cqVatmypSRp8uTJ+vDDD7Vz505t3bpVH330keOxqoSCFn4mOdn8j2T/ftfrriwW8/Hk5MrvGwAAAHwnNVXq08dcHnLggPnH9uTkih2xKuiNN97Q2LFjdfvttysvL0+///3vtWzZMgUHB0uSbDabRo8erX379ikyMlK33XabXnjhBUlSSEiIJk2apN27d6tmzZpKTk7W4sWLK6fjlchiGGWtdF915ebmqk6dOsrJyVFkZGSlP7+9WqDkHLDsw75UCwQAAAgsZ8+e1a5du9S4cWOFhYX5ujsopLjfT2myAdMC/ZCvh30BAAAAlB7hyk+lpkq7d0s9e5r377tP2rWLYAUAAAD4K8KVH7NaJfvea/HxlTefFgAAAEDpEa78nH3K59mzvu0HAAAAgOIRrvwc4QoAAAAIDIQrP0e4AgAAAAID4crP1axpfiVcAQAAAP6NcOXnGLkCAAAAAgPhys/Zw9WZM77tBwAAAIDiEa78mc2my35dpYFapBYHVkk2m697BAAAAF+y2aRVq6RFi8yvAfD5MCkpSbNnz/aorcVi0dKlSyu0PxWphq87ADfS06WxY5Wyb59SJGmzpKQEac4cdhIGAACoji5+PtS+fZeOJfD50J8wcuWP0tOlfv2c/8ORpP37zePp6b7pFwAAAHyDz4cBgXDlb2w28y8ShlH0MfuxceMCYggYAAAAbhiGdOqUZ7fcXOnBB4v/fDh2rNnOk+u5uo4br776qho2bKj8/Hyn43369NG9996rX375RX369FFcXJzCw8N1/fXX6/PPPy/PT8bJli1bdPPNN6tmzZqKjo7WyJEjdfLkScfjq1at0g033KDatWsrKipKnTt31p49eyRJ33//vbp27aqIiAhFRkaqffv2+uabb7zWN1cIV/5mzZqif5EoyDCkzEyzHQAAAALT6dNSeLhntzp1zBEqdwzD/PxYp45n1zt92uNu9u/fX9nZ2Vq5cqXj2NGjR7V8+XINHjxYJ0+eVM+ePZWRkaFvv/1Wt912m3r37q29e/eW56cjSTp16pS6d++uunXrauPGjXrvvff0+eefa8yYMZKkCxcu6M4779RNN92kH374QevXr9fIkSNlsVgkSYMHD1ZCQoI2btyoTZs26dFHH1VwcHC5+1Uc1lz5mwMHvNsOAAAAKKO6deuqR48eeuedd9StWzdJUlpammJiYtS1a1cFBQWpbdu2jvZPPPGEPvjgA/373/92hKCyeuedd3T27Fm99dZbql27tiTppZdeUu/evfX3v/9dwcHBysnJ0e23364mTZpIklq2bOk4f+/evXr44Yd15ZVXSpKaNWtWrv54gpErfxMf7912AAAA8D+1akknT3p2W7bMs2suW+bZ9WrVKlVXBw8erPfff1/nzp2TJL399tsaOHCggoKCdPLkSU2cOFEtW7ZUVFSUwsPDtW3bNq+MXG3btk1t27Z1BCtJ6ty5s/Lz87V9+3bVq1dPw4YNU/fu3dW7d2/NmTNHBwoMQEyYMEEjRoxQSkqKnnnmGf3yyy/l7lNJCFf+JjnZrPpycTizCItFSkw02wEAACAwWSxS7dqe3W691bPPh7fe6tn13F3Hjd69e8swDH388cfKzMzUmjVrNHjwYEnSxIkT9cEHH+jpp5/WmjVr9N1336l169bKy8sr70/II2+88YbWr1+vTp06acmSJWrevLm++uorSdLUqVO1detW9erVS1988YVatWqlDz74oEL7Q7jyN1arWU5TKvLGN+z3Z8822wEAAKDqK+bzoSrh82FYWJhSU1P19ttva9GiRWrRooWuvfZaSdLatWs1bNgw3XXXXWrdurUaNGig3bt3e+V5W7Zsqe+//16nTp1yHFu7dq2CgoLUokULx7FrrrlGkyZN0rp163T11VfrnXfecTzWvHlzjR8/Xp999plSU1P1xhtveKVv7hCu/FFqqpSWJjVq5Hy8UYJ5nH0MAAAAqhd3nw8TKufz4eDBg/Xxxx/r9ddfd4xaSeY6pvT0dH333Xf6/vvvdc899xSpLFie5wwLC9PQoUP1v//9TytXrtRf/vIX/fGPf1RcXJx27dqlSZMmaf369dqzZ48+++wz7dixQy1bttSZM2c0ZswYrVq1Snv27NHatWu1ceNGpzVZFYGCFv4qNVXq00e2W7rLujJDL+kBDd0yVxFRjFgBAABUSxc/H2rNGrO4WXy8uVSkEmY03XzzzapXr562b9+ue+65x3F81qxZuvfee9WpUyfFxMTor3/9q3Jzc73ynLVq1dKnn36qsWPH6vrrr1etWrXUt29fzZo1y/H4Tz/9pDfffFPZ2dmKj4/X6NGj9ac//UkXLlxQdna2hgwZooMHDyomJkapqamaNm2aV/rmjsUwSlHovprIzc1VnTp1lJOTo8jISJ/2xRh+rywL39CjmqGHDj2q2FifdgcAAABlcPbsWe3atUuNGzdWWFiYr7uDQor7/ZQmGzAt0M9ZwkIlSaE6pzNnfNwZAAAAAG4RrvxdqBmuwnRWZ8/6uC8AAABAGb399tsKDw93ebvqqqt83T2vYM2Vvwu9NHJFuAIAAECguuOOO9ShQweXjwUHB1dybyoG4crfXZzzSbgCAABAIIuIiFBERISvu1GhmBbo7xi5AgAAqDKoJeefvPV7IVz5O8IVAABAwLNeLJeel5fn457AldOnT0sq//REpgX6O8IVAABAwKtRo4Zq1aqlw4cPKzg4WEFBjHH4A8MwdPr0aR06dEhRUVGOEFxWhCt/F0opdgAAgEBnsVgUHx+vXbt2ac+ePb7uDgqJiopSgwYNyn0dwpW/K1CK/TgjVwAAAAErJCREzZo1Y2qgnwkODi73iJUd4crfMS0QAACgyggKClLYxWrQqHqY7OnvKMUOAAAABATClb9j5AoAAAAICIQrf0e4AgAAAAIC4crfUS0QAAAACAiEK39XIFx9/720apVks/m2SwAAAACKIlz5uc/XXCrFvny51LWrlJQkpaf7tl8AAAAAnBGu/Fh6ujRq/KVqgXb790v9+hGwAAAAAH9CuPJTNps0dqx0VpemBdoZhvl13DimCAIAAAD+gnDlp9askfbtk845wlWeJMPxuGFImZlmOwAAAAC+R7jyUwcOmF/t4UqSQpTnth0AAAAA3yJc+an4ePNrwXBVcGpg4XYAAAAAfItw5aeSk6WEBOm8QhzHwnRpF2GLRUpMNNsBAAAA8D3ClZ+yWqU5cyTDEqQ8BUu6NHJlsZhtZs822wEAAADwPcKVH0tNldLSpHMW53LsCQnm8dRUX/YOAAAAQEE1fN0BFC81VcqLDJVyTqhR9Dn9M82cCsiIFQAAAOBffD5yNW/ePCUlJSksLEwdOnTQ119/XWz748ePa/To0YqPj1doaKiaN2+uZcuWOR6fOnWqLBaL0+3KK6+s6JdRsULMohbhwefUpQvBCgAAAPBHPh25WrJkiSZMmKD58+erQ4cOmj17trp3767t27erfv36Rdrn5eXplltuUf369ZWWlqZGjRppz549ioqKcmp31VVX6fPPP3fcr1EjsAfojFAzXFnyilYLBAAAAOAffJo6Zs2apfvvv1/Dhw+XJM2fP18ff/yxXn/9dT366KNF2r/++us6evSo1q1bp+Bgs8hDUlJSkXY1atRQgwYNKrTvleriyFXQecIVAAAA4K98Ni0wLy9PmzZtUkpKyqXOBAUpJSVF69evd3nOv//9b3Xs2FGjR49WXFycrr76aj399NOy2WxO7Xbs2KGGDRvqiiuu0ODBg7V3795i+3Lu3Dnl5uY63fxK2MVwlXe2hIYAAAAAfMVn4erIkSOy2WyKi4tzOh4XF6esrCyX5/z6669KS0uTzWbTsmXL9Pjjj+v555/Xk08+6WjToUMHLVy4UMuXL9fLL7+sXbt2KTk5WSdOnHDblxkzZqhOnTqOW2JiondepLeEMnIFAAAA+LuAWoyUn5+v+vXr69VXX5XValX79u21f/9+zZw5U1OmTJEk9ejRw9G+TZs26tChgy6//HK9++67uu+++1xed9KkSZowYYLjfm5url8FLEtNsxR7jfxzys+XgnxehgQAAABAYT4LVzExMbJarTp48KDT8YMHD7pdLxUfH6/g4GBZC5TLa9mypbKyspSXl6eQkJAi50RFRal58+bauXOn276EhoYq9OLokD8KujgtMFTnlJcnhYX5uEMAAAAAivDZGEhISIjat2+vjIwMx7H8/HxlZGSoY8eOLs/p3Lmzdu7cqfz8fMexn3/+WfHx8S6DlSSdPHlSv/zyi+Lj4737AiqRpaZzuAIAAADgf3w6wWzChAlasGCB3nzzTW3btk2jRo3SqVOnHNUDhwwZokmTJjnajxo1SkePHtXYsWP1888/6+OPP9bTTz+t0aNHO9pMnDhRq1ev1u7du7Vu3TrdddddslqtGjRoUKW/Pm8pOHJ1jmVXAAAAgF/y6ZqrAQMG6PDhw5o8ebKysrLUrl07LV++3FHkYu/evQoqsMAoMTFRn376qcaPH682bdqoUaNGGjt2rP7617862uzbt0+DBg1Sdna2YmNjdeONN+qrr75SbGxspb8+b7EQrgAAAAC/ZzEMw/B1J/xNbm6u6tSpo5ycHEVGRvq6O9J990mvv66/6SmN+OVvuuIKX3cIAAAAqB5Kkw2oOxcILlawYOQKAAAA8F+Eq0AQyrRAAAAAwN8RrgJBKNUCAQAAAH9HuAoEwcGSpKv0P4V9tUqy2XzbHwAAAABFEK78XXq6NGeOJOlWfa42Y7tKSUnmcQAAAAB+g3Dlz9LTpX79pNxc5+P795vHCVgAAACA3yBc+SubTRo7VnJVKd9+bNw4pggCAAAAfoJw5a/WrJH27XP/uGFImZlmOwAAAAA+R7jyVwcOeLcdAAAAgApFuPJX8fHebQcAAACgQhGu/FVyspSQIFksrh+3WKTERLMdAAAAAJ8jXPkrq9VRgr0Ie+CaPdtsBwAAAMDnCFf+LDVVSkuTYmKcjyckmMdTU33TLwAAAABFEK78XWqq9M47kqRMJWjh0JXSrl0EKwAAAMDP1PB1B+CBsDBJ0mnV0s8Nu0jMBAQAAAD8DiNXgSAkRJIUrPM6d87HfQEAAADgEuEqEAQHS5JClEe4AgAAAPwU4SoQXAxXwTqvvDwf9wUAAACAS4SrQHBxWiAjVwAAAID/IlwFggIjV4QrAAAAwD8RrgJBgYIWTAsEAAAA/BPhKhBcHLkKVZ7OnTV83BkAAAAArhCuAsHFcCVJF87ZfNgRAAAAAO4QrgLBxWmBkpR/lnmBAAAAgD8iXAWCAiNXtrPnfdgRAAAAAO4QrgJBgXBFRQsAAADAPxGuAkFQkIwgqyRGrgAAAAB/RbgKEEYNc/Qq/xzhCgAAAPBHhKsAYQRfLGrBtEAAAADALxGuAoRxcd2VkcfIFQAAAOCPCFeBIoSRKwAAAMCfEa4Chb1i4HlGrgAAAAB/RLgKFBfXXOXnndfKlZLN5uP+AAAAAHBCuAoA6enSr/vMkasQ5enmm6WkJPM4AAAAAP9AuPJz6elSv37SmQtmuAqWOS1w/37zOAELAAAA8A+EKz9ms0ljx0qGIeXJnBYYIrOghWGYbcaNY4ogAAAA4A8IV35szRpp3z7z+/NyHrmSzICVmWm2AwAAAOBbhCs/duDApe/tI1cFw5WrdgAAAAB8g3Dlx+LjL31vH7myTwt01w4AAACAbxCu/FhyspSQIFksrqcFWixSYqLZDgAAAIBvEa78mNUqzZljfl+4oIXFYh6fPdtsBwAAAMC3CFd+LjVVSkuTatR0HrlKSDCPp6b6sncAAAAA7AhXASA1Vep556WCFs8+K+3aRbACAAAA/AnhKkAEhVwqaHHllUwFBAAAAPwN4SpQBF+aFnj2rI/7AgAAAKAIwlWgCLlU0IJwBQAAAPgfwlWgYOQKAAAA8GuEq0BRYOTq3Dkf9wUAAABAEYSrQMHIFQAAAODXCFeBgnAFAAAA+DXCVaBgWiAAAADg1whXgYKRKwAAAMCvEa4CBaXYAQAAAL9GuAoUjFwBAAAAfo1wFSgujlwF6zxrrgAAAAA/RLgKFBdHrpgWCAAAAPgnwlWgYFogAAAA4NcIV4GCUuwAAACAXyNcBQpGrgAAAAC/RrgKFAUKWhCuAAAAAP9DuAoUFLQAAAAA/BrhKlAUmBbImisAAADA/xCuAkWBghaMXAEAAAD+h3AVKChoAQAAAPg1wlWgKFDQgmmBAAAAgP/xebiaN2+ekpKSFBYWpg4dOujrr78utv3x48c1evRoxcfHKzQ0VM2bN9eyZcvKdc2AQEELAAAAwK/5NFwtWbJEEyZM0JQpU7R582a1bdtW3bt316FDh1y2z8vL0y233KLdu3crLS1N27dv14IFC9SoUaMyXzNgFJoWaBg+7g8AAAAAJxbD8N3H9A4dOuj666/XSy+9JEnKz89XYmKi/vKXv+jRRx8t0n7+/PmaOXOmfvrpJwVfDBvlvaYrubm5qlOnjnJychQZGVnGV+dle/dKl1+uswpVTZ3V2bNSaKivOwUAAABUbaXJBj4bucrLy9OmTZuUkpJyqTNBQUpJSdH69etdnvPvf/9bHTt21OjRoxUXF6err75aTz/9tGw2W5mvKUnnzp1Tbm6u083vFBi5ksS6KwAAAMDP+CxcHTlyRDabTXFxcU7H4+LilJWV5fKcX3/9VWlpabLZbFq2bJkef/xxPf/883ryySfLfE1JmjFjhurUqeO4JSYmlvPVVYCLBS2syleQbKy7AgAAAPyMzwtalEZ+fr7q16+vV199Ve3bt9eAAQP02GOPaf78+eW67qRJk5STk+O4ZWZmeqnHXhR06VfVTRk6e8rmw84AAAAAKMxn4SomJkZWq1UHDx50On7w4EE1aNDA5Tnx8fFq3ry5rFar41jLli2VlZWlvLy8Ml1TkkJDQxUZGel08yvp6VLr1o67n6m74jslmccBAAAA+AWfhauQkBC1b99eGRkZjmP5+fnKyMhQx44dXZ7TuXNn7dy5U/n5+Y5jP//8s+Lj4xUSElKma/q99HSpXz9p/36nwzUO7jePE7AAAAAAv+DTaYETJkzQggUL9Oabb2rbtm0aNWqUTp06peHDh0uShgwZokmTJjnajxo1SkePHtXYsWP1888/6+OPP9bTTz+t0aNHe3zNgGKzSWPHuqy7brEfGzfObAcAAADAp2r48skHDBigw4cPa/LkycrKylK7du20fPlyR0GKvXv3KqjAWqPExER9+umnGj9+vNq0aaNGjRpp7Nix+utf/+rxNQPKmjXSvn3uHzcMKTPTbNelS6V1CwAAAEBRPt3nyl/5zT5XixZJ99xTcrt33pEGDar4/gAAAADVTEDscwUPxMd7tx0AAACACkO48mfJyVJCgmSxuH7cYpESE812AAAAAHyKcOXPrFZpzhzz+0IBy9DF+7Nnm+0AAAAA+BThyt+lpkppaVKjRk6Hj0ckyPZumvk4AAAAAJ8jXAWC1FSlz9qt3dYrJEkPaaZiTuxS0vhUtrkCAAAA/AThKgCkp0v9Blh1yBYtSfpJLZUvq/azjzAAAADgNwhXfq7gPsJ5CpEkheqcpEt7C7OPMAAAAOB7hCs/V3Af4XMKlXQpXEnO+wgDAAAA8B3ClZ87cODS9/aRqxDlFdsOAAAAQOUjXPm5gvsDuxq5ctUOAAAAQOUjXPm5gvsIuwpX7CMMAAAA+AfClZ8ruI9w4WmBFvYRBgAAAPwG4SoA2PcRrlHLeeQqIcE8zj7CAAAAgO/V8HUH4JnUVCl/RYg03xy5atlS2rKFESsAAADAXzByFUCCwi6NXFmtBCsAAADAnxCuAknopXB18qSP+wIAAADACeEqkIRcKmhBuAIAAAD8C+EqkDByBQAAAPgtwlUgKTBydfasdOGCj/sDAAAAwIFwFUhCnUuxnzrly84AAAAAKIhwFUguhqswixmumBoIAAAA+A/CVSC5OC2wljVPEuEKAAAA8CeEq0ByceSqlpWRKwAAAMDfEK4CycWRq7AgRq4AAAAAf0O4CiT2NVdBjFwBAAAA/oZwFUgoaAEAAAD4LcJVICmwz5VEuAIAAAD8CeEqkBTa5+rECV92BgAAAEBBhKtAcnHkKthg5AoAAADwN4SrQHJx5Co4nzVXAAAAgL8hXAUSwhUAAADgtwhXgeTitECrjWmBAAAAgL8hXAWSiyNXNWznJBmEKwAAAMCPEK4CycWRK4thyCqbduyQVq2SbDbfdgsAAAAA4SqwXBy5ksxy7D/8IHXtKiUlSenpvusWAAAAAMJVQPlgmXO4stu/X+rXj4AFAAAA+BLhKkDYbNKD463Kl0WSFKI8x2OGYX4dN44pggAAAICvEK4CxJo10r79Fp2TOXpVcORKMgNWZqbZDgAAAEDlI1wFiAMHzK95MotaFBy5ctUOAAAAQOUiXAWI+Hjzq7uRq8LtAAAAAFQuwlWASE6WEhLchyuLRUpMNNsBAAAAqHyEqwBhtUpz5rieFmgxa1xo9myzHQAAAIDKR7gKIKmpUmxC0ZGrhAQpLc18HAAAAIBvEK4CTGS0OXLVNNEcuXriCWnXLoIVAAAA4GuEq0ATao5cJcSaI1eNGjEVEAAAAPAHhKtAczFc1a1thqtjx3zZGQAAAAB2hKtAE2JOC4yqaU4LPHrUl50BAAAAYEe4CjQXR67qhJkjV4QrAAAAwD8QrgKJzSbl5EiSLsvZoiDZCFcAAACAnyBcBYr0dCkpSVq7VpJ07eoXtFtJarkt3bf9AgAAACCJcBUY0tOlfv2kffucDjfSfj3+Qz/zcQAAAAA+RbjydzabNHasZBhFHgrSxWPjxpntAAAAAPgM4crfrVlTZMSqoCAZUmam2Q4AAACAzxCu/N2BA95tBwAAAKBCEK78XXy8R81s9T1rBwAAAKBiEK78XXKylJAgWSwuH86XRXuVqCuGJlPXAgAAAPAhwpW/s1qlOXPM7wsFrHyZ98dptjJ/s6ofhQMBAAAAnyFcBYLUVCktTUajRk6H9ylB/ZSmD5TqKCZI4UAAAADANwhXgSI1VasX7tb/6QlJ0la1VGPt0gdKdTQxKBwIAAAA+AzhKoAcOGTVenWUJOXLqnxZXbejcCAAAABQ6QhXASQ+XjqpcElSuE4W2w4AAABA5SJcBZDkZKl2XIQkKUInijxusUiJiWY7AAAAAJWLcBVArFbp4WmuR67shQRnzzbbAQAAAKhchKsA06O/Ga7CdE41dN5xPCFBSkszCwsCAAAAqHxlCldvvvmmPv74Y8f9Rx55RFFRUerUqZP27Nnjtc7BhfBwx7dL/mmOXlmt0q+/EqwAAAAAXypTuHr66adVs2ZNSdL69es1b948Pfvss4qJidH48eO92kEUEhJi3iTd3sUMVzabdOyYLzsFAAAAoEzhKjMzU02bNpUkLV26VH379tXIkSM1Y8YMrSnDJkvz5s1TUlKSwsLC1KFDB3399ddu2y5cuFAWi8XpFhYW5tRm2LBhRdrcdtttpe6X37o4ehWSd1L165uH9u/3YX8AAAAAlC1chYeHKzs7W5L02Wef6ZZbbpEkhYWF6cyZM6W61pIlSzRhwgRNmTJFmzdvVtu2bdW9e3cdOnTI7TmRkZE6cOCA4+ZqKuJtt93m1GbRokWl6pdfizArBurECUfZ9XfekVatMkexAAAAAFS+MoWrW265RSNGjNCIESP0888/q2fPnpKkrVu3KikpqVTXmjVrlu6//34NHz5crVq10vz581WrVi29/vrrbs+xWCxq0KCB4xYXF1ekTWhoqFObunXrlqpffu3iyNWaT05q+3bz0MyZUteuUlKSlJ7uu64BAAAA1VWZwtW8efPUsWNHHT58WO+//76io6MlSZs2bdKgQYM8vk5eXp42bdqklJSUSx0KClJKSorWr1/v9ryTJ0/q8ssvV2Jiovr06aOtW7cWabNq1SrVr19fLVq00KhRoxwjba6cO3dOubm5Tje/djFcPTf1pM6edX5o/36pXz8CFgAAAFDZapTlpKioKL300ktFjk+bNq1U1zly5IhsNluRkae4uDj99NNPLs9p0aKFXn/9dbVp00Y5OTl67rnn1KlTJ23dulUJCQmSzCmBqampaty4sX755Rf97W9/U48ePbR+/XpZXWwCNWPGjFL33ZeM8AhZJIW72EjYMMw9r8aNk/r0Yc8rAAAAoLKUaeRq+fLl+vLLLx33582bp3bt2umee+7RsQouW9exY0cNGTJE7dq100033aT09HTFxsbqlVdecbQZOHCg7rjjDrVu3Vp33nmnPvroI23cuFGrVq1yec1JkyYpJyfHccvMzKzQ11BeR8653kjYzjCkzEypDLVFAAAAAJRRmcLVww8/7Jg6t2XLFj300EPq2bOndu3apQkTJnh8nZiYGFmtVh08eNDp+MGDB9WgQQOPrhEcHKxrrrlGO3fudNvmiiuuUExMjNs2oaGhioyMdLr5s5MqPlzZHThQGb0BAAAAIJUxXO3atUutWrWSJL3//vu6/fbb9fTTT2vevHn65JNPPL5OSEiI2rdvr4yMDMex/Px8ZWRkqGPHjh5dw2azacuWLYq3l81zYd++fcrOzi62TSAJqWdWCywpXFWRlwsAAAAEhDKFq5CQEJ0+fVqS9Pnnn+vWW2+VJNWrV6/UxSAmTJigBQsW6M0339S2bds0atQonTp1SsOHD5ckDRkyRJMmTXK0nz59uj777DP9+uuv2rx5s/7whz9oz549GjFihCSz2MXDDz+sr776Srt371ZGRob69Omjpk2bqnv37mV5uX4nvpk5chXpYs2VZK65SkyUkpMrs1cAAABA9VamghY33nijJkyYoM6dO+vrr7/WkiVLJEk///yzo6iEpwYMGKDDhw9r8uTJysrKUrt27bR8+XJHkYu9e/cqKOhSBjx27Jjuv/9+ZWVlqW7dumrfvr3WrVvnGEmzWq364Ycf9Oabb+r48eNq2LChbr31Vj3xxBMKDQ0ty8v1O0GRl6YFWizmGis7i8X8Ons2xSwAAACAymQxjIIfzT2zd+9ePfDAA8rMzNSDDz6o++67T5I0fvx42Ww2zZ071+sdrUy5ubmqU6eOcnJy/HP91QsvSBMmaO/vB6vzr//Svn2XHoqNlebNk/r39133AAAAgKqiNNmgTCNXl112mT766KMix1944YWyXA6ldXGfq8uiTuiFF6ShQ6WLszR1+LA0YYI5apWa6sM+AgAAANVMmcKVZBaSWLp0qbZt2yZJuuqqq3THHXe43EcKXlarliQp59udmvfvVTqrZEmXfu72jYTT0ghYAAAAQGUp07TAnTt3qmfPntq/f79atGghSdq+fbsSExP18ccfq0mTJl7vaGXy62mB6enSyJFSdrbjUKYSNFZz9IEuJSmLRUpIkHbtYu0VAAAAUFalyQZlqhb44IMPqkmTJsrMzNTmzZu1efNm7d27V40bN9aDDz5Ypk7DA+np5pBUgWAlSY20X2nqp7uU7jjGRsIAAABA5SrTyFXt2rX11VdfqXXr1k7Hv//+e3Xu3FknTxa//5K/88uRK5tNSkqSU/WKAvJl0T4lqLF2Kb/AFMF33pEGDaqkPgIAAABVTIWPXIWGhurEiaJ7LJ08eVIhISFluSRKsmaN22AlSUEydJkylSznoSo2EgYAAAAqR5nC1e23366RI0dqw4YNMgxDhmHoq6++0p///Gfdcccd3u4jJOnAAY+axetSO6tVOnKkojoEAAAAoKAyhau5c+eqSZMm6tixo8LCwhQWFqZOnTqpadOmmj17tpe7CEkeD0Ed0KV2Npt0993mUi0AAAAAFatMa67sdu7c6SjF3rJlSzVt2tRrHfMlv15ztX+/Wa2iEHdrrqgaCAAAAJRdhWwiPGHChGIfX7lypeP7WbNmeXpZeMpqlebMMasFWixOAStfFknSOM12ClaSc9XALl0qs8MAAABA9eJxuPr22289amexWMrcGZQgNdXcGXjsWKfiFvuUoHGa7bTPVWEeLtkCAAAAUEYeh6uCI1PwodRUqU8fqXlz6ddftfPPM9Vi/vgiI1aFUTUQAAAAqFhlKmgBH7NapYYNJUmNuyapYYJVxQ0YJiZKycmV1DcAAACgmiJcBao6dSRJ1pO5mjOn+KYDB1LMAgAAAKhohKtAZa9UkpOj1FRp4kT3TZ97jnLsAAAAQEUjXAWqiyNXysmRzSYtWlR883HjzGruAAAAACoG4SpQ2UeucnO1Zo1T8cAiCpZjBwAAAFAxCFeBqsDIladl1inHDgAAAFQcwlWgKjBy5WmZ9R07Kq47AAAAQHVHuApUBUaukpOlRo1KPuXVV1l3BQAAAFQUwlWgKjByZbVKI0eWfMr+/dJTT1VstwAAAIDqinAVqAqMXElSs2aenTZlCmXZAQAAgIpAuApUBUauJHm87kqiLDsAAABQEQhXgarQyFVyspSQ4NmplGUHAAAAvI9wFajsI1enTkk2m6xWac4cz0+nLDsAAADgXYSrQGUPV5JjamBqqjRtmmenl2YaIQAAAICSEa4CVY0aUnCw+f2nnzoWUT32mGfTAw8erMC+AQAAANUQ4SoQpadLSUnS+fPm/UGDzPvp6bJapVmzSr7EPfdIS5ZUZCcBAACA6oVwFWjS06V+/aR9+5yP799vHk9PV2xsyZfJz5cGDpQeeaRiugkAAABUN4SrQGKzSWPHSoZR9DH7sXHjlLXf8zrrM2dKaWle6h8AAABQjRGuAsmaNUVHrAoyDCkzU1ceLl2d9fvuY98rAAAAoLwIV4HEw/rpbWIPeLznlWQWG3zqqTL2CQAAAIAkwlVg8bB+elCj+FLteSVJzz0n5eWVoU8AAAAAJBGuAktyslln3WJx/bjFIiUmSsnJSk2V3n1XCvLwN3zihBQba9bLAAAAAFB6hKtAYrXKMSRVOGDZ78+ebbaT1L+/9M47nl8+N1fq21d6773ydxUAAACobghXgSY11Szv16iR8/GEBPN4aqrT4QEDzFtpDBpEBUEAAACgtAhXgSg1Vdq9W/r73837TZpIu3YVCVZ2b78tRUd7fnmbzRz1YoogAAAA4DnCVaCyWqWuXc3vz51zTAV01/TVV0v/FPffT4l2AAAAwFOEq0AWE2N+PXLE9cbCBaSmStOmle7yR49KgweXsW8AAABANUO4CmT2cHX2rHT6dInNH3us6FKtkixZIk2ezAgWAAAAUBLCVSALD5dCQszvs7NLbG61SnPnlv5pnnhCiotjDRYAAABQHMJVILNYLlWqOHLEo1NSU6X335fq1SvdU2Vnm2Xap05lFAsAAABwhXAV6Aquu/JQaqp06JA0dGjpn27aNEaxAAAAAFcIV4GuDOFKMqcIvvaaFBlZ+qe0j2Kx2TAAAABwCeEq0NmnBX72mbRqVanm7Fmt0j//WfanHjCAYhcAAACAHeEqkKWnS598Yn7/5pvmvldJSaWas9e/v/Tww2V7esMwi13Uri1Nn07IAgAAQPVGuApU6elSv37SqVPOx/fvN4+XImA9+6y0eLFZH6Mszp2Tpkwxi2QwVRAAAADVFeEqENls0tixrjcOth8bN65UQ0kDBkjvvlu+buXmSnffLQ0cyCgWAAAAqh/CVSBas0bat8/944YhZWaa7UqhX7+ylWkvbMkScwuuYcOkt98u9VIwAAAAICARrgLRgQPebVeAvUz7lCmlPtXJ2bPmMrA//KFMS8EAAACAgEO4CkTx8d5tV4jVam4W7I1RLLt9+8zy7RS+AAAAQFVFuApEyclSQkLxFSgSE8125WAfxXr88XJdxsmUKVKDBhS+AAAAQNVTw9cdQBlYrdKcOeYiKXcGDjTbeeGppk83p/nNnFnuy0ky9zu++26pQwepWzdzdKxBA6lRIzMPeqHbAAAAQKWzGIarknPVW25ururUqaOcnBxFRkb6ujvuPfKI+8RjsUhpaebwk5ekpUn33WdWBawoMTHSP/5h7r8FAAAA+FppsgHTAgOVzSYtWlR8m1KWYy9Jv37S0aPS559LHTt67bJO7KNalHMHAABAoCFcBaoKKsdeEqvVnMq3bp25bioiwquXd1iyxLz21KmELAAAAAQGwlWgqsBy7J7q1086dszcgLginDkjTZsm1axpThP87DMpI8McsGPvLAAAAPgbCloEqgoux+4pq1VavNgMWhW1Huv8eXO9V1qa8/HoaKlLF6lFC4piAAAAwPcoaOFCQBS0sNnMnXn37zenABZmsZjl2nftqrSkYbOZI0rz5xcNQpWJohgAAADwltJkA8KVCwERriQpPf1SOfaCv0b7/lderhZYGunp0tixxS8Lq2iFS703aGAeP3TIHNBjhAsAAAAlIVyVU8CEK8l1iklMlGbP9lmwsrPZzHoazz0nffyxT7viUnS0NGaMOa2QsAUAAABXCFflFFDhSjJTzPjx0osvSr//vfTFF36XEtLSpAcekA4f9nVP3IuMlIYMkZo0kWJjWb8FAACA0mUDClpUBVarGapefFHKz/fLNNCvn3TXXeZI1v790sGDUna29NNP0iefmJUBfS03V3rpJedjERHSrbeaUwyPHzePRUWZ3wcFmQU1unTxyx85AAAAKhkjVy4E3MiVJH35pTnM0rSptGOHr3tTKjabNHiwubdVILIHsJYtCVsAAABVDdMCyykgw9XOnVKzZlJ4uHTihK97UyZpaRVXzr0yhYaaI12dOpnFNAqOciUnmxsw799vTpFk+iEAAIB/I1yVU0CGq5MnzSEUyQxX4eG+7U8ZFSzn/tFH0tmzvu5R5YiIkG65RbrySnPa4dGjZo2Syy6Tbr6Z0TAAAABfCbhwNW/ePM2cOVNZWVlq27atXnzxRd1www0u2y5cuFDDhw93OhYaGqqzBT6FG4ahKVOmaMGCBTp+/Lg6d+6sl19+Wc2aNfOoPwEZriSpdm3p9Glp1izpmmsCfjikugYtV8LCpF69zBExe/BKSDD39GLzZAAAgIoTUAUtlixZogkTJmj+/Pnq0KGDZs+ere7du2v79u2qX7++y3MiIyO1fft2x32LfV+ni5599lnNnTtXb775pho3bqzHH39c3bt3148//qiwsLAKfT0+k54unTtnfj9hgvk1IUGaM8fnJdnLymo196nq1s0MWk89Zb6co0cvtYmIMMuo//yz7/pZGc6eld5/37y5ExEhpaSYGTsh4dKURMm5CIc9hB06JNn/E7N/by+fL7F+DAAAoLR8PnLVoUMHXX/99XrpYpm2/Px8JSYm6i9/+YseffTRIu0XLlyocePG6bj9U2MhhmGoYcOGeuihhzRx4kRJUk5OjuLi4rRw4UINHDiwyDnnzp3TOXswkZlOExMTA2fkyr6ZcOFfpR9sJuxt9g//Bw44700VCKXeA1HNmlKPHlKrVsWHLXe/FwAAgEAXMNMC8/LyVKtWLaWlpenOO+90HB86dKiOHz+uDz/8sMg5Cxcu1IgRI9SoUSPl5+fr2muv1dNPP62rrrpKkvTrr7+qSZMm+vbbb9WuXTvHeTfddJPatWunOXPmFLnm1KlTNW3atCLHAyJc2WxSUpLzJsIFWSzmMMauXVX+0679A37hUu8ZGVJOjq97VzW4Ktbh6mdct67Up485klZwymLBEFZw1IxABgAA/FXATAs8cuSIbDab4uLinI7HxcXpp59+cnlOixYt9Prrr6tNmzbKycnRc889p06dOmnr1q1KSEhQVlaW4xqFr2l/rLBJkyZpgn0qnS6NXAWENWvcByvJHM3KzDTbdelSad3yBavV9Uss+IF++3Zp3jzpyJFK716VcO6c9N//mrfiHDsmLVxo3iRzyuJVV0nbtrkPusVt4lwwOB8+LEVHm+GZaosAAMCf+HzNVWl17NhRHTt2dNzv1KmTWrZsqVdeeUVPPPFEma4ZGhqq0NBQb3Wxch044N12VVDh0PX440VHT/7zHzMIMMJVMU6ckL76qvg27jZxLimU2dvZN3t2VfCjQQMzoK1eLe3d6/matPh4c5Ru3TqmPAIAgJL5NFzFxMTIarXq4MGDTscPHjyoBg0aeHSN4OBgXXPNNdq5c6ckOc47ePCg4uPjna5ZcJpglVHgNXqlXTXgaoSrWzfp+eedQ5d9tCQ//9KH759+Mpe4oXJ4Esrs7Uoq+FFWFovzcsbwcOnaa6Ubb5Ruusl8P2VlOY+oRUeb97OzzXPq1XMOefaiIYWDHMENAIDA5tNwFRISovbt2ysjI8Ox5io/P18ZGRkaM2aMR9ew2WzasmWLevbsKUlq3LixGjRooIyMDEeYys3N1YYNGzRq1KiKeBm+lZxs/hl+//6iBS2kS2uukpMrv28BxlXouvXWou3S06WxY51nY4aFmSEsL69CuwgfKPyf1cmTl6ZGPv20d5+rpIqPBUflXI28FW532WVFA6C7qZSFi5IwYgcAQOn5fFrghAkTNHToUF133XW64YYbNHv2bJ06dcqxl9WQIUPUqFEjzZgxQ5I0ffp0/e53v1PTpk11/PhxzZw5U3v27NGIESMkmWXZx40bpyeffFLNmjVzlGJv2LChU9GMKsNqNeuT9+tX9E/s9mqBs2fzqciLUlPNYg2Fq+NJ5r5cX3zhPPXsq6+kZcuK7tNV+NcFnDghffCBd6/pLgBGR5sVNg1D+vHHokVJCr8/yzL1suBjkucjdFSfBAAEKp+HqwEDBujw4cOaPHmysrKy1K5dOy1fvtxRkGLv3r0KCgpytD927Jjuv/9+ZWVlqW7dumrfvr3WrVunVq1aOdo88sgjOnXqlEaOHKnjx4/rxhtv1PLly6vuHlepqWYt8sLDKTExZvWGKlKG3Z+4K55h35erMPuGyKtWmfe7dDE/MK5b51zdULo08rBunfT119V782RUnOxsqbhlqoWDvzenXoaHS3fdJdnrBhUcecvMlJYuNZ/Pzl2wK+3Inn1NneR6PzdG7wAA5eXzfa78UWnKLfqVtDTpD3+4tJmwFPAbCVd3BUOZfe2X/QPkqVPmKBlFOIDyCQuT2rUrWjjF3Xq7glsRSO6nZZY2AHoy7bNePXNNqH1tn71qZnGjg/Z/R+yj6pddJt18s+tQWZFbJDAiCSBQBcw+V/4qIMNVNdpIGJcU3tvr8GH3H86++kpascJ5RMAVpisCgc8+OnjqlOtpyZL7UFlQRIR0yy3SlVeWLhwWni66YIH06afO//7ExJgVQuPiLm2zUHBNoFR078KgoOJHG70d2AiEACTCVbkFXLhiI2F4qPB+Ua7+4m2fCmX/QLN2rfsPZwBQEcLCzCB1+rTrx2vWlNq0MdcLFp5CWlxRGFfTQwvPCii4XrbwH6Q8LTpTuJiMuz9+uQqLBWcr2CuNFi5CU3i0seB6x4LPXXAE0v56vR0USxNACasIVISrcgq4cLVqldS1a8ntVq6s8hsJo2IUnlZU+EOF/QPAjh1s0gwApWUfRfzhB/eB0h7ssrOlb78teRZC4etbLNKZM5eOuVr76Ek4LDit9IMPzMI4hw87X9fV9FlX6ykTEqRZs8x2Bf//Unjk09UaydIovO65MrfBIFBWDYSrcgq4cLVokXTPPSW3e+cdadCgiu8PqjVXf1G1/3W44HqRgqNmpdnEmWmLAOB79pG2yhYaaha3KbzXoLvRwZ9+kpYvdx9apbIVzfFkk/qffipaidVd+Cy8nrLgXonu1l0W/P9ocT8D+3kFR0Gl0oW+ktZueiKQgybhqpwCLlwxcoUqoKRQZv+fQsEKbq42ey6u4EdxBQk8XZMGAEAgczXttriA6W5LGfu1evUqPpjWq2eOXL7zTtFRTleVY+1TdytrdNEThKtyCrhwZV9zVdJGwqy5QjVT2r+SFV6TVrgam/2vk19+WXRaTuERtchIcwqPVPIeUgAAwD1fF78mXJVTwIUr6VK1QMn1RsJUCwS8qjR7IhXX1l76uqQpHZ6W6i64T1pp12UAAOCPfP1xlnBVTgEZriQzYBXeSDgxUZo9m2AFVEOFQ92hQ9KYMc7TMuxTQTp2dL9/UlmmXkZHsw8bAMB7fDkRi3BVTgEbriTzU9ATT0jTpkkNG0pvvVW20joAqiRvLygu7nrFraMrPNpW1nLZno7ssaYOAKoGX5QQIFyVU0CHq/R0adQo80/Udr6eqAoAfsDV9Mw1a5z3FCpu9M5VNTDJ9bTMsgTAkqZ9uqo8Vpr1e6GhUpMm0q+/sm8dgMDli+LXhKtyCthwZV935e5X+u67Uv/+ldsnAIDXlLR+z9XooKcb1RYs6Wwv6FK4HLQn4dDVdNGICKl7d6llS+nll0u/F17NmubXgvs0AaieGLkKQAEZruwVAwuutyrMapUWL75U+AIAgArgyXRRe1XO2NhL2ywUHkl0tS/P/v1m8ZfsbOeSzSUVhXG1iW1J+xuV5vrFFZOJjJS6dZNq1/asvLXkulx2ccLDzb2HMjOZ/oqqKzGRNVcBKSDDlad7XUnS++8zRRAAUO1U1iamnj6Pu1FEd4HSHuwuu+zSBrqF9wFyt9ZRcr2OsTThMDtb+vrrooEwPFx66CEzIJc0ffbnn6VPPy0aAO2bA7sqhhMWZvY3L690vwdX2AojcFksVAsMWAEZrhYtku65x7O2vor9AADA75U08mgPhJLzlNPSXL+4a7h6fsls/8UXrvcadDU6WHAT29JuRF/aqa/uNqkvOO3W3dpNV+sp7XslXnll8esuC3L3M/jpJ2n5cs9HQYvjq7Wb0dHSq6+yz1XACshwVZqRK8k3E1YBAAC8oLJGISvr+Usz2llwZFIqOnrp7vru1lp6EjBLWrvp7jxXgTAmxhx5bdHCefTU1dTdevXMXYYee8y3YwKEq3IKyHDlyZqrgnxRagUAAADVSmmCqK9DszulyQY1KqlPqGhWq1luvW9fz9rHx1dsfwAAAFDtWa2eT5YqTVt/FeTrDsCLUlPNcuslRfzo6EsTmAEAAAB4BeGqqunf3yy3XpzsbOnDDyunPwAAAEA1Qbiqiu66yxydcsdikcaNMye2AgAAAPAKwlVVtGaNOTrljmGYJVnsG18AAAAAKDfCVVV04IB32wEAAAAoEeGqKvK0EiAVAwEAAACvIVxVRcnJ5g5uFov7NgkJVAwEAAAAvIhwVRXZ97yS3AesM2eoGAgAAAB4EeGqqkpNldLSpHr1XD+enW1uOJyeXrn9AgAAAKoowlVV1qePVLNm8W1GjqQkOwAAAOAFhKuqbM0aad++4ttkZ0tPPVU5/QEAAACqMMJVVeZpqfW5cxm9AgAAAMqJcFWVeVpqPTubDYUBAACAciJcVWXJye4LWhTGhsIAAABAuRCuqjKrVRo71rO2O3ZUbF8AAACAKo5wVdU99phno1cLFrDuCgAAACgHwlVV5+no1b59rLsCAAAAyoFwVR00a+ZZO9ZdAQAAAGVGuKoOPK0a6Gk7AAAAAEUQrqqD5GQpIUGyWNy3sVqlI0cqr08AAABAFUO4qg6sVmnOnOLb2GzS3XdL6emV0ycAAACgiiFcVRepqdKSJWbQcscwpHHjqBoIAAAAlAHhqjqJjS05OGVmUjUQAAAAKAPCVXXiaTXADz+s2H4AAAAAVRDhqjrxtBrgv/7F1EAAAACglAhX1UlyshQTU3K7I0ekp56q+P4AAAAAVQjhqjqxWqU//MGztlOmUDkQAAAAKAXCVXXTp4/nbceOZXogAAAA4CHCVXVj31DYE/v2SSNGELAAAAAADxCuqhtPNhQuaOFCKS6OKYIAAABACQhX1VFqqjRtmufts7Olfv0IWAAAAEAxCFfV1WOPeT49UJIMQxo3jimCAAAAgBuEq+rKapVmzSrdOZmZ0po1FdMfAAAAIMARrqqz2NjSn3PggPf7AQAAAFQBhKvqrCxBaccO7/cDAAAAqAIIV9VZfHzpz1mwgHVXAAAAgAuEq+rMvueVxeL5Ofv2se4KAAAAcIFwVZ0V3POqNAHrgw8qpj8AAABAACNcVXepqVJamtSokefnzJ0rPfJIxfUJAAAACECEK5gBa/du6fPPpXr1PDtn5kwzlAEAAACQRLiCndUqdetmFqzw1H33SXl5FdcnAAAAIIAQruAsNVUaN86ztrm55l5Z6ekV2iUAAAAgEBCuUFSfPp63zc2V+vaVpk+nRDsAAACqNcIVikpOlmJiSnfOlClSUhKjWAAAAKi2CFcoymqV/vGP0p+3b5/Urx8BCwAAANUS4Qqu9e8vPfxw2c4dN44pggAAAKh2CFdw79lnpalTS3eOYUiZmdKqVRXRIwAAAMBvEa5QvP/7v9JtMGx3991MDwQAAEC14hfhat68eUpKSlJYWJg6dOigr7/+2qPzFi9eLIvFojvvvNPp+LBhw2SxWJxut912WwX0vBqwWqW5c0t/3tGjZhVBAhYAAACqCZ+HqyVLlmjChAmaMmWKNm/erLZt26p79+46dOhQseft3r1bEydOVHJyssvHb7vtNh04cMBxW7RoUUV0v3pITZWmTSvbuUOGSBkZrMECAABAlefzcDVr1izdf//9Gj58uFq1aqX58+erVq1aev31192eY7PZNHjwYE2bNk1XXHGFyzahoaFq0KCB41a3bt2KegnVw2OPSQkJpT/v1CkpJYUy7QAAAKjyfBqu8vLytGnTJqWkpDiOBQUFKSUlRevXr3d73vTp01W/fn3dd999btusWrVK9evXV4sWLTRq1ChlZ2e7bXvu3Dnl5uY63VCI1SrNmSNZLGU7f98+NhsGAABAlebTcHXkyBHZbDbFxcU5HY+Li1NWVpbLc7788ku99tprWrBggdvr3nbbbXrrrbeUkZGhv//971q9erV69Oghm5sP9TNmzFCdOnUct8TExLK/qKosNVVKSyv9BsMFTZki1a1rViEkZAEAAKAK8fm0wNI4ceKE/vjHP2rBggWKKeYD/sCBA3XHHXeodevWuvPOO/XRRx9p48aNWuWmPPikSZOUk5PjuGVmZlbQK6gCUlOl/ful2NiyX+PECXMNV1QUUwUBAABQZdTw5ZPHxMTIarXq4MGDTscPHjyoBg0aFGn/yy+/aPfu3erdu7fjWH5+viSpRo0a2r59u5o0aVLkvCuuuEIxMTHauXOnunXrVuTx0NBQhYaGlvflVB8hIdL8+VK/fua+VmV18qQ5VfDdd81NiwEAAIAA5tORq5CQELVv314ZGRmOY/n5+crIyFDHjh2LtL/yyiu1ZcsWfffdd47bHXfcoa5du+q7775zO51v3759ys7OVnx8fIW9lmrHPkWwLHtgFTZokHktyZwquGqVtGiR+ZWpgwAAAAgQPh25kqQJEyZo6NChuu6663TDDTdo9uzZOnXqlIYPHy5JGjJkiBo1aqQZM2YoLCxMV199tdP5UVFRkuQ4fvLkSU2bNk19+/ZVgwYN9Msvv+iRRx5R06ZN1b1790p9bVVeaqrUp4/0xBNlL9UumQGqf3/p9tulDRukw4cvPZaQYBbSSE0tf38BAACACuTzcDVgwAAdPnxYkydPVlZWltq1a6fly5c7ilzs3btXQUGeD7BZrVb98MMPevPNN3X8+HE1bNhQt956q5544gmm/lUEq9UsTnH6tDRzZvmu9dFHRY/t329OP0xLI2ABAADAr1kMozyLZqqm3Nxc1alTRzk5OYqMjPR1dwJHWpo0apR05Ih3r2uxmCNYu3aZYQ4AAACoJKXJBgFVLRB+rl8/KSurfFMEXTEMKTNTevFF1mABAADAbxGu4F1WqzR5svT+++ZokzeNHy8lJVG+HQAAAH6JcIWKkZoq7d4trVwpjR0r1azpnevu22eWb58+nVEsAAAA+BXWXLnAmqsKYLNJ990nvfmm967ZqJE0cqTUrJkUHy8lJ7MmCwAAAF5VmmxAuHKBcFVBbDYpLk7Kzq6Y61O2HQAAAF5GQQv4J6tVevXViru+vWw7a7IAAADgA4QrVK7UVLPYRXS0969tGObt/vuljAzWZAEAAKBSEa5Q+VJTpYMHzZLt9ep5//pHj0opKVL9+uYGxxkZ0qJF0qpVBC4AAABUGNZcucCaq0pks0lr1kgvvWSOaFU01mUBAACgFChoUU6EKx+w2cw9rPbtq5znmzaNKoMAAAAoEQUtEHisVnNEyWIxbxVtyhTpnnukrl2lunWl/v1ZpwUAAIByIVzBf6SmSmlp5v5VlenECfN5U1KkqCg2KAYAAECZMC3QBaYF+ph9HdaBA9KOHWZRisp+m0ZGmmXj4+LMftinD0qX+saUQgAAgCqvNNmgRiX1CfCc1Sp16XLp/tVXS2PHVt56LEnKzZUGDnQ+Fh4uBQWZj9nVq2f27bHHCFkAAADVHCNXLjBy5Yfso1lLl5prs/xNZKQ0bJjUuLEUG2tObWSkCwAAIOBRLbCcCFd+Lj298keyysK+UXJ29qVjlIIHAAAIKFQLRNWWmirt3i2tXCmNG2eOGvmj7GznYCWZgbBvXzMcsqkxAABAlcLIlQuMXAUYm80MKqtWST/9ZFb+CxSMZAEAAPg1pgWWE+EqwAXKtMGCpk0zi2JIl4KiZBb26NKFdVoAAAA+QrgqJ8JVFVCwnHv9+ub3zz8vnTzp6565FxYm5edLeXnOxyMjpXvvlfr0oSAGAABAJSNclRPhqooqOH1QulTNb8oU6auvfNWr0omOlm66SapVS7rsMunmmxnZAgAAqECEq3IiXFVDDz8sPfecr3tRNuHhZv8ffVRat46y7wAAAF5EuConwlU1lZYmPfCAdPiwr3tSNkFB5rRCO1fFMgpOlySAAQAAlIhwVU6Eq2rMHj727zdDVmystGOH9NJLRcuqB4opU6ROnaQFC6RPP5VOnLj0WESENGGC9PjjhCwAAAAXCFflRLhCETab9NRTZlCpikJDpb/9zaxYSMgCAABwIFyVE+EKbrkq8x4TI50969+VCD0VESHdckvRghlS0RLxycnmGq/9+6WDB82RvaCgS1MNDx0yKzVK5vdMQwQAAAGIcFVOhCsUy9W6pQ8/lPr1Mx+vav9JhYebXwuHR4ul9K81Jkb6xz+k/v09P4d1YgAAwIdKkw2CKqlPQNVhtZojN4MGXSqDnppqFsRo1Mj9eQkJ0oABldVL7zl50vWoXFlC5JEj0t13S717m6NgNlvx7dPTpaQkqWtX6Z57zK9JSeZxAAAAP8PIlQuMXKHMCm9eLBWdEudqamF11aiRNHKk1KyZ88+ouDVuFov5NS3NuRIiAABABWBaYDkRrlDhCocw+/2ffjJDQ1mm3FUFMTHS735nbup85EjxbevUkV58UWrY0LyfleW89qvgurDiphSWdtqhq/YSUxcBAKiiCFflRLiCT5U0shUWZhbQQMkKh9R69cyf7aOPmmFo/vyi5ekjI6Vhw6S77ioaklz9bqKjza8FS/W72mPMFdaTAQDg9whX5US4gs8VN72wUyepSROzSh//+VasmBhzj7O4OGnpUjMwecKTqYvuKk/+4Q9Snz4ELQAA/AThqpwIV/B76elVtzphVZKQIO3ebX5fcITKXtijuN+dp6NfAACgQhGuyolwhYDgauSjbl2pXTtp5Ur35z3+uNSxo9S3r3TmTIV3s9pr1cocdSy4hsyTNXX2NtOmFS34Ibke3czKkg4flmJjzWIh7tqXdQqiJ9dgqiMAoIohXJUT4QoBw90HWVfBKzFRmj370kiIffSLfwICh7264tGj0sKFUk5O8e3t0xq3bzdHwY4evfRYdLT0wAOXfv9dulzaWsDdXm6F31OFR9fS0sxrHj58qY19ndtjj5n3CV4AgABDuConwhWqBE9GEFyFsLJUKgwJkfLyyt9n+FZEhHTVVdLWrc5FPiIinO8X1rev+R7asMF9m5o1zfdfwT3TYmLM/csaNzZH2xo0MN+3q1dLe/ea4a1ePen4cbN9vXpmG09G5STz2P79zqN5nTo5V5AsfJ/ABwAohHBVToQrVCuFP5h6sh7IruCoxIcfmqMqBavmARWluFG5iAgz8Lt6LwYFSfn57u/XrWsWFElJMYOc5FxMZt26ooGtrIGMKZQAEBAIV+VEuEK152pEKzbW3FcqLs79h0H75r+FP+zaRx8KfogFAo27UV17+f477rgUmCTnqZZ29sc//FB6+23XUygffdT9aJrNJq1aZd5cPYe7wFYRQa681yRcAggQhKtyIlwBKt8HH1fnfvCB1L9/xfYZ8Df2wNSkibRihfmHi+KmWLoSESHdcos5ylZ4XzbJ3PuuVy+pVi1zy4CCjyckSAMGSG+95Rzk7Nds1erSf9uHDjlvap6fL0VFmX8YCQpybrdjh7RggfMfYAqOZLsKlCWt4avsrQgIdwA8RLgqJ8IVUEFcjYiFh5sf3HJziz83MrLkNjVrmh+YWP8F+E50tPTqq2ahE1f/zXuyEXpMjPSPf5h/kCkcgopbJ1dcYCo46vfjj1JGhnNRGHfbHwRCCCtNHwPh9QB+hnBVToQroAJ5Wnyg8HoXd3/tjo2VBg++9NduyfXURFdTusLCpNtvl+6/3/xw8cIL0scfV9xrB6qT22+XPvqofNdo1szcYqDgaJx9mqOdvYrmsWNFp1rap2zWrSvNmuXZqGFqqvSnP5nP89FHRa9ZcIStYNBzteG7J4+5+3cwOtpcN1jS1gquRhHtQbFPH+ctG9asMad3F/y3kT31gBIRrsqJcAX4MU//6urqr91r1rhfq2Lnqpw4ALhSuCBKQcVVXi38WESEeb9gNc3C7EExOrpomHJ3fXtAK47FYn5dssQMcqUd0bL/W+tJKPRURa8d9Mc1iPBrhKtyIlwB1VzhDwu7d5t/2XX3YalfP3P0SzI/9Hz0UcnTngDAnxQOivapmamp7mcbuCrMUlhp1uJZra7/wJWQIA0aJC1a5Bwq7aOWrjZaL6jglNCffjK/FtzY3d1+fIVHHN1d39X01/KuIfTXsFbc7BN/66sXEa7KiXAFoAhPNma2s/+PfP78ogUIYmKk3//eLD5w2WXSTTeZ04OmTauMVwEApVN4H8Patc0/Mp0+Xbrr2EfeGjc2/2D1zjtFA1T79mZgKytXFTd37JDmzvVsm5CwMPP1ulvf6yrMffih+Qe24j5Ol3bqpav/33h7+mZJ6xNd7RN45Ig0frxzv6Kjza8Ff76eht4AQrgqJ8IVAJfK8pdET89x9T/T4tSqZU5r3LiRKYwA4Avh4ea/8WfOeNZ+yBAzJBWuvmkfIcvKMgutvPGG+2ssXux6S5TS/P/JXXibNUvatq3omuXyio6WxoyRWrQofj2iu+/9IKARrsqJcAXAJwr+z7Hw/2Tc7Z/kahPoP//Z9V9p7dMap0279BdFV3+JLOmvt4WnD0VESLfeKnXsaBYi+eUXaepU87Gy/C8mPFy69lrzf8D2PdIyMqQNG0p/LQCoytxNmYyONqdX2v8Ntk/fW7DAnHrpS8WtR3THx4VXCFflRLgCENDcbeZc3DTG4ubQe1LprPBfFN1tRN2hgxmSCk8Huv/+kqeQFFdsJCLC/B+2q0BYuLpc4f+x16snpaRIX3zhvA6jOGX5cAAAKBt74ZW0NJ8ELMJVORGuAFQJvl4QXREVv9ytBXBX0r9Ro6Jh0F04LM3IYXKyeY2lS6WFC533SyoLwhoAFM9iMf8Yt2tXpU8RJFyVE+EKAOCxkvYcsouMNEfIWrVyXm/haqsA++NZWdLBg9LatdKKFc7FURISpPvuM58/P1+KijKnUP78c9FCKpJZiKB/f+nmm83w6eqarnToIH3/PRUwAfiHlSvNP3JVIsJVORGuAABlVtzoWnn+2lqaEb+Cpacl9/u62a/pqqR2wWmk9qmmM2cWvxdTSSIipFtuca6WaQ+Z27dL8+Z5PjWzPMLCpAEDzED6xhvu1xcC8D/vvGOuM6tEhKtyIlwBAKodT8Kbu/V8xW2mK7nf68hdH/bvN0fssrPdj8SVxFXJcFf9sAfRL76QvvxS+vbb0j8XgMrDyFXgIVwBAFCMwkHM3bRGb5VRLjwSV3DapH10sEED87GCzymVb/uEwmvv1qyRXnzROViGh5vhsuDoV9265uaxdeoU3c+puMcKh9SEBLMK53vvleanVbzISEbqELgSE1lzFYgIVwAAwKWSqmu625DVk8fcFVt55BFzSqY706ZJTZqYQS062vyanV10P6WCG9+WZl89TxWuzAl4k8VCtcBARbgCAAB+xdVWBO62V/CEuxG6wiORP/1kfi24Fq5OHWnoUOmOO4qeZw+HxRV38URYWPFFVMLDndf/eVpxs3AATEiQOnc2i7sUHJGMiTGLv7z7bun7Du+LjpZefZV9rgIV4QoAAPgdX22vUNbnLa5gSmysNHiwdPvtZrvVq6W9e81CJzffbK6p+eAD94GyTx/PpqYWnDrqamuGkraIcLVnX0yM2b/iNuNt3958PQX7XrOm+fXMmZJ/dgX5w1YNPXuaFUbdbTvhaq/ByEhzmmt5iuB4ul6zghGuyolwBQAA4EXlDWi+2q+vuD642yx93jxz24PSbNDu7nt7cHzmmbJV6wwLM0NPwUAXHi717WtWyyxp3V/hqqHuRjvdTY8teMxVRdCCIVsqGoZ98ft2gXBVToQrAAAAlKgyw5+nWyJMmyY1a1a2NYHuRva8+Rp8HZbLgHBVToQrAAAA+CV3WyKUZw0eikW4KifCFQAAAPxagI4CBaLSZIMaldQnAAAAAN5itVb6ZrooWZCvOwAAAAAAVQHhCgAAAAC8gHAFAAAAAF5AuAIAAAAALyBcAQAAAIAXEK4AAAAAwAv8IlzNmzdPSUlJCgsLU4cOHfT11197dN7ixYtlsVh05513Oh03DEOTJ09WfHy8atasqZSUFO3YsaMCeg4AAAAAJp+HqyVLlmjChAmaMmWKNm/erLZt26p79+46dOhQseft3r1bEydOVHJycpHHnn32Wc2dO1fz58/Xhg0bVLt2bXXv3l1nz56tqJcBAAAAoJrzebiaNWuW7r//fg0fPlytWrXS/PnzVatWLb3++utuz7HZbBo8eLCmTZumK664wukxwzA0e/Zs/d///Z/69OmjNm3a6K233tJvv/2mpUuXVvCrAQAAAFBd+TRc5eXladOmTUpJSXEcCwoKUkpKitavX+/2vOnTp6t+/fq67777ijy2a9cuZWVlOV2zTp066tChg9trnjt3Trm5uU43AAAAACgNn4arI0eOyGazKS4uzul4XFycsrKyXJ7z5Zdf6rXXXtOCBQtcPm4/rzTXnDFjhurUqeO4JSYmlvalAAAAAKjmfD4tsDROnDihP/7xj1qwYIFiYmK8dt1JkyYpJyfHccvMzPTatQEAAABUDzV8+eQxMTGyWq06ePCg0/GDBw+qQYMGRdr/8ssv2r17t3r37u04lp+fL0mqUaOGtm/f7jjv4MGDio+Pd7pmu3btXPYjNDRUoaGh5X05AAAAAKoxn4arkJAQtW/fXhkZGY5y6vn5+crIyNCYMWOKtL/yyiu1ZcsWp2P/93//pxMnTmjOnDlKTExUcHCwGjRooIyMDEeYys3N1YYNGzRq1CiP+mUYhuM8AAAAANWXPRPYM0JxfBquJGnChAkaOnSorrvuOt1www2aPXu2Tp06peHDh0uShgwZokaNGmnGjBkKCwvT1Vdf7XR+VFSUJDkdHzdunJ588kk1a9ZMjRs31uOPP66GDRsW2Q/LnRMnTkgSa68AAAAASDIzQp06dYpt4/NwNWDAAB0+fFiTJ09WVlaW2rVrp+XLlzsKUuzdu1dBQaVbGvbII4/o1KlTGjlypI4fP64bb7xRy5cvV1hYmEfnN2zYUJmZmYqIiJDFYin1a/KW3NxcJSYmKjMzU5GRkT7rBwIH7xmUFu8ZlBbvGZQF7xuUlj+9ZwzD0IkTJ9SwYcMS21oMT8a34BO5ubmqU6eOcnJyfP6mQmDgPYPS4j2D0uI9g7LgfYPSCtT3TEBVCwQAAAAAf0W4AgAAAAAvIFz5sdDQUE2ZMoUy8fAY7xmUFu8ZlBbvGZQF7xuUVqC+Z1hzBQAAAABewMgVAAAAAHgB4QoAAAAAvIBwBQAAAABeQLgCAAAAAC8gXPmpefPmKSkpSWFhYerQoYO+/vprX3cJPvLf//5XvXv3VsOGDWWxWLR06VKnxw3D0OTJkxUfH6+aNWsqJSVFO3bscGpz9OhRDR48WJGRkYqKitJ9992nkydPVuKrQGWaMWOGrr/+ekVERKh+/fq68847tX37dqc2Z8+e1ejRoxUdHa3w8HD17dtXBw8edGqzd+9e9erVS7Vq1VL9+vX18MMP68KFC5X5UlBJXn75ZbVp00aRkZGKjIxUx44d9cknnzge5/2CkjzzzDOyWCwaN26c4xjvGxQ2depUWSwWp9uVV17peLwqvGcIV35oyZIlmjBhgqZMmaLNmzerbdu26t69uw4dOuTrrsEHTp06pbZt22revHkuH3/22Wc1d+5czZ8/Xxs2bFDt2rXVvXt3nT171tFm8ODB2rp1q1asWKGPPvpI//3vfzVy5MjKegmoZKtXr9bo0aP11VdfacWKFTp//rxuvfVWnTp1ytFm/Pjx+s9//qP33ntPq1ev1m+//abU1FTH4zabTb169VJeXp7WrVunN998UwsXLtTkyZN98ZJQwRISEvTMM89o06ZN+uabb3TzzTerT58+2rp1qyTeLyjexo0b9corr6hNmzZOx3nfwJWrrrpKBw4ccNy+/PJLx2NV4j1jwO/ccMMNxujRox33bTab0bBhQ2PGjBk+7BX8gSTjgw8+cNzPz883GjRoYMycOdNx7Pjx40ZoaKixaNEiwzAM48cffzQkGRs3bnS0+eSTTwyLxWLs37+/0voO3zl06JAhyVi9erVhGOZ7JDg42HjvvfccbbZt22ZIMtavX28YhmEsW7bMCAoKMrKyshxtXn75ZSMyMtI4d+5c5b4A+ETdunWNf/7zn7xfUKwTJ04YzZo1M1asWGHcdNNNxtixYw3D4N8ZuDZlyhSjbdu2Lh+rKu8ZRq78TF5enjZt2qSUlBTHsaCgIKWkpGj9+vU+7Bn80a5du5SVleX0fqlTp446dOjgeL+sX79eUVFRuu666xxtUlJSFBQUpA0bNlR6n1H5cnJyJEn16tWTJG3atEnnz593et9ceeWVuuyyy5zeN61bt1ZcXJyjTffu3ZWbm+sYzUDVZLPZtHjxYp06dUodO3bk/YJijR49Wr169XJ6f0j8OwP3duzYoYYNG+qKK67Q4MGDtXfvXklV5z1Tw9cdgLMjR47IZrM5vWkkKS4uTj/99JOPegV/lZWVJUku3y/2x7KyslS/fn2nx2vUqKF69eo52qDqys/P17hx49S5c2ddffXVksz3REhIiKKiopzaFn7fuHpf2R9D1bNlyxZ17NhRZ8+eVXh4uD744AO1atVK3333He8XuLR48WJt3rxZGzduLPIY/87AlQ4dOmjhwoVq0aKFDhw4oGnTpik5OVn/+9//qsx7hnAFAFXY6NGj9b///c9pTjvgSosWLfTdd98pJydHaWlpGjp0qFavXu3rbsFPZWZmauzYsVqxYoXCwsJ83R0EiB49eji+b9OmjTp06KDLL79c7777rmrWrOnDnnkP0wL9TExMjKxWa5HKKAcPHlSDBg181Cv4K/t7orj3S4MGDYoUQ7lw4YKOHj3Ke6qKGzNmjD766COtXLlSCQkJjuMNGjRQXl6ejh8/7tS+8PvG1fvK/hiqnpCQEDVt2lTt27fXjBkz1LZtW82ZM4f3C1zatGmTDh06pGuvvVY1atRQjRo1tHr1as2dO1c1atRQXFwc7xuUKCoqSs2bN9fOnTurzL81hCs/ExISovbt2ysjI8NxLD8/XxkZGerYsaMPewZ/1LhxYzVo0MDp/ZKbm6sNGzY43i8dO3bU8ePHtWnTJkebL774Qvn5+erQoUOl9xkVzzAMjRkzRh988IG++OILNW7c2Onx9u3bKzg42Ol9s337du3du9fpfbNlyxanYL5ixQpFRkaqVatWlfNC4FP5+fk6d+4c7xe41K1bN23ZskXfffed43bddddp8ODBju9536AkJ0+e1C+//KL4+Piq82+NrytqoKjFixcboaGhxsKFC40ff/zRGDlypBEVFeVUGQXVx4kTJ4xvv/3W+Pbbbw1JxqxZs4xvv/3W2LNnj2EYhvHMM88YUVFRxocffmj88MMPRp8+fYzGjRsbZ86ccVzjtttuM6655hpjw4YNxpdffmk0a9bMGDRokK9eEirYqFGjjDp16hirVq0yDhw44LidPn3a0ebPf/6zcdlllxlffPGF8c033xgdO3Y0Onbs6Hj8woULxtVXX23ceuutxnfffWcsX77ciI2NNSZNmuSLl4QK9uijjxqrV682du3aZfzwww/Go48+algsFuOzzz4zDIP3CzxTsFqgYfC+QVEPPfSQsWrVKmPXrl3G2rVrjZSUFCMmJsY4dOiQYRhV4z1DuPJTL774onHZZZcZISEhxg033GB89dVXvu4SfGTlypWGpCK3oUOHGoZhlmN//PHHjbi4OCM0NNTo1q2bsX37dqdrZGdnG4MGDTLCw8ONyMhIY/jw4caJEyd88GpQGVy9XyQZb7zxhqPNmTNnjAceeMCoW7euUatWLeOuu+4yDhw44HSd3bt3Gz169DBq1qxpxMTEGA899JBx/vz5Sn41qAz33nuvcfnllxshISFGbGys0a1bN0ewMgzeL/BM4XDF+waFDRgwwIiPjzdCQkKMRo0aGQMGDDB27tzpeLwqvGcshmEYvhkzAwAAAICqgzVXAAAAAOAFhCsAAAAA8ALCFQAAAAB4AeEKAAAAALyAcAUAAAAAXkC4AgAAAAAvIFwBAAAAgBcQrgAAAADACwhXAACU06pVq2SxWHT8+HFfdwUA4EOEKwAAAADwAsIVAAAAAHgB4QoAEPDy8/M1Y8YMNW7cWDVr1lTbtm2VlpYm6dKUvY8//lht2rRRWFiYfve73+l///uf0zXef/99XXXVVQoNDVVSUpKef/55p8fPnTunv/71r0pMTFRoaKiaNm2q1157zanNpk2bdN1116lWrVrq1KmTtm/f7njs+++/V9euXRUREaHIyEi1b99e33zzTQX9RAAAvkC4AgAEvBkzZuitt97S/PnztXXrVo0fP15/+MMftHr1akebhx9+WM8//7w2btyo2NhY9e7dW+fPn5dkhqK7775bAwcO1JYtWzR16lQ9/vjjWrhwoeP8IUOGaNGiRZo7d662bdumV155ReHh4U79eOyxx/T888/rm2++UY0aNXTvvfc6Hhs8eLASEhK0ceNGbdq0SY8++qiCg4Mr9gcDAKhUFsMwDF93AgCAsjp37pzq1aunzz//XB07dnQcHzFihE6fPq2RI0eqa9euWrx4sQYMGCBJOnr0qBISErRw4ULdfffdGjx4sA4fPqzPPvvMcf4jjzyijz/+WFu3btXPP/+sFi1aaMWKFUpJSSnSh1WrVqlr1676/PPP1a1bN0nSsmXL1KtXL505c0ZhYWGKjIzUiy++qKFDh1bwTwQA4CuMXAEAAtrOnTt1+vRp3XLLLQoPD3fc3nrrLf3yyy+OdgWDV7169dSiRQtt27ZNkrRt2zZ17tzZ6bqdO3fWjh07ZLPZ9N1338lqteqmm24qti9t2rRxfB8fHy9JOnTokCRpwoQJGjFihFJSUvTMM8849Q0AUDUQrgAAAe3kyZOSpI8//ljfffed4/bjjz861l2VV82aNT1qV3Can8VikWSuB5OkqVOnauvWrerVq5e++OILtWrVSh988IFX+gcA8A+EKwBAQGvVqpVCQ0O1d+9eNW3a1OmWmJjoaPfVV185vj927Jh+/vlntWzZUpLUsmVLrV271um6a9euVfPmzWW1WtW6dWvl5+c7reEqi+bNm2v8+PH67LPPlJqaqjfeeKNc1wMA+Jcavu4AAADlERERoYkTJ2r8+PHKz8/XjTfeqJycHK1du1aRkZG6/PLLJUnTp09XdHS04uLi9NhjjykmJkZ33nmnJOmhhx7S9ddfryeeeEIDBgzQ+vXr9dJLL+kf//iHJCkpKUlDhw7Vvffeq7lz56pt27bas2ePDh06pLvvvrvEPp45c0YPP/yw+vXrp8aNG2vfvn3auHGj+vbtW2E/FwBA5SNcAQAC3hNPPKHY2FjNmDFDv/76q6KionTttdfqb3/7m2Na3jPPPKOxY8dqx44dateunf7zn/8oJCREknTttdfq3Xff1eTJk/XEE08oPj5e06dP17BhwxzP8fLLL+tvf/ubHnjgAWVnZ+uyyy7T3/72N4/6Z7ValZ2drSFDhujgwYOKiYlRamqqpk2b5vWfBQDAd6gWCACo0uyV/I4dO6aoqChfdwcAUIWx5goAAAAAvIBwBQAAAABewLRAAAAAAPACRq4AAAAAwAsIVwAAAADgBYQrAAAAAPACwhUAAAAAeAHhCgAAAAC8gHAFAAAAAF5AuAIAAAAALyBcAQAAAIAX/H8U5F02upn9GwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the graph\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = initial_history.history['loss']\n",
    "val_loss = initial_history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, loss, 'bo-', label='loss')\n",
    "plt.plot(epochs, val_loss, 'ro-', label='val_loss')\n",
    "plt.title('training vs validation Loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "QeXEyYiq9hkx",
    "outputId": "891bc89d-beb1-4eae-c98a-34fd987cf084"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "recall = initial_history.history['recall']\n",
    "val_recall = initial_history.history['val_recall']\n",
    "epochs = range(1, len(recall) + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, recall, 'bo-', label='recall')\n",
    "plt.plot(epochs, val_recall, 'ro-', label='validation recall')\n",
    "plt.title('training and validation recall')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('recall')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "ack6m9Ql-U2g",
    "outputId": "28563b8b-722d-4150-de87-f54717d6958e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ecall = initial_history.history['precision']\n",
    "val_recall = initial_history.history['val_precision']\n",
    "epochs = range(1, len(recall) + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, recall, 'bo-', label='precision')\n",
    "plt.plot(epochs, val_recall, 'ro-', label='validation precision')\n",
    "plt.title('training and validation precision')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('precision')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzNz-vd-NawK"
   },
   "source": [
    "Since validation loss fell to 0.41 and began to plateau at around that level at roughly the epochs=300 mark, we will use this to train our final model. It is interesting to observe that while the validation loss continues to very gradually fall after the epochs=300 mark, the validation recall rate began to decrease slightly. This is possibly due to the model trying to balance out between the recall and precision rates or trying to obtain fewer false positives, while maintaining a sufficiently high recall rate. The validation precision is significantly lower than the training precision rate, which is largely due to the fact that the training set is oversampled, while the validation set remains hugely imbalanced.\n",
    "\n",
    "With the most optimal hyperparameters obtained from our experiments, we can now begin to train the final model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9G9S7jHddcl",
    "outputId": "61083ef7-251f-43c7-8b25-498a06b24a7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "268/268 [==============================] - 2s 4ms/step - loss: 0.6786 - accuracy: 0.5758 - recall_1: 0.4304 - precision_1: 0.5141\n",
      "Epoch 2/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.6354 - accuracy: 0.6587 - recall_1: 0.4607 - precision_1: 0.6518\n",
      "Epoch 3/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.5902 - accuracy: 0.6967 - recall_1: 0.5726 - precision_1: 0.6791\n",
      "Epoch 4/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.5610 - accuracy: 0.7156 - recall_1: 0.6368 - precision_1: 0.6862\n",
      "Epoch 5/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.5475 - accuracy: 0.7235 - recall_1: 0.6656 - precision_1: 0.6879\n",
      "Epoch 6/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.5389 - accuracy: 0.7300 - recall_1: 0.6895 - precision_1: 0.6892\n",
      "Epoch 7/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.5337 - accuracy: 0.7335 - recall_1: 0.7008 - precision_1: 0.6904\n",
      "Epoch 8/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.5297 - accuracy: 0.7373 - recall_1: 0.7123 - precision_1: 0.6920\n",
      "Epoch 9/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.5263 - accuracy: 0.7396 - recall_1: 0.7197 - precision_1: 0.6929\n",
      "Epoch 10/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.5237 - accuracy: 0.7409 - recall_1: 0.7237 - precision_1: 0.6933\n",
      "Epoch 11/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.5217 - accuracy: 0.7433 - recall_1: 0.7297 - precision_1: 0.6947\n",
      "Epoch 12/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.5194 - accuracy: 0.7437 - recall_1: 0.7324 - precision_1: 0.6944\n",
      "Epoch 13/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.5182 - accuracy: 0.7452 - recall_1: 0.7346 - precision_1: 0.6958\n",
      "Epoch 14/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.5168 - accuracy: 0.7458 - recall_1: 0.7359 - precision_1: 0.6964\n",
      "Epoch 15/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.5151 - accuracy: 0.7469 - recall_1: 0.7389 - precision_1: 0.6969\n",
      "Epoch 16/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.5140 - accuracy: 0.7483 - recall_1: 0.7404 - precision_1: 0.6984\n",
      "Epoch 17/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.5126 - accuracy: 0.7487 - recall_1: 0.7414 - precision_1: 0.6986\n",
      "Epoch 18/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.5126 - accuracy: 0.7496 - recall_1: 0.7422 - precision_1: 0.6997\n",
      "Epoch 19/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.5113 - accuracy: 0.7506 - recall_1: 0.7440 - precision_1: 0.7005\n",
      "Epoch 20/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.5104 - accuracy: 0.7505 - recall_1: 0.7443 - precision_1: 0.7004\n",
      "Epoch 21/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.5097 - accuracy: 0.7511 - recall_1: 0.7450 - precision_1: 0.7010\n",
      "Epoch 22/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.5088 - accuracy: 0.7514 - recall_1: 0.7452 - precision_1: 0.7013\n",
      "Epoch 23/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.5085 - accuracy: 0.7525 - recall_1: 0.7465 - precision_1: 0.7025\n",
      "Epoch 24/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.5075 - accuracy: 0.7525 - recall_1: 0.7471 - precision_1: 0.7024\n",
      "Epoch 25/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.5066 - accuracy: 0.7533 - recall_1: 0.7494 - precision_1: 0.7026\n",
      "Epoch 26/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.5066 - accuracy: 0.7535 - recall_1: 0.7470 - precision_1: 0.7038\n",
      "Epoch 27/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.5062 - accuracy: 0.7530 - recall_1: 0.7474 - precision_1: 0.7029\n",
      "Epoch 28/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.5060 - accuracy: 0.7536 - recall_1: 0.7489 - precision_1: 0.7033\n",
      "Epoch 29/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.5053 - accuracy: 0.7543 - recall_1: 0.7497 - precision_1: 0.7040\n",
      "Epoch 30/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.5045 - accuracy: 0.7537 - recall_1: 0.7480 - precision_1: 0.7038\n",
      "Epoch 31/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.5041 - accuracy: 0.7549 - recall_1: 0.7504 - precision_1: 0.7047\n",
      "Epoch 32/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.5037 - accuracy: 0.7550 - recall_1: 0.7505 - precision_1: 0.7048\n",
      "Epoch 33/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.5028 - accuracy: 0.7555 - recall_1: 0.7523 - precision_1: 0.7048\n",
      "Epoch 34/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.5033 - accuracy: 0.7555 - recall_1: 0.7512 - precision_1: 0.7053\n",
      "Epoch 35/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.5023 - accuracy: 0.7558 - recall_1: 0.7525 - precision_1: 0.7052\n",
      "Epoch 36/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.5022 - accuracy: 0.7564 - recall_1: 0.7537 - precision_1: 0.7057\n",
      "Epoch 37/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.5018 - accuracy: 0.7558 - recall_1: 0.7531 - precision_1: 0.7050\n",
      "Epoch 38/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.5016 - accuracy: 0.7563 - recall_1: 0.7546 - precision_1: 0.7051\n",
      "Epoch 39/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.5015 - accuracy: 0.7563 - recall_1: 0.7541 - precision_1: 0.7054\n",
      "Epoch 40/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.5013 - accuracy: 0.7567 - recall_1: 0.7545 - precision_1: 0.7058\n",
      "Epoch 41/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.5006 - accuracy: 0.7571 - recall_1: 0.7551 - precision_1: 0.7062\n",
      "Epoch 42/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.5008 - accuracy: 0.7570 - recall_1: 0.7556 - precision_1: 0.7058\n",
      "Epoch 43/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4999 - accuracy: 0.7575 - recall_1: 0.7567 - precision_1: 0.7061\n",
      "Epoch 44/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4998 - accuracy: 0.7578 - recall_1: 0.7572 - precision_1: 0.7064\n",
      "Epoch 45/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.5000 - accuracy: 0.7579 - recall_1: 0.7579 - precision_1: 0.7064\n",
      "Epoch 46/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4996 - accuracy: 0.7580 - recall_1: 0.7573 - precision_1: 0.7067\n",
      "Epoch 47/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4992 - accuracy: 0.7574 - recall_1: 0.7571 - precision_1: 0.7059\n",
      "Epoch 48/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4992 - accuracy: 0.7584 - recall_1: 0.7582 - precision_1: 0.7070\n",
      "Epoch 49/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4989 - accuracy: 0.7586 - recall_1: 0.7586 - precision_1: 0.7072\n",
      "Epoch 50/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4991 - accuracy: 0.7577 - recall_1: 0.7572 - precision_1: 0.7064\n",
      "Epoch 51/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4983 - accuracy: 0.7583 - recall_1: 0.7574 - precision_1: 0.7071\n",
      "Epoch 52/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4985 - accuracy: 0.7586 - recall_1: 0.7593 - precision_1: 0.7069\n",
      "Epoch 53/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4978 - accuracy: 0.7591 - recall_1: 0.7594 - precision_1: 0.7076\n",
      "Epoch 54/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4977 - accuracy: 0.7584 - recall_1: 0.7597 - precision_1: 0.7063\n",
      "Epoch 55/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4982 - accuracy: 0.7585 - recall_1: 0.7587 - precision_1: 0.7069\n",
      "Epoch 56/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4980 - accuracy: 0.7588 - recall_1: 0.7606 - precision_1: 0.7067\n",
      "Epoch 57/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4975 - accuracy: 0.7591 - recall_1: 0.7610 - precision_1: 0.7069\n",
      "Epoch 58/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4973 - accuracy: 0.7592 - recall_1: 0.7614 - precision_1: 0.7069\n",
      "Epoch 59/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4971 - accuracy: 0.7590 - recall_1: 0.7614 - precision_1: 0.7066\n",
      "Epoch 60/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4970 - accuracy: 0.7590 - recall_1: 0.7606 - precision_1: 0.7070\n",
      "Epoch 61/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4970 - accuracy: 0.7594 - recall_1: 0.7625 - precision_1: 0.7068\n",
      "Epoch 62/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4969 - accuracy: 0.7585 - recall_1: 0.7616 - precision_1: 0.7059\n",
      "Epoch 63/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4970 - accuracy: 0.7589 - recall_1: 0.7623 - precision_1: 0.7062\n",
      "Epoch 64/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4960 - accuracy: 0.7596 - recall_1: 0.7620 - precision_1: 0.7073\n",
      "Epoch 65/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4963 - accuracy: 0.7592 - recall_1: 0.7630 - precision_1: 0.7063\n",
      "Epoch 66/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4965 - accuracy: 0.7594 - recall_1: 0.7624 - precision_1: 0.7069\n",
      "Epoch 67/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4963 - accuracy: 0.7605 - recall_1: 0.7638 - precision_1: 0.7080\n",
      "Epoch 68/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4959 - accuracy: 0.7597 - recall_1: 0.7631 - precision_1: 0.7070\n",
      "Epoch 69/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4957 - accuracy: 0.7599 - recall_1: 0.7644 - precision_1: 0.7069\n",
      "Epoch 70/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4959 - accuracy: 0.7593 - recall_1: 0.7629 - precision_1: 0.7065\n",
      "Epoch 71/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4950 - accuracy: 0.7601 - recall_1: 0.7649 - precision_1: 0.7070\n",
      "Epoch 72/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4955 - accuracy: 0.7604 - recall_1: 0.7646 - precision_1: 0.7075\n",
      "Epoch 73/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4955 - accuracy: 0.7603 - recall_1: 0.7656 - precision_1: 0.7070\n",
      "Epoch 74/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4953 - accuracy: 0.7600 - recall_1: 0.7641 - precision_1: 0.7071\n",
      "Epoch 75/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4954 - accuracy: 0.7602 - recall_1: 0.7654 - precision_1: 0.7069\n",
      "Epoch 76/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4950 - accuracy: 0.7604 - recall_1: 0.7651 - precision_1: 0.7074\n",
      "Epoch 77/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4947 - accuracy: 0.7598 - recall_1: 0.7658 - precision_1: 0.7062\n",
      "Epoch 78/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4948 - accuracy: 0.7603 - recall_1: 0.7669 - precision_1: 0.7066\n",
      "Epoch 79/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4943 - accuracy: 0.7610 - recall_1: 0.7682 - precision_1: 0.7071\n",
      "Epoch 80/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4947 - accuracy: 0.7606 - recall_1: 0.7667 - precision_1: 0.7071\n",
      "Epoch 81/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4947 - accuracy: 0.7599 - recall_1: 0.7663 - precision_1: 0.7062\n",
      "Epoch 82/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4946 - accuracy: 0.7604 - recall_1: 0.7673 - precision_1: 0.7065\n",
      "Epoch 83/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4945 - accuracy: 0.7604 - recall_1: 0.7668 - precision_1: 0.7067\n",
      "Epoch 84/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4941 - accuracy: 0.7608 - recall_1: 0.7676 - precision_1: 0.7069\n",
      "Epoch 85/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4942 - accuracy: 0.7604 - recall_1: 0.7660 - precision_1: 0.7070\n",
      "Epoch 86/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4940 - accuracy: 0.7608 - recall_1: 0.7680 - precision_1: 0.7069\n",
      "Epoch 87/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4940 - accuracy: 0.7604 - recall_1: 0.7677 - precision_1: 0.7063\n",
      "Epoch 88/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4937 - accuracy: 0.7608 - recall_1: 0.7681 - precision_1: 0.7067\n",
      "Epoch 89/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4935 - accuracy: 0.7610 - recall_1: 0.7695 - precision_1: 0.7066\n",
      "Epoch 90/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4939 - accuracy: 0.7613 - recall_1: 0.7693 - precision_1: 0.7071\n",
      "Epoch 91/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4937 - accuracy: 0.7608 - recall_1: 0.7677 - precision_1: 0.7070\n",
      "Epoch 92/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4934 - accuracy: 0.7608 - recall_1: 0.7692 - precision_1: 0.7064\n",
      "Epoch 93/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4934 - accuracy: 0.7608 - recall_1: 0.7690 - precision_1: 0.7064\n",
      "Epoch 94/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4937 - accuracy: 0.7618 - recall_1: 0.7704 - precision_1: 0.7074\n",
      "Epoch 95/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4939 - accuracy: 0.7611 - recall_1: 0.7685 - precision_1: 0.7072\n",
      "Epoch 96/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4931 - accuracy: 0.7611 - recall_1: 0.7696 - precision_1: 0.7066\n",
      "Epoch 97/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4933 - accuracy: 0.7609 - recall_1: 0.7693 - precision_1: 0.7065\n",
      "Epoch 98/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4930 - accuracy: 0.7610 - recall_1: 0.7692 - precision_1: 0.7066\n",
      "Epoch 99/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4930 - accuracy: 0.7612 - recall_1: 0.7711 - precision_1: 0.7063\n",
      "Epoch 100/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4927 - accuracy: 0.7610 - recall_1: 0.7693 - precision_1: 0.7066\n",
      "Epoch 101/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4929 - accuracy: 0.7617 - recall_1: 0.7710 - precision_1: 0.7071\n",
      "Epoch 102/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4928 - accuracy: 0.7611 - recall_1: 0.7703 - precision_1: 0.7064\n",
      "Epoch 103/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4929 - accuracy: 0.7615 - recall_1: 0.7700 - precision_1: 0.7072\n",
      "Epoch 104/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4924 - accuracy: 0.7615 - recall_1: 0.7724 - precision_1: 0.7062\n",
      "Epoch 105/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4925 - accuracy: 0.7614 - recall_1: 0.7714 - precision_1: 0.7065\n",
      "Epoch 106/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4930 - accuracy: 0.7612 - recall_1: 0.7708 - precision_1: 0.7064\n",
      "Epoch 107/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4925 - accuracy: 0.7611 - recall_1: 0.7704 - precision_1: 0.7064\n",
      "Epoch 108/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4924 - accuracy: 0.7619 - recall_1: 0.7732 - precision_1: 0.7066\n",
      "Epoch 109/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4924 - accuracy: 0.7612 - recall_1: 0.7708 - precision_1: 0.7063\n",
      "Epoch 110/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4921 - accuracy: 0.7618 - recall_1: 0.7717 - precision_1: 0.7069\n",
      "Epoch 111/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4923 - accuracy: 0.7617 - recall_1: 0.7721 - precision_1: 0.7066\n",
      "Epoch 112/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4923 - accuracy: 0.7617 - recall_1: 0.7722 - precision_1: 0.7065\n",
      "Epoch 113/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4924 - accuracy: 0.7616 - recall_1: 0.7728 - precision_1: 0.7062\n",
      "Epoch 114/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4921 - accuracy: 0.7612 - recall_1: 0.7723 - precision_1: 0.7057\n",
      "Epoch 115/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4921 - accuracy: 0.7617 - recall_1: 0.7736 - precision_1: 0.7060\n",
      "Epoch 116/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4920 - accuracy: 0.7620 - recall_1: 0.7718 - precision_1: 0.7071\n",
      "Epoch 117/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4919 - accuracy: 0.7615 - recall_1: 0.7727 - precision_1: 0.7061\n",
      "Epoch 118/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4919 - accuracy: 0.7622 - recall_1: 0.7727 - precision_1: 0.7071\n",
      "Epoch 119/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4917 - accuracy: 0.7618 - recall_1: 0.7735 - precision_1: 0.7062\n",
      "Epoch 120/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4916 - accuracy: 0.7618 - recall_1: 0.7731 - precision_1: 0.7064\n",
      "Epoch 121/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4920 - accuracy: 0.7614 - recall_1: 0.7735 - precision_1: 0.7056\n",
      "Epoch 122/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4919 - accuracy: 0.7615 - recall_1: 0.7730 - precision_1: 0.7060\n",
      "Epoch 123/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4919 - accuracy: 0.7617 - recall_1: 0.7745 - precision_1: 0.7056\n",
      "Epoch 124/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4917 - accuracy: 0.7616 - recall_1: 0.7736 - precision_1: 0.7059\n",
      "Epoch 125/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4917 - accuracy: 0.7622 - recall_1: 0.7740 - precision_1: 0.7067\n",
      "Epoch 126/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4918 - accuracy: 0.7616 - recall_1: 0.7728 - precision_1: 0.7062\n",
      "Epoch 127/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4913 - accuracy: 0.7614 - recall_1: 0.7736 - precision_1: 0.7057\n",
      "Epoch 128/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4913 - accuracy: 0.7623 - recall_1: 0.7737 - precision_1: 0.7069\n",
      "Epoch 129/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4916 - accuracy: 0.7617 - recall_1: 0.7743 - precision_1: 0.7057\n",
      "Epoch 130/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4914 - accuracy: 0.7618 - recall_1: 0.7743 - precision_1: 0.7060\n",
      "Epoch 131/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4912 - accuracy: 0.7621 - recall_1: 0.7749 - precision_1: 0.7061\n",
      "Epoch 132/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4915 - accuracy: 0.7619 - recall_1: 0.7741 - precision_1: 0.7062\n",
      "Epoch 133/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4912 - accuracy: 0.7621 - recall_1: 0.7743 - precision_1: 0.7065\n",
      "Epoch 134/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4911 - accuracy: 0.7617 - recall_1: 0.7738 - precision_1: 0.7060\n",
      "Epoch 135/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4914 - accuracy: 0.7623 - recall_1: 0.7743 - precision_1: 0.7068\n",
      "Epoch 136/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4910 - accuracy: 0.7621 - recall_1: 0.7754 - precision_1: 0.7060\n",
      "Epoch 137/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4914 - accuracy: 0.7616 - recall_1: 0.7737 - precision_1: 0.7058\n",
      "Epoch 138/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4909 - accuracy: 0.7622 - recall_1: 0.7748 - precision_1: 0.7063\n",
      "Epoch 139/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4908 - accuracy: 0.7621 - recall_1: 0.7754 - precision_1: 0.7060\n",
      "Epoch 140/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4908 - accuracy: 0.7622 - recall_1: 0.7744 - precision_1: 0.7065\n",
      "Epoch 141/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4909 - accuracy: 0.7619 - recall_1: 0.7753 - precision_1: 0.7058\n",
      "Epoch 142/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4908 - accuracy: 0.7620 - recall_1: 0.7749 - precision_1: 0.7060\n",
      "Epoch 143/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4912 - accuracy: 0.7619 - recall_1: 0.7753 - precision_1: 0.7057\n",
      "Epoch 144/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4908 - accuracy: 0.7619 - recall_1: 0.7755 - precision_1: 0.7056\n",
      "Epoch 145/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4903 - accuracy: 0.7626 - recall_1: 0.7761 - precision_1: 0.7064\n",
      "Epoch 146/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4909 - accuracy: 0.7616 - recall_1: 0.7747 - precision_1: 0.7054\n",
      "Epoch 147/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4906 - accuracy: 0.7620 - recall_1: 0.7753 - precision_1: 0.7059\n",
      "Epoch 148/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4907 - accuracy: 0.7626 - recall_1: 0.7765 - precision_1: 0.7062\n",
      "Epoch 149/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4906 - accuracy: 0.7625 - recall_1: 0.7755 - precision_1: 0.7065\n",
      "Epoch 150/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4902 - accuracy: 0.7626 - recall_1: 0.7771 - precision_1: 0.7061\n",
      "Epoch 151/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4909 - accuracy: 0.7624 - recall_1: 0.7755 - precision_1: 0.7064\n",
      "Epoch 152/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4904 - accuracy: 0.7626 - recall_1: 0.7753 - precision_1: 0.7068\n",
      "Epoch 153/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4903 - accuracy: 0.7623 - recall_1: 0.7764 - precision_1: 0.7059\n",
      "Epoch 154/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4903 - accuracy: 0.7628 - recall_1: 0.7768 - precision_1: 0.7066\n",
      "Epoch 155/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4904 - accuracy: 0.7625 - recall_1: 0.7766 - precision_1: 0.7061\n",
      "Epoch 156/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4902 - accuracy: 0.7621 - recall_1: 0.7759 - precision_1: 0.7057\n",
      "Epoch 157/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4899 - accuracy: 0.7617 - recall_1: 0.7754 - precision_1: 0.7053\n",
      "Epoch 158/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4900 - accuracy: 0.7623 - recall_1: 0.7767 - precision_1: 0.7059\n",
      "Epoch 159/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4899 - accuracy: 0.7622 - recall_1: 0.7767 - precision_1: 0.7056\n",
      "Epoch 160/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4898 - accuracy: 0.7627 - recall_1: 0.7769 - precision_1: 0.7063\n",
      "Epoch 161/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4901 - accuracy: 0.7624 - recall_1: 0.7762 - precision_1: 0.7062\n",
      "Epoch 162/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4902 - accuracy: 0.7622 - recall_1: 0.7760 - precision_1: 0.7059\n",
      "Epoch 163/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4903 - accuracy: 0.7622 - recall_1: 0.7773 - precision_1: 0.7054\n",
      "Epoch 164/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4900 - accuracy: 0.7624 - recall_1: 0.7770 - precision_1: 0.7058\n",
      "Epoch 165/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4900 - accuracy: 0.7623 - recall_1: 0.7772 - precision_1: 0.7056\n",
      "Epoch 166/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4899 - accuracy: 0.7625 - recall_1: 0.7772 - precision_1: 0.7059\n",
      "Epoch 167/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4899 - accuracy: 0.7629 - recall_1: 0.7774 - precision_1: 0.7064\n",
      "Epoch 168/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4899 - accuracy: 0.7625 - recall_1: 0.7773 - precision_1: 0.7058\n",
      "Epoch 169/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4900 - accuracy: 0.7620 - recall_1: 0.7764 - precision_1: 0.7055\n",
      "Epoch 170/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4897 - accuracy: 0.7622 - recall_1: 0.7763 - precision_1: 0.7057\n",
      "Epoch 171/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4899 - accuracy: 0.7628 - recall_1: 0.7783 - precision_1: 0.7059\n",
      "Epoch 172/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4895 - accuracy: 0.7624 - recall_1: 0.7767 - precision_1: 0.7060\n",
      "Epoch 173/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4897 - accuracy: 0.7626 - recall_1: 0.7770 - precision_1: 0.7062\n",
      "Epoch 174/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4895 - accuracy: 0.7626 - recall_1: 0.7779 - precision_1: 0.7058\n",
      "Epoch 175/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4893 - accuracy: 0.7629 - recall_1: 0.7769 - precision_1: 0.7066\n",
      "Epoch 176/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4894 - accuracy: 0.7628 - recall_1: 0.7778 - precision_1: 0.7062\n",
      "Epoch 177/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4895 - accuracy: 0.7629 - recall_1: 0.7767 - precision_1: 0.7066\n",
      "Epoch 178/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4896 - accuracy: 0.7627 - recall_1: 0.7764 - precision_1: 0.7065\n",
      "Epoch 179/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4893 - accuracy: 0.7628 - recall_1: 0.7776 - precision_1: 0.7062\n",
      "Epoch 180/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4892 - accuracy: 0.7629 - recall_1: 0.7774 - precision_1: 0.7064\n",
      "Epoch 181/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4894 - accuracy: 0.7627 - recall_1: 0.7775 - precision_1: 0.7061\n",
      "Epoch 182/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4895 - accuracy: 0.7629 - recall_1: 0.7773 - precision_1: 0.7064\n",
      "Epoch 183/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4894 - accuracy: 0.7628 - recall_1: 0.7778 - precision_1: 0.7062\n",
      "Epoch 184/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4894 - accuracy: 0.7627 - recall_1: 0.7776 - precision_1: 0.7061\n",
      "Epoch 185/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4889 - accuracy: 0.7630 - recall_1: 0.7780 - precision_1: 0.7064\n",
      "Epoch 186/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4897 - accuracy: 0.7626 - recall_1: 0.7766 - precision_1: 0.7063\n",
      "Epoch 187/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4890 - accuracy: 0.7626 - recall_1: 0.7772 - precision_1: 0.7061\n",
      "Epoch 188/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4892 - accuracy: 0.7629 - recall_1: 0.7769 - precision_1: 0.7066\n",
      "Epoch 189/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4892 - accuracy: 0.7624 - recall_1: 0.7778 - precision_1: 0.7055\n",
      "Epoch 190/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4893 - accuracy: 0.7630 - recall_1: 0.7778 - precision_1: 0.7065\n",
      "Epoch 191/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4891 - accuracy: 0.7633 - recall_1: 0.7778 - precision_1: 0.7069\n",
      "Epoch 192/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4890 - accuracy: 0.7632 - recall_1: 0.7783 - precision_1: 0.7065\n",
      "Epoch 193/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4890 - accuracy: 0.7627 - recall_1: 0.7772 - precision_1: 0.7062\n",
      "Epoch 194/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4891 - accuracy: 0.7627 - recall_1: 0.7781 - precision_1: 0.7059\n",
      "Epoch 195/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4887 - accuracy: 0.7626 - recall_1: 0.7776 - precision_1: 0.7059\n",
      "Epoch 196/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4893 - accuracy: 0.7630 - recall_1: 0.7778 - precision_1: 0.7064\n",
      "Epoch 197/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4892 - accuracy: 0.7627 - recall_1: 0.7775 - precision_1: 0.7060\n",
      "Epoch 198/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4889 - accuracy: 0.7624 - recall_1: 0.7762 - precision_1: 0.7062\n",
      "Epoch 199/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4890 - accuracy: 0.7630 - recall_1: 0.7768 - precision_1: 0.7068\n",
      "Epoch 200/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4888 - accuracy: 0.7629 - recall_1: 0.7781 - precision_1: 0.7062\n",
      "Epoch 201/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4887 - accuracy: 0.7628 - recall_1: 0.7773 - precision_1: 0.7063\n",
      "Epoch 202/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4891 - accuracy: 0.7632 - recall_1: 0.7787 - precision_1: 0.7064\n",
      "Epoch 203/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4889 - accuracy: 0.7631 - recall_1: 0.7776 - precision_1: 0.7066\n",
      "Epoch 204/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4886 - accuracy: 0.7634 - recall_1: 0.7781 - precision_1: 0.7069\n",
      "Epoch 205/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4887 - accuracy: 0.7630 - recall_1: 0.7782 - precision_1: 0.7062\n",
      "Epoch 206/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4888 - accuracy: 0.7634 - recall_1: 0.7778 - precision_1: 0.7070\n",
      "Epoch 207/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4886 - accuracy: 0.7629 - recall_1: 0.7777 - precision_1: 0.7063\n",
      "Epoch 208/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4886 - accuracy: 0.7629 - recall_1: 0.7784 - precision_1: 0.7060\n",
      "Epoch 209/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4886 - accuracy: 0.7632 - recall_1: 0.7791 - precision_1: 0.7062\n",
      "Epoch 210/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4886 - accuracy: 0.7629 - recall_1: 0.7783 - precision_1: 0.7060\n",
      "Epoch 211/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4886 - accuracy: 0.7633 - recall_1: 0.7784 - precision_1: 0.7066\n",
      "Epoch 212/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4886 - accuracy: 0.7629 - recall_1: 0.7779 - precision_1: 0.7062\n",
      "Epoch 213/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4884 - accuracy: 0.7635 - recall_1: 0.7782 - precision_1: 0.7070\n",
      "Epoch 214/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4885 - accuracy: 0.7631 - recall_1: 0.7782 - precision_1: 0.7064\n",
      "Epoch 215/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4885 - accuracy: 0.7636 - recall_1: 0.7787 - precision_1: 0.7069\n",
      "Epoch 216/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4886 - accuracy: 0.7626 - recall_1: 0.7779 - precision_1: 0.7057\n",
      "Epoch 217/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4885 - accuracy: 0.7627 - recall_1: 0.7774 - precision_1: 0.7061\n",
      "Epoch 218/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4883 - accuracy: 0.7634 - recall_1: 0.7782 - precision_1: 0.7068\n",
      "Epoch 219/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4889 - accuracy: 0.7630 - recall_1: 0.7787 - precision_1: 0.7061\n",
      "Epoch 220/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4887 - accuracy: 0.7633 - recall_1: 0.7783 - precision_1: 0.7066\n",
      "Epoch 221/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4883 - accuracy: 0.7631 - recall_1: 0.7781 - precision_1: 0.7065\n",
      "Epoch 222/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4882 - accuracy: 0.7633 - recall_1: 0.7785 - precision_1: 0.7066\n",
      "Epoch 223/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4885 - accuracy: 0.7636 - recall_1: 0.7778 - precision_1: 0.7072\n",
      "Epoch 224/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4884 - accuracy: 0.7631 - recall_1: 0.7783 - precision_1: 0.7063\n",
      "Epoch 225/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4884 - accuracy: 0.7634 - recall_1: 0.7779 - precision_1: 0.7070\n",
      "Epoch 226/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4885 - accuracy: 0.7635 - recall_1: 0.7791 - precision_1: 0.7066\n",
      "Epoch 227/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4882 - accuracy: 0.7632 - recall_1: 0.7780 - precision_1: 0.7066\n",
      "Epoch 228/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4881 - accuracy: 0.7635 - recall_1: 0.7782 - precision_1: 0.7070\n",
      "Epoch 229/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4884 - accuracy: 0.7631 - recall_1: 0.7778 - precision_1: 0.7065\n",
      "Epoch 230/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4885 - accuracy: 0.7630 - recall_1: 0.7781 - precision_1: 0.7063\n",
      "Epoch 231/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4878 - accuracy: 0.7637 - recall_1: 0.7792 - precision_1: 0.7069\n",
      "Epoch 232/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4879 - accuracy: 0.7637 - recall_1: 0.7791 - precision_1: 0.7070\n",
      "Epoch 233/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4879 - accuracy: 0.7635 - recall_1: 0.7792 - precision_1: 0.7067\n",
      "Epoch 234/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4882 - accuracy: 0.7631 - recall_1: 0.7773 - precision_1: 0.7068\n",
      "Epoch 235/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4879 - accuracy: 0.7634 - recall_1: 0.7780 - precision_1: 0.7069\n",
      "Epoch 236/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4880 - accuracy: 0.7631 - recall_1: 0.7791 - precision_1: 0.7061\n",
      "Epoch 237/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4881 - accuracy: 0.7634 - recall_1: 0.7787 - precision_1: 0.7067\n",
      "Epoch 238/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4880 - accuracy: 0.7629 - recall_1: 0.7783 - precision_1: 0.7061\n",
      "Epoch 239/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4880 - accuracy: 0.7636 - recall_1: 0.7791 - precision_1: 0.7068\n",
      "Epoch 240/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4879 - accuracy: 0.7638 - recall_1: 0.7794 - precision_1: 0.7069\n",
      "Epoch 241/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4878 - accuracy: 0.7634 - recall_1: 0.7799 - precision_1: 0.7062\n",
      "Epoch 242/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4879 - accuracy: 0.7636 - recall_1: 0.7787 - precision_1: 0.7069\n",
      "Epoch 243/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4882 - accuracy: 0.7633 - recall_1: 0.7781 - precision_1: 0.7067\n",
      "Epoch 244/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4881 - accuracy: 0.7633 - recall_1: 0.7781 - precision_1: 0.7067\n",
      "Epoch 245/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4879 - accuracy: 0.7630 - recall_1: 0.7777 - precision_1: 0.7065\n",
      "Epoch 246/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4876 - accuracy: 0.7637 - recall_1: 0.7791 - precision_1: 0.7069\n",
      "Epoch 247/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4877 - accuracy: 0.7639 - recall_1: 0.7790 - precision_1: 0.7073\n",
      "Epoch 248/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4880 - accuracy: 0.7635 - recall_1: 0.7790 - precision_1: 0.7067\n",
      "Epoch 249/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4878 - accuracy: 0.7632 - recall_1: 0.7781 - precision_1: 0.7066\n",
      "Epoch 250/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4878 - accuracy: 0.7636 - recall_1: 0.7791 - precision_1: 0.7068\n",
      "Epoch 251/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4875 - accuracy: 0.7632 - recall_1: 0.7789 - precision_1: 0.7062\n",
      "Epoch 252/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4878 - accuracy: 0.7635 - recall_1: 0.7786 - precision_1: 0.7068\n",
      "Epoch 253/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4877 - accuracy: 0.7636 - recall_1: 0.7786 - precision_1: 0.7070\n",
      "Epoch 254/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4878 - accuracy: 0.7636 - recall_1: 0.7792 - precision_1: 0.7068\n",
      "Epoch 255/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4875 - accuracy: 0.7634 - recall_1: 0.7788 - precision_1: 0.7066\n",
      "Epoch 256/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4876 - accuracy: 0.7635 - recall_1: 0.7790 - precision_1: 0.7068\n",
      "Epoch 257/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4874 - accuracy: 0.7636 - recall_1: 0.7790 - precision_1: 0.7068\n",
      "Epoch 258/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4873 - accuracy: 0.7638 - recall_1: 0.7790 - precision_1: 0.7072\n",
      "Epoch 259/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4877 - accuracy: 0.7637 - recall_1: 0.7794 - precision_1: 0.7068\n",
      "Epoch 260/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4872 - accuracy: 0.7638 - recall_1: 0.7795 - precision_1: 0.7070\n",
      "Epoch 261/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4875 - accuracy: 0.7642 - recall_1: 0.7788 - precision_1: 0.7078\n",
      "Epoch 262/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4874 - accuracy: 0.7634 - recall_1: 0.7785 - precision_1: 0.7068\n",
      "Epoch 263/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4870 - accuracy: 0.7635 - recall_1: 0.7793 - precision_1: 0.7066\n",
      "Epoch 264/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4874 - accuracy: 0.7634 - recall_1: 0.7785 - precision_1: 0.7067\n",
      "Epoch 265/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4875 - accuracy: 0.7638 - recall_1: 0.7783 - precision_1: 0.7074\n",
      "Epoch 266/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4875 - accuracy: 0.7636 - recall_1: 0.7787 - precision_1: 0.7070\n",
      "Epoch 267/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4873 - accuracy: 0.7635 - recall_1: 0.7786 - precision_1: 0.7068\n",
      "Epoch 268/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4871 - accuracy: 0.7638 - recall_1: 0.7789 - precision_1: 0.7072\n",
      "Epoch 269/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4875 - accuracy: 0.7637 - recall_1: 0.7792 - precision_1: 0.7069\n",
      "Epoch 270/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4871 - accuracy: 0.7633 - recall_1: 0.7779 - precision_1: 0.7068\n",
      "Epoch 271/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4874 - accuracy: 0.7636 - recall_1: 0.7795 - precision_1: 0.7067\n",
      "Epoch 272/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4875 - accuracy: 0.7637 - recall_1: 0.7788 - precision_1: 0.7071\n",
      "Epoch 273/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4870 - accuracy: 0.7636 - recall_1: 0.7792 - precision_1: 0.7068\n",
      "Epoch 274/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4876 - accuracy: 0.7641 - recall_1: 0.7796 - precision_1: 0.7073\n",
      "Epoch 275/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4872 - accuracy: 0.7636 - recall_1: 0.7785 - precision_1: 0.7070\n",
      "Epoch 276/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4874 - accuracy: 0.7638 - recall_1: 0.7791 - precision_1: 0.7070\n",
      "Epoch 277/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4869 - accuracy: 0.7640 - recall_1: 0.7801 - precision_1: 0.7069\n",
      "Epoch 278/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4873 - accuracy: 0.7639 - recall_1: 0.7792 - precision_1: 0.7072\n",
      "Epoch 279/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4874 - accuracy: 0.7637 - recall_1: 0.7787 - precision_1: 0.7071\n",
      "Epoch 280/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4872 - accuracy: 0.7636 - recall_1: 0.7790 - precision_1: 0.7068\n",
      "Epoch 281/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4870 - accuracy: 0.7638 - recall_1: 0.7790 - precision_1: 0.7072\n",
      "Epoch 282/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4873 - accuracy: 0.7636 - recall_1: 0.7785 - precision_1: 0.7071\n",
      "Epoch 283/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4870 - accuracy: 0.7638 - recall_1: 0.7780 - precision_1: 0.7076\n",
      "Epoch 284/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4869 - accuracy: 0.7641 - recall_1: 0.7798 - precision_1: 0.7072\n",
      "Epoch 285/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4871 - accuracy: 0.7636 - recall_1: 0.7783 - precision_1: 0.7071\n",
      "Epoch 286/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4869 - accuracy: 0.7637 - recall_1: 0.7785 - precision_1: 0.7072\n",
      "Epoch 287/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4870 - accuracy: 0.7631 - recall_1: 0.7773 - precision_1: 0.7068\n",
      "Epoch 288/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4870 - accuracy: 0.7636 - recall_1: 0.7788 - precision_1: 0.7068\n",
      "Epoch 289/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4869 - accuracy: 0.7636 - recall_1: 0.7791 - precision_1: 0.7068\n",
      "Epoch 290/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4872 - accuracy: 0.7636 - recall_1: 0.7783 - precision_1: 0.7070\n",
      "Epoch 291/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4871 - accuracy: 0.7639 - recall_1: 0.7782 - precision_1: 0.7075\n",
      "Epoch 292/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4873 - accuracy: 0.7641 - recall_1: 0.7796 - precision_1: 0.7074\n",
      "Epoch 293/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4874 - accuracy: 0.7639 - recall_1: 0.7786 - precision_1: 0.7074\n",
      "Epoch 294/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4869 - accuracy: 0.7637 - recall_1: 0.7786 - precision_1: 0.7072\n",
      "Epoch 295/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4870 - accuracy: 0.7637 - recall_1: 0.7785 - precision_1: 0.7071\n",
      "Epoch 296/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4864 - accuracy: 0.7642 - recall_1: 0.7792 - precision_1: 0.7077\n",
      "Epoch 297/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4867 - accuracy: 0.7640 - recall_1: 0.7792 - precision_1: 0.7074\n",
      "Epoch 298/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4870 - accuracy: 0.7639 - recall_1: 0.7793 - precision_1: 0.7071\n",
      "Epoch 299/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4865 - accuracy: 0.7639 - recall_1: 0.7793 - precision_1: 0.7071\n",
      "Epoch 300/300\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.4867 - accuracy: 0.7642 - recall_1: 0.7796 - precision_1: 0.7075\n"
     ]
    }
   ],
   "source": [
    "#now combine the partial and val datasets and train for the final model\n",
    "\n",
    "x_combined = pd.concat([x_partial, x_val])\n",
    "y_combined = pd.concat([y_partial, y_val])\n",
    "\n",
    "final_model, final_history = train_model(300, x_combined, y_combined)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iqJlr3wGQPuE"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bCU3LYj9-y0j",
    "outputId": "fa2724ed-af85-4245-8085-1aa03234e6d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1941/1941 [==============================] - 4s 2ms/step - loss: 0.3806 - accuracy: 0.7713 - recall_1: 0.7611 - precision_1: 0.2367\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3806227743625641,\n",
       " 0.7713356018066406,\n",
       " 0.7611458897590637,\n",
       " 0.2366822063922882]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9YDuzmjLVNC"
   },
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zIGA4t_HHzS"
   },
   "source": [
    "The final model achieved a recall of 0.76. This means it is sufficiently good at finding the true cases of heart disease. It correctly identified 76% of the people who actually have heart disease. This is significantly better than just guessing 'no' for all individuals, the baseline recall rate, which would miss all true cases of heart disease.\n",
    "\n",
    "Due to the hugely imbalanced dataset, the accuracy rate is irrelevant.\n",
    "\n",
    "The model's precision is around 0.24. This means that when the model predicts heart disease, it is correct about 24% of the time. While this might seem low, in a situation like heart disease detection, it is more important to ensure that the model finds as many true cases as possible, even if it means obtaining more false alarms. This is because missing a true case could have serious consequences.\n",
    "\n",
    "Statistical power, in this context, means how good the model is at finding the cases of heart disease among many non-cases. A high recall indicates that the model has strong statistical power because it is able to detect the relatively rare 'yes' cases effectively.\n",
    "\n",
    "# Discussion\n",
    "\n",
    "\n",
    "Precision measures the proportion of true positive predictions in all positive predictions made by the model. Given the imbalance of 'no' cases, the model is challenged by a much smaller pool of 'yes' instances to learn from, making every prediction of heart disease more prone to error simply due to the rarity of positive cases. Hence, a lower precision should be interpreted with the understanding that the model operates under the constraint of limited positive examples.\n",
    "\n",
    "In the context of heart disease detection, the priority is to minimise the number of undiagnosed individuals. The model's higher recall (0.78) demonstrates its effectiveness in identifying a large portion of true positive cases. Missing a diagnosis could have far more severe implications than incorrectly identifying someone as at risk when they are not. Therefore, while precision is crucial, its lower value in this case is offset by the model's ability to achieve its main goal of identifying as many true cases of heart disease as possible.\n",
    "\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "Our model successfully proves the hypothesis that it is possible to predict heart disease from 22 medical features. Achieving a recall of 0.76, it effectively identifies 76% of true heart disease cases, far surpassing the baseline of guessing 'no', the majority class, for all individuals, which would detect none. This high recall is crucial for a first line medical diagnosis tool (before other tests like the ECG are deemed necessary), where missing a true case could have severe implications.\n",
    "\n",
    "The precision of the model is 0.24, indicating that about 24% of the model's predictions are correct. Despite the low rate, this is acceptable for a first line heart disease detection system, where the priority is to identify as many true cases as possible, even at the cost of more false alarms, for further evaluation. The precision rate could be improved significantly with other more sophisticated data science techniques, such as the Random Forest algorithm [4], which are beyond the scope of this module.\n",
    "\n",
    "Therefore, the model's performance supports our hypothesis, demonstrating its capability to detect heart disease with significant effectiveness.\n",
    "\n",
    "# References\n",
    "\n",
    "\n",
    "1. Blackwell, T. (n.d.) 9.2 Classifying Movie Reviews. Available at: https://www.coursera.org/learn/uol-cm3015-machine-learning-and-neural-networks/ungradedLab/1nlzq/9-210-imdb [Accessed: 11 March 2024]\n",
    "\n",
    "2. Pytlak, K (2023) https://www.kaggle.com/datasets/kamilpytlak/personal-key-indicators-of-heart-disease/data. Available at: https://www.kaggle.com/datasets/kamilpytlak/personal-key-indicators-of-heart-disease/data [Accessed: 11 March 2024]\n",
    "\n",
    "3. Centers for Disease Control and Prevention (2022) 2020 BRFSS Survey Data and Documentation. Available at: https://www.cdc.gov/brfss/annual_data/annual_2020.html [Accessed: 11 March 2024]\n",
    "\n",
    "4. Huh, Kyoun (2021) Surviving in a Random Forest with Imbalanced Datasets. Available at: https://medium.com/sfu-cspmp/surviving-in-a-random-forest-with-imbalanced-datasets-b98b963d52eb [Accessed: 11 March 2024]\n",
    "\n",
    "5. Tensorflow (n.d.) Classification on imbalanced data. Available at: https://www.tensorflow.org/tutorials/structured_data/imbalanced_data [Accessed: 11 March 2024]\n",
    "\n",
    "6. Kim, HY. (2013) Statistical notes for clinical researchers: assessing normal distribution (2) using skewness and kurtosis. Available at: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3591587/ [Accessed: 11 March 2024]\n",
    "\n",
    "7. Tamil Selvan, S (2020) Types Of Transformations For Better Normal Distribution. Available at: https://towardsdatascience.com/types-of-transformations-for-better-normal-distribution-61c22668d3b9 [Accessed: 11 March 2024]\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
